{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Python Modules for Data Analysis\"\n",
    "execute:\n",
    "    echo: true\n",
    "    eval: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy is a library for scientific and technical computing. It is built on top of Numpy. Scipy has many methods for numerical integration, optimization, interpolation, linear algebra, signal processing, image processing, etc.    <br>\n",
    "More information can be found under [Scipy website](https://www.scipy.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the subpackages are `scipy.cluster`, `scipy.constants`, `scipy.fftpack`, `scipy.integrate`, `scipy.interpolate`, `scipy.io`, `scipy.linalg`, `scipy.ndimage`, `scipy.odr`, `scipy.optimize`, `scipy.signal`, `scipy.sparse`, `scipy.spatial`, `scipy.special`, `scipy.stats`, `scipy.weave`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `scipy.stats` subpackage to perform statistical analysis. You can calculate the mean, median, mode, standard deviation, variance, skewness, kurtosis, etc. You can also perform hypothesis tests, like t-test, chi-square test, f-test, etc. You can also perform statistical modeling, like linear regression, logistic regression, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistical summary: \n",
      " DescribeResult(nobs=1000, minmax=(-3.1200489778662845, 3.4799474574915226), mean=0.03012725467181336, variance=0.9874106047908483, skewness=0.16006045274259637, kurtosis=0.17838129320176854)\n"
     ]
    }
   ],
   "source": [
    "# example of scipy.stats\n",
    "import scipy.stats as stats\n",
    "\n",
    "# create a normal distributed random variable\n",
    "rvs = stats.norm.rvs(size=1000)\n",
    "print(\"statistical summary: \\n\", stats.describe(rvs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope:  8.244088114253186 +/- 0.3297107686445956\n",
      "intercept:  -1.4285450637070412 +/- 1.9083869914860114\n",
      "rvalue:  0.9297801648805708\n",
      "pvalue:  2.526825425057965e-44\n",
      "intercept (95%): -1.428545 +/- 3.787132\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import linregress\n",
    "# linear regression\n",
    "def fun(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "x = np.linspace(0, 10, 100) \n",
    "y = 8*x + 10*np.random.normal(0, 1, 100)\n",
    "res = linregress(x, y)\n",
    "print(\"slope: \", res.slope, \"+/-\", res.stderr)\n",
    "print(\"intercept: \", res.intercept, \"+/-\", res.intercept_stderr)\n",
    "print(\"rvalue: \", res.rvalue)\n",
    "print(\"pvalue: \", res.pvalue)\n",
    "from scipy.stats import t\n",
    "# Two-sided inverse Studentâ€™s t-distribution\n",
    "# p probability, df degrees of freedom\n",
    "tinv = lambda p, df: abs(t.ppf(p/2, df))\n",
    "# 95% confidence interval\n",
    "ts = tinv(0.05, len(x)-2)\n",
    "print(f\"intercept (95%): {res.intercept:.6f}\" f\" +/- {ts*res.intercept_stderr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `scipy.optimize` subpackage to perform optimization. You can minimize or maximize a function. You can also perform non-linear least squares fitting, curve fitting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop:  [ 8.08884688 -1.63434236]\n",
      "pcov:  [[ 0.12089433 -0.60447165]\n",
      " [-0.60447165  4.05016356]]\n",
      "perr:  [0.34769862 2.01250182]\n",
      "m:  8.08884687649677  +/-  0.34769862200054785\n",
      "b:  -1.6343423576530771  +/-  2.0125018162642294\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "# define a function\n",
    "def linear_func(x, m, b):\n",
    "    return m*x + b\n",
    "\n",
    "# generate some data\n",
    "x = np.linspace(0, 10, 100) \n",
    "y = 8*x + 10*np.random.normal(0, 1, 100)\n",
    "\n",
    "# fit the data\n",
    "# non-linear least squares\n",
    "pop, pcov = opt.curve_fit(linear_func, x, y)\n",
    "print(\"pop: \", pop)\n",
    "print(\"pcov: \", pcov)\n",
    "print(\"perr: \", np.sqrt(np.diag(pcov)))\n",
    "print(\"m: \", pop[0] , \" +/- \", np.sqrt(pcov[0,0]))\n",
    "print(\"b: \", pop[1] , \" +/- \", np.sqrt(pcov[1,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can use the `scipy.interpolate` subpackage to perform interpolation. You can interpolate 1D, 2D, and 3D data. You can also perform spline fitting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `scipy.linalg` subpackage to perform linear algebra operations. You can calculate the determinant, inverse, eigenvalues, eigenvectors, etc. You can also solve linear systems of equations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [0.  0.5]\n",
      "A*x:  [1. 2.]\n",
      "b:  [1 2]\n",
      "Eigenvalues:  [-0.37228132  5.37228132]\n",
      "Eigenvectors:  EigResult(eigenvalues=array([-0.37228132,  5.37228132]), eigenvectors=array([[-0.82456484, -0.41597356],\n",
      "       [ 0.56576746, -0.90937671]]))\n",
      "Determinant:  -2.0000000000000004\n",
      "Inverse:  [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import solve\n",
    "# solve the linear equation\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([1, 2])\n",
    "x = solve(A, b)\n",
    "print(\"x: \", x)\n",
    "print(\"A*x: \", np.dot(A, x))\n",
    "print(\"b: \", b)\n",
    "print(\"Eigenvalues: \", np.linalg.eigvals(A))\n",
    "print(\"Eigenvectors: \", np.linalg.eig(A))\n",
    "print(\"Determinant: \", np.linalg.det(A))\n",
    "print(\"Inverse: \", np.linalg.inv(A))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `scipy.constants` subpackage to access physical and mathematical constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed of light:  299792458.0\n",
      "Planck constant:  6.62607015e-34\n",
      "Boltzmann constant:  1.380649e-23\n",
      "Avogadro constant:  6.02214076e+23\n"
     ]
    }
   ],
   "source": [
    "import scipy.constants as const\n",
    "print(\"speed of light: \", const.c)\n",
    "print(\"Planck constant: \", const.h)\n",
    "print(\"Boltzmann constant: \", const.k)\n",
    "print(\"Avogadro constant: \", const.N_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed of light:  (299792458.0, 'm s^-1', 0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.constants import physical_constants\n",
    "print(\"speed of light: \", physical_constants[\"speed of light in vacuum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels is a library for statistical modeling and testing. It is built on top of Numpy and Scipy. Statsmodels has many methods for statistical modeling, hypothesis testing, time series analysis, etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information [click here](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformula\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msmf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression {.unnumbered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.753\n",
      "Model:                            OLS   Adj. R-squared:                  0.745\n",
      "Method:                 Least Squares   F-statistic:                     91.38\n",
      "Date:                Fri, 01 Mar 2024   Prob (F-statistic):           1.29e-10\n",
      "Time:                        00:03:06   Log-Likelihood:                -80.015\n",
      "No. Observations:                  32   AIC:                             164.0\n",
      "Df Residuals:                      30   BIC:                             167.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         37.2851      1.878     19.858      0.000      33.450      41.120\n",
      "wt            -5.3445      0.559     -9.559      0.000      -6.486      -4.203\n",
      "==============================================================================\n",
      "Omnibus:                        2.988   Durbin-Watson:                   1.252\n",
      "Prob(Omnibus):                  0.225   Jarque-Bera (JB):                2.399\n",
      "Skew:                           0.668   Prob(JB):                        0.301\n",
      "Kurtosis:                       2.877   Cond. No.                         12.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.get_rdataset(\"mtcars\").data\n",
    "data = sm.add_constant(data)\n",
    "model = sm.OLS(data[\"mpg\"], data[[\"const\", \"wt\"]])\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn {.unnumbered}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-learn` is a library for machine learning. It is built on top of Numpy, Scipy, and Statsmodels. `Scikit-learn` has many methods for supervised learning, unsupervised learning, clustering, dimensionality reduction, etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information [click here](https://scikit-learn.org/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of supervised learning are linear regression, logistic regression, decision trees, random forests, support vector machines, k-nearest neighbors, etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression {.unnumbered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [938.23786125]\n",
      "Mean squared error: 2548.07\n",
      "Coefficient of determination: 0.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the diabetes dataset \n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes.data[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test = diabetes.target[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "\n",
    "print('Coefficient of determination: %.2f'\n",
    "        % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
