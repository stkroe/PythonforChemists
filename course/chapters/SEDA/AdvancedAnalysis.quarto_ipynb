{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Advanced Statistical Analysis\"\n",
        "execute: \n",
        "  echo: True\n",
        "  eval: True\n",
        "other-links: \n",
        "      - text: \"Download Code\"\n",
        "        href: \"https://github.com/stkroe/PythonforChemists/blob/main/course/notebooks/MultiVariantAnalysis.ipynb\"\n",
        "        icon: \"journal\"\n",
        "code-links:\n",
        "      - text: \"Open in Colab\"\n",
        "        href: \"https://colab.research.google.com/github/stkroe/PythonForChemists/blob/main/course/notebooks/MultiVariantAnalysis.ipynb\"\n",
        "        icon: \"laptop\"\n",
        "--- \n",
        "\n",
        "\n",
        "\n",
        "Often data has multi dimensions and it is not easy to analyze it. <br>\n",
        "Multivariate analysis is a set of statistical techniques used to analyze data with multiple variables.\n",
        "It helps in understanding the relationships between variables and identifying patterns in complex data. \n",
        "\n",
        "What can be done to analyze multi-dimensional data?\n",
        "\n",
        "-   **Dimensionality Reduction:** Simplifies complex datasets by reducing the number of variables.\n",
        "-   **Pattern Recognition:** Identifies underlying patterns and trends in data.\n",
        "-   **Feature Selection:** Selects relevant variables for modeling and prediction.\n",
        "\n",
        "## Dimensionality Reduction Techniques {.unnumbered}\n",
        "1.  **Principal Component Analysis (PCA):** reduces the dimensionality of data by transforming variables into uncorrelated components. It retains most of the variance in the data while reducing noise and redundancy.\n",
        "The PCA decomposition is based on the eigenvalues and eigenvectors of the covariance matrix of the data to identify the principal components with the highest variance.\n"
      ],
      "id": "cf30ddd7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Plot PCA results\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Analysis')\n",
        "plt.show()"
      ],
      "id": "08e12f0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes the data needs to be scaled before applying PCA. The `StandardScaler` from `sklearn` can be used to scale the data.\n"
      ],
      "id": "7f1cd404"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Standardizing Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applying PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot PCA results\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA Analysis')\n",
        "plt.show()\n"
      ],
      "id": "0e57ccb1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Independent Component Analysis (ICA):** Separates a multivariate signal into additive subcomponents.\n"
      ],
      "id": "5e9034b3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import FastICA\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "ica = FastICA(n_components=2)\n",
        "X_ica = ica.fit_transform(X)\n",
        "\n",
        "# Plot ICA results\n",
        "plt.scatter(X_ica[:, 0], X_ica[:, 1])\n",
        "plt.xlabel('ICA1')\n",
        "plt.ylabel('ICA2')\n",
        "plt.title('Independent Component Analysis')\n",
        "plt.show()"
      ],
      "id": "644993d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**t-Distributed Stochastic Neighbor Embedding (t-SNE):** Visualizes high-dimensional data in lower dimensions.\n"
      ],
      "id": "161c5769"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "tsne = TSNE(n_components=2,perplexity=3,learning_rate='auto')\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
        "plt.xlabel('t-SNE1')\n",
        "plt.ylabel('t-SNE2')\n",
        "plt.title('t-SNE Analysis')\n",
        "plt.show()"
      ],
      "id": "ba981582",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**UMAP (Uniform Manifold Approximation and Projection):** Reduces the dimensionality of data while preserving local and global structure.\n"
      ],
      "id": "ea49d3e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "umap_model = umap.UMAP(n_components=2)\n",
        "X_umap = umap_model.fit_transform(X)\n",
        "\n",
        "# Plot UMAP results\n",
        "plt.scatter(X_umap[:, 0], X_umap[:, 1])\n",
        "plt.xlabel('UMAP1')\n",
        "plt.ylabel('UMAP2')\n",
        "plt.title('UMAP Analysis')\n",
        "plt.show()"
      ],
      "id": "293d1978",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Factor Analysis:** Identifies latent factors that explain the variance in observed variables.\n"
      ],
      "id": "b0804309"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "fa = FactorAnalysis(n_components=2)\n",
        "X_fa = fa.fit_transform(X)\n",
        "\n",
        "# Plot Factor Analysis results\n",
        "plt.scatter(X_fa[:, 0], X_fa[:, 1])\n",
        "plt.xlabel('Factor 1')\n",
        "plt.ylabel('Factor 2')\n",
        "plt.title('Factor Analysis')\n",
        "plt.show()"
      ],
      "id": "c3d516f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Canonical Correlation Analysis (CCA):** Analyzes the relationship between two sets of variables.\n"
      ],
      "id": "45bd7f36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cross_decomposition import CCA\n",
        "\n",
        "# Sample dataset\n",
        "X = np.random.rand(10, 3)\n",
        "Y = np.random.rand(10, 3)\n",
        "cca = CCA(n_components=2)\n",
        "X_c, Y_c = cca.fit_transform(X, Y)\n",
        "\n",
        "# Plot CCA results\n",
        "plt.scatter(X_c[:, 0], Y_c[:, 0])\n",
        "plt.xlabel('CCA1')\n",
        "plt.ylabel('CCA2')\n",
        "\n",
        "plt.title('Canonical Correlation Analysis')\n",
        "plt.show()"
      ],
      "id": "56bfc035",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Clustering Techniques  {.unnumbered}\n",
        "\n",
        "\n",
        "\n",
        "Clustering is an unsupervised learning technique used to group similar data points based on patterns. In chemistry and materials science, clustering helps in categorizing material properties, identifying experimental trends, and classifying samples.\n",
        "\n",
        "- **k-Means Clustering:** Partitions data into k clusters based on similarity.\n"
      ],
      "id": "0714d7d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Example dataset\n",
        "X = np.random.rand(20, 2)\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "kmeans.fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Plot Clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.show()"
      ],
      "id": "90eee815",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **Hierarchical Clustering:** Builds a tree of clusters using agglomerative or divisive methods.\n"
      ],
      "id": "d7689ab4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Example dataset\n",
        "X = np.random.rand(20, 2)\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Plot Clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Hierarchical Clustering')\n",
        "plt.show()"
      ],
      "id": "bf02002d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/stk/y/envs/myenv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}