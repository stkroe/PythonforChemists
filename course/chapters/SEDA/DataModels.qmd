---
title: "Inferential Statistics"
execute: 
  echo: True
  eval: True
other-links: 
      - text: "Download Code"
        href: "https://github.com/stkroe/PythonforChemists/blob/main/course/notebooks/SimpleDataModels.ipynb"
        icon: "journal"
code-links:
      - text: "Open in Colab"
        href: "https://colab.research.google.com/github/stkroe/PythonForChemists/blob/main/course/notebooks/SimpleDataModels.ipynb"
        icon: "laptop"
--- 
# Data Models {.unnumbered}

## Linear RegressionÂ {.unnumbered}

Often we want to find a model which can explain the data.
It is important to understand the data and the model to be able to interpret the results and make predictions.

The simplest model is the linear regression model. It assumes that the data has a linear relationship with the target variable. 
For example, if we have a single feature $x$ and a target variable $y$, the linear regression model can be defined as:

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

where $y$ is the target variable, $x$ is the feature, $\beta_0$ and $\beta_1$ are the coefficients, and $\epsilon$ is the error term.

For multiple features $x_1, x_2, \ldots, x_n$, the linear regression model can be defined as:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n + \epsilon
$$

where $y$ is the target variable, $x_1, x_2, \ldots, x_n$ are the features, $\beta_0, \beta_1, \ldots, \beta_n$ are the coefficients, and $\epsilon$ is the error term.


In `python` there exists serveral libraries which can be used to fit a linear regression model.

### Using `scipy`{.unnumbered}

A easy way is to use `scipy` library. The `scipy` library has a function called `linregress` which can be used to fit a linear regression model. The function returns the slope, intercept, r-value, p-value, and the standard error of the estimate. And with scipy version 1.15.2 also the intercept error.

```{python}
from scipy.stats import linregress
import numpy as np

x = [1, 2, 3, 4, 5]
y = x*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)

results = linregress(x, y)

slope = results.slope
intercept = results.intercept
r_value = results.rvalue
p_value = results.pvalue
std_err = results.stderr
intercept_err = results.intercept_stderr
print("slope: %f    intercept: %f" % (slope, intercept))
print("R-squared: %f" % r_value**2)
print("p-value: %f" % p_value)
print("standard error: %f" % std_err)
print("Intercept error: %f" %intercept_err)
# Two-sided inverse Students t-distribution
# p - probability, df - degrees of freedom
from scipy.stats import t
tinv = lambda p, df: abs(t.ppf(p/2, df))
print("95% confidence interval: " + str(intercept - tinv(0.05, len(x)-2)*intercept_err) + " to " + str(intercept + tinv(0.05, len(x)-2)*intercept_err))
```

:::{.callout-important}
For older version `scipy`only returned 5 values with fields slope, intercept, rvalue, pvalue and stderr. For compatiblity reasons the return values are 5 elements tuple.

```{python}
from scipy.stats import linregress
slope, intercept, r, p, se = linregress(x, y)
```
And if you want to get the intercept error you can use the following return value as a object:

```{python}
from scipy.stats import linregress
results = linregress(x, y)
print(results.intercept_stderr)
```
:::

### Using `statsmodels`{.unnumbered}

Another library is `statsmodels`. The `statsmodels` library provides more detailed information about the model, such as the coefficients, standard errors, t-values, p-values, and confidence intervals.

```{python}
import statsmodels.api as sm
import numpy as np

X = np.array([1, 2, 3, 4, 5])
y = X*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)

X_with_const = sm.add_constant(X)  # Add intercept term
model = sm.OLS(y, X_with_const).fit()
print(model.summary())
```


### Using `scikit-learn` {.unnumbered}
One of the most popular libraries is `scikit-learn`. The following code shows how to fit a linear regression model using `scikit-learn`:


```{python}
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# Sample data
X = np.linspace(1, 5, 100).reshape(-1, 1)
y = 2 * X + 1 + np.random.normal(0, 1, 100).reshape(-1, 1)

# Model fitting
model = LinearRegression()
model.fit(X, y)

# Predictions
y_pred = model.predict(X)

# Plot
plt.scatter(X, y, color='blue', label='Data')
plt.plot(X, y_pred, color='red', label='Linear Fit')
plt.legend()
plt.show()
```
---

## Non-Linear Fits
Linear regression may not always be sufficient, especially for complex relationships. Non-linear models provide more flexibility.

### Polynomial Regression

Polynomial regression is a type of linear regression where the relationship between the independent variable $x$ and the dependent variable $y$ is modeled as an $n$-th degree polynomial.

```{python}
import numpy as np
import matplotlib.pyplot as plt


# Sample data
X = np.linspace(1, 5, 100)
y = X**2 + np.random.normal(0, 1, 100)

model = np.polyfit(X, y, 2)

model = np.poly1d(model)

X_range = np.linspace(1, 5, 100)
y_fit = model(X_range)

print(model)

plt.scatter(X, y, color='blue', label='Data')
plt.plot(X_range, y_fit, color='purple', label='Polynomial Fit')
plt.legend()


```

### Curve Fitting with `scipy`

`scipy` provides the `curve_fit` function to fit a non-linear model to the data. The function requires the model function and the data as input.

```{python}
from scipy.optimize import curve_fit

def nonlinear_func(x, a, b, c):
    return a * np.sin(b * x) + c

params, _ = curve_fit(nonlinear_func, X.flatten(), y)

X_range = np.linspace(1, 5, 100)
y_fit = nonlinear_func(X_range, *params)

plt.scatter(X, y, color='blue', label='Data')
plt.plot(X_range, y_fit, color='purple', label='Non-Linear Fit')
plt.legend()
plt.show()
```


