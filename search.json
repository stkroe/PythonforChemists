[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "",
    "text": "Welcome to this Course!\nThis course is designed for better understanding how Python can be used for data analysis and visualization in the fields of chemistry and materials science. Before diving into the course content, please take a moment to review the following important informations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to this Course!</span>"
    ]
  },
  {
    "objectID": "index.html#who-is-the-course-for",
    "href": "index.html#who-is-the-course-for",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "Who is the course for?",
    "text": "Who is the course for?\n\nFor everyone who is interested in data analysis and visualization in the field of chemistry and materials science.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to this Course!</span>"
    ]
  },
  {
    "objectID": "index.html#what-can-you-expect-from-this-course",
    "href": "index.html#what-can-you-expect-from-this-course",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "What can you expect from this course",
    "text": "What can you expect from this course\n\nThis course wants to show you the advantage of using a programming language for data analysis and visualization in comparison to GUI-based softwares.\nThe course will give you a very brief introduction to Python and its libraries.\nThe main focus will be on the libraries numpy, pandas, scipy, scikit-learn, matplotlib.\nThe course is organized interactivly. You will get the chance to practice with exercises.\nUpon successful completion of this course, you will have acquired a comprehensive understanding of the fundamental components of Python and the key packages necessary for the analysis and presentation of your own research data.\nAt the end of this course you will be able to find enough resources to dive deeper into Python and you can test your knowledge by an example exam.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to this Course!</span>"
    ]
  },
  {
    "objectID": "index.html#what-can-you-not-expect-from-this-course",
    "href": "index.html#what-can-you-not-expect-from-this-course",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "What can you NOT expect from this course",
    "text": "What can you NOT expect from this course\n\nYou will not get a deep explanation of the Python language. Please consider full Python tutorials to get a deeper overview of Python.\nYou will not learn object-oriented programming or functional programming in Python.\nThis is not a statistics course.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to this Course!</span>"
    ]
  },
  {
    "objectID": "index.html#how-this-course-is-structured",
    "href": "index.html#how-this-course-is-structured",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "How this course is structured",
    "text": "How this course is structured\nThe plan is to create different levels of content, so that you can choose the level of difficulty that suits you best. The course is designed to be flexible and adaptable to your needs, allowing you to focus on the areas that are most relevant to your work or interests.\n\n\n\n\n\n\nImportant\n\n\n\nDue to the fact that this course is still under construction, the content may change over time. The current version of the course is a work in progress and may not reflect the final structure or content.\nIf someone is interested in contributing to this course, please feel free to contribute. The course is hosted via GitHub. Contributions, suggestions, and feedback are highly appreciated. Please refer in the README to the Contribution Guidelines for more details.\n\n\nThis course is divided into three possible paths to accomplish the learning objectives:\n\nBeginner Path: Focuses on first contact with Python, covering basic concepts and introductory modules.\nAdvanced Path: Provides a deeper dive into different libraries and specialized visualization plots. [work in progress]\nChallenging Path: Explores advanced topics and complex data visualization techniques, with more difficult exercises and in-depth analysis. [TODO]\n\nThe different difficulty levels are marked with the following icons:\n\nIntroduction Parts:      \nBeginner Path:            \nAdvanced Path:         \nChallenging Path:           \n\nThe icons are located under the title of each lecture. The more stars you see, the more difficult the content is.\nEach lecture includes examples related to chemistry and material science. At the end of each part, there are exercises to test your understanding and reinforce the concepts learned. These exercises are designed to be practical and relevant to real-world scenarios in chemistry and materials science.\nComprehensive Exam [TODO] At the very end of the course, there is a comprehensive exam which covers a complete data analysis and visualization example.\n\n\n\n\n\n\nImportant\n\n\n\nIt is recommended to try out every lecture and exercise to get the most out of this course. If the lecture is created as interactive notebook, you can run the notebook in the cloud using Google Colab via this icon  or download it and run it on your local machine via this icon . If any data files are used you can download them via this icon  and .",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome to this Course!</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Introduction.html",
    "href": "course/chapters/Essentials/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What do you need for data analysis and visualization?\nDifficulty level:\nA very simple approach would be the use of basic spreadsheet programs with graphical user interfaces e.g. Excel, LibreOffice Calc …\nBut for more advanced analysis you need probably specific plotting and analysis tools:\nThis is a small election of tools, that can be used for data analysis and plotting.\nFor a larger list see these Wikipedia lists:",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Introduction.html#what-do-you-need-for-data-analysis-and-visualization",
    "href": "course/chapters/Essentials/Introduction.html#what-do-you-need-for-data-analysis-and-visualization",
    "title": "Introduction",
    "section": "",
    "text": "Research normally contains handling a lot of data.\nClear presentation of data is essential to the understanding of data.\nIt helps to improve your scientific communication and make your results more accessible to others.\nThere are many tools available for data analysis and visualization.\n\n\n\n\nYou can either use programs with GUIs e.g. LabPlot, QtiPlot, Scilab, SciDAVis, Origin …\nor a GUI based program with command line interface e.g. Xmgrace, GNU Octave …\nor command line based tools e.g. Gnuplot, Matlab, Mathematica …\nor programming languages e.g. Python, R, Julia …\n\n\n\n\nList of Numerical Analysis Software\nList of Graphical Software\nList of Statistical Software\nList of Computer Algebra Systems",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html",
    "href": "course/chapters/Essentials/WhatIsPython.html",
    "title": "Python?",
    "section": "",
    "text": "Short History of Python\nDifficulty level:\n(see Wikipedia for more details)\nVan Rossum designed Python as a “Computer Programming Language for Everybody”.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#short-history-of-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#short-history-of-python",
    "title": "Python?",
    "section": "",
    "text": "1989: the beginning of python\n1991: Guido van Rossum, a Dutch programmer and father of Python, implemented and published the first version of Python.\nFun fact: The name Python came from the show Monty Python’s Flying Circus.\n1994: Python 1.0 was released\n2000: Python 2.0 was released\n2008: Python 3.0 was released with new syntax and features\nImportant: python3 to python2 is backward incompatible e.g. print(\"Hello World\") python3 and print \"Hello World\" python2.\npython2 is nowadays outdated. It is not recommended using it for new projects.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#what-is-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#what-is-python",
    "title": "Python?",
    "section": "What is Python?",
    "text": "What is Python?\n\nPython is an interpreted language compared to compiled languages e.g., C or C++.\nPython is often slower compared to compiled languages.\nPython has dynamic type checking and garbage collection.\nPython supports both object-oriented and functional programming.\nPython is a high-level language, meaning it is closer to human language than the machine language understood by computers.\n\n\n\nSimplified Scheme how Python works internally:\nIf you execute a python program, the python interpreter will convert the code into byte code and then the byte code will be executed by the python virtual machine (PVM).\n\n\n\nInterpreter Scheme - Simplified",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#how-i-can-learn-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#how-i-can-learn-python",
    "title": "Python?",
    "section": "How I can learn Python?",
    "text": "How I can learn Python?\nThere are a lot of resources to dive deeper into Python e.g.:\n\nhttps://www.py4e.com/ (Python for Everybody)\nhttps://py-tutorial-de.readthedocs.io/de/python-3.3/ (German Tutorial)\nhttps://www.w3schools.com/python/default.asp\nhttps://jakevdp.github.io/PythonDataScienceHandbook/\nhttps://exercism.org/ (Coding Exercises)\nhttps://www.freecodecamp.org/ (Coding Exercises)\nhttps://realpython.com/tutorials/data-viz/\nhttps://www.youtube.com/watch?v=LHBE6Q9XlzI",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#use-ai-to-learn-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#use-ai-to-learn-python",
    "title": "Python?",
    "section": "Use AI to learn Python",
    "text": "Use AI to learn Python\n\n\n\n\n\n\nImportant\n\n\n\n\nLanguage Models like GPT-3 can help you to learn Python.\nToday it is not always necessary to learn the syntax of a programming languages by heart.\nYou can use AI tools to help you with the syntax and concentrate on the problem-solving part.\nLearn how to prompt your problem to the AI and get a proper solution.\nBut be aware that the AI is not perfect and can give you wrong solutions.\nSo it is important that you can understand the code and can debug it.\nAI is only a tool not a replacement for a human.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#practice-practice-practice",
    "href": "course/chapters/Essentials/WhatIsPython.html#practice-practice-practice",
    "title": "Python?",
    "section": "Practice, Practice, Practice",
    "text": "Practice, Practice, Practice\n\n\n\n\n\n\nImportant\n\n\n\nAs it is with languages, you can only learn it if you practice it.  So, start you own projects and have fun with it!",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html",
    "href": "course/chapters/Essentials/InstallationGuide.html",
    "title": "Installation Guide",
    "section": "",
    "text": "How to install Python locally?\nDifficulty level:\nHere is a short instruction how to install Python on your local PC or you can use Google Colab to solve all the exercises of this course.\nFundamental Python websites:",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#install-python-interpreter-you-do-not-needed-if-you-use-an-environment-manager",
    "href": "course/chapters/Essentials/InstallationGuide.html#install-python-interpreter-you-do-not-needed-if-you-use-an-environment-manager",
    "title": "Installation Guide",
    "section": "1. Install Python Interpreter (you do not needed if you use an environment manager)",
    "text": "1. Install Python Interpreter (you do not needed if you use an environment manager)\nYour preferred searching engine is your friend to find the best way to install Python on your system. Please choose that method which is suitable for you.\nPython can be installed by several ways:\n\ndirectly by Python official site\n\nthe installation guide can be found under python wiki\n\nor via package manager of your os:\n\ne.g.: sudo apt install python (linux debian) or brew install python (macOS)\n\nor via docker, wsl ect.\nor via a conda or mamba python package and environment managers which have a python interpreter on board and are available for Windows, Linux and macOS [my recommendation]",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#python-package-and-environment-manager",
    "href": "course/chapters/Essentials/InstallationGuide.html#python-package-and-environment-manager",
    "title": "Installation Guide",
    "section": "2. Python Package and Environment manager",
    "text": "2. Python Package and Environment manager\nThe advantage of using a python package and environment manager is that you have a python interpreter directly on board, but you can also directly create different python enviroments and install and remove python packages.\n\nConda\nThere are differerent conda installer: (Please pay attention which one is suitable for you (https://docs.anaconda.com/distro-or-miniconda/).\n\n\n\n\n\n\nWarning\n\n\n\nPlease read the Anaconda Terms of Service FAQs and Terms of Service) not every case is free of charge of use.\n\n\n\nAnaconda Distribution is a comprehensive distribution which includes conda and hundreds of preinstalled packages and tools.\nminiconda is the light version of it which contains only conda, python interpreter and few fundamental packages\nminiforge minimal installer for conda and using only the community conda-forge channel\n\n\n\nMamba\nAnother python package and environment manager is mamba.\nmamba is a reimplementation of conda:\n\nmicromamba is a statically linked version of mamba\nmambaand it is currently faster than conda\n\n\n\n\n\n\n\nTip\n\n\n\nRecommendation: micromamba  \nInstall it like it is explained under the micromamba documentation: - https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html\n\n\nPlease install in one of the above explained ways Python and use your preferred searching engine to get more information.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#set-up-an-environment",
    "href": "course/chapters/Essentials/InstallationGuide.html#set-up-an-environment",
    "title": "Installation Guide",
    "section": "3. Set up an environment",
    "text": "3. Set up an environment\nIt is often very useful to have different python environments for different python projects because of the need of different python package versions.\nYou can use conda or micromamba to create different environments. There exists also other virtual environment manager.\nIn this course the explanation is restricted to micromamba as an example. If you want to use something else there exists tons of information online how to use other programs.\n\nMicromamba: Most important commands are:\nRead for more detail: https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html\nCreating a new virtual environment:\nmicromamba create --name &lt;myenvname&gt;\nInstall new packages:\nmicromamba install &lt;packagename&gt;\nList all environments:\nmicromamba env list\nActivate an environment:\nmicromamba activate &lt;myenvname&gt;\nList all packages of this environment:\nmicromamba list",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#usefull-packages-for-data-analysis-and-visualization",
    "href": "course/chapters/Essentials/InstallationGuide.html#usefull-packages-for-data-analysis-and-visualization",
    "title": "Installation Guide",
    "section": "4. Usefull packages for Data Analysis and Visualization:",
    "text": "4. Usefull packages for Data Analysis and Visualization:\n\npip - package installer for python instead of conda\njupyter-notebook/jupyterlab - interactive computing environment\nipykernel - IPython Kernel for Jupyter\nmatplotlib - data visualization library\nnumpy - numerical library\nscipy - scientific library\npandas - data manipulation library\nseaborn - data visualization library\nscikit-learn - machine learning library\nstatsmodels - statistical library\n\nspecial packages for data visualization:\n\nbokeh - interactive data visualization library\nplotly - interactive data visualization library\nnetworkx - network analysis library\npython-ternary - ternary plot library\naltair - declarative statistical visualization library\numap-learn - dimensionality reduction library\n\nspecial packages for chemistry and material science:\n\nase - atomic simulation environment\npymatgen - Python Materials Genomics\nRdKit - cheminformatics library\nopenbabel - cheminformatics library\npymol-opensource - molecular visualization library\n\nFor more chemistry and material science packages please check the Awesome Python Chemistry repository.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#short-cut",
    "href": "course/chapters/Essentials/InstallationGuide.html#short-cut",
    "title": "Installation Guide",
    "section": "Short Cut",
    "text": "Short Cut\n\n\n\n\n\n\nTip\n\n\n\nRecommendation  Use yml-file with all needed packages and configurations:\n\n\nSave your needed packages in an environment.yml file e.g.:\nname: myenv\nchannels:\n - conda-forge\ndependencies:\n - python=3.12\n - pip\n - ipykernel\n - jupyterlab\n - pandas\n - numpy\n - matplotlib\n - scikit-learn\n - scipy\n - seaborn\n - statsmodels\nand create an environment with this specific packages:\nmicromamba env create -f environment.yml\nThen you can activate it via:\nmicromamba activate myenv",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#test-your-installation",
    "href": "course/chapters/Essentials/InstallationGuide.html#test-your-installation",
    "title": "Installation Guide",
    "section": "5. Test your installation",
    "text": "5. Test your installation\nTest your installation by opening the interactive python mode by typing in your terminal (Linux, macOS) / command prompt (Windows):\npython\nthen something like this should be opened in your terminal (Linux, macOS) / command prompt (Windows)\nPython 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \nthen type:\nprint(\"Hello World!\")\nIf this works your installation was successful!",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Editors.html",
    "href": "course/chapters/Essentials/Editors.html",
    "title": "Editors",
    "section": "",
    "text": "Choose an editor\nDifficulty level:\nAfter you installed Python successfully you need an editor for writing your Python programs. A python script is a text file with the ending .py. Technicallly you could use everything where you can write text, but it is not really purposeful.\nAn editor with syntax highlighting, code completion and debugging is very useful.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editors</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Editors.html#using-jupyter-notebooks-in-vs-code",
    "href": "course/chapters/Essentials/Editors.html#using-jupyter-notebooks-in-vs-code",
    "title": "Editors",
    "section": "Using Jupyter Notebooks in VS Code",
    "text": "Using Jupyter Notebooks in VS Code\n\n\n\nVS Code\n\n\n\n\n\nPython Cell\n\n\n\nDifference Between Jupyter Notebooks and Python Scripts\n\nInteractivity: Jupyter Notebooks allow you to run code in chunks (cells) and see the output immediately, making them ideal for experimentation and visualization. Python scripts (.py files) are typically executed all at once.\nDocumentation: Notebooks support Markdown cells for adding rich text, equations, and images alongside your code. Python scripts are plain text files and require comments for documentation.\nUse Case: Notebooks are great for exploratory data analysis and teaching, while Python scripts are better suited for production code and automation.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editors</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Editors.html#google-colab-tutorial",
    "href": "course/chapters/Essentials/Editors.html#google-colab-tutorial",
    "title": "Editors",
    "section": "Google Colab Tutorial",
    "text": "Google Colab Tutorial\nGoogle Colab is a free, cloud-based platform that allows you to run Jupyter Notebooks without any setup. To get started with Google Colab:\n\nThe use of Google Colab needs a Google Account. Please read the Terms of Service and Privacy Policy\n\n\nVisit Google Colab and sign in with your Google account.\nCreate a new notebook or upload an existing .ipynb file.\nWrite and execute code in cells, just like in Jupyter Notebooks.\nSave your work to Google Drive or download it as a .ipynb file.\n\n\nKey Features of Google Colab\n\nFree Access to GPUs/TPUs: Accelerate your computations by enabling GPU or TPU support from the “Runtime” menu.\nCollaboration: Share notebooks with others and work on them simultaneously, similar to Google Docs.\nPre-installed Libraries: Many popular Python libraries are pre-installed, saving you setup time.\nIntegration with Google Drive: Easily access and save files to your Google Drive.\n\n\n\n\nGoogle Colab Interface\n\n\nUpload data on Google Colab:\n\nfrom google.colab import files\n\n\n\nUpload a file\n\nuploaded = files.upload()\n\n\n\nAccess the uploaded file\n\nfor filename in uploaded.keys():\n    print(f'User uploaded file \"{filename}\" with size {len(uploaded[filename])} bytes.')#\n\n\n\nImportant Short Cuts in Google Colab\n\nRun cell and select next cell: Shift + Enter\nRun focused celll: ⌘/Ctrl + Enter\nInsert cell below: ⌘/Ctrl + M B\nInsert cell above: ⌘/Ctrl + M A\nDelete cell: ⌘/Ctrl + M D\nUndo last action: ⌘/Ctrl + M Z\nComment/Uncomment line: ⌘/Ctrl + Alt + M\nFind and replace: ⌘/Ctrl + H\nShow keyboard shortcuts: ⌘/Ctrl + K H",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Editors</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html",
    "href": "course/chapters/Python/Basics.html",
    "title": "Python Start",
    "section": "",
    "text": "Elementary Syntax\nDifficulty level:\nThis will be a very fast and basic start into coding with Python.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#commands",
    "href": "course/chapters/Python/Basics.html#commands",
    "title": "Python Start",
    "section": "Commands",
    "text": "Commands\nEach line of code is a command. The computer reads the code from top to bottom and executes each command in succession order.\nFirst some terminology:\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCommand\nA line of code that tells the computer to do something.\n\n\nSyntax\nThe rules that govern how commands are written.\n\n\nExecute\nTo run a command.\n\n\nComment\nA note in the code that is not executed.\n\n\nIndentation\nThe space at the beginning of a line of code that tells the computer that the line is part of a block of code.\n\n\nError\nA message that tells you that something is wrong with your code.\n\n\nStack trace\nA list of error messages that Python prints when an error occurs.\n\n\nVariable\nA name that stores a value.\n\n\nData type\nThe type of value that a variable stores.\n\n\nDeclaration\nThe process of assigning memory to a variable.\n\n\nInteger\nA whole number.\n\n\nFloat\nA number with a decimal point.\n\n\nBoolean\nA value that is either True or False.\n\n\nList\nA collection of items.\n\n\nString\nA sequence of characters.\n\n\nFunction\nA block of code that performs a specific task.\n\n\nArgument\nA value that is passed to a function.\n\n\nParameter\nA variable that is used in a function.\n\n\nReturn\nThe value that a function returns.\n\n\nKeyword Arguments\nArguments that are passed to a function by name.\n\n\nPositional Arguments\nArguments that are passed to a function by position.\n\n\nDefault Argument\nAn argument that has a default value.\n\n\nOperator\nA symbol that performs a specific operation.\n\n\nExpression\nA combination of values and operators that evaluates to a value.\n\n\nStatement\nA line of code that performs an action.\n\n\nBlock\nA group of statements that are executed together.\n\n\nMethod\nA function that is associated with an object.\n\n\nClass\nA blueprint for creating objects.\n\n\nObject\nAn instance of a class.\n\n\nInstance\nA specific object created from a class.\n\n\nMutable\nA type of object that can be changed.\n\n\nImmutable\nA type of object that cannot be changed.\n\n\nModule\nA collection of functions and variables.\n\n\nLibrary\nA collection of modules.\n\n\nPackage\nA collection of related modules.\n\n\nScript\nA file that contains code that can be run independently.\n\n\n\n\n\n\n\n\n\nExample - Hello World\n\n\n\nprint is a command that tells the computer to display the text or variable that follows it.\n\n\nTry it out:\n\nCopy the code below and paste it into a code cell.\nOr open this file in Google Colab and run the code.\nOr download this file and run it on your local machine.\n\n\nprint(\"Hello World\")\n\nHello World\n\n\nIf you want to span a command over multiple lines you can use \\ or  if you are using parenthesis in your command you can directly use a new line.\n\n result = 1 + 2 + 3 + \\\n          7 + 8 + 9\n numbers = [\n     1, 2, 3,\n     4, 5, 6,\n     7, 8, 9\n ]",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#indentation",
    "href": "course/chapters/Python/Basics.html#indentation",
    "title": "Python Start",
    "section": "Indentation",
    "text": "Indentation\nIn Python code blocks are not structured by brackets or semicolons like C/C++ or Java but by indentation. This means that the code inside a loop or a function is indented by a tab or spaces.\n\n\n\n\n\n\nWarning\n\n\n\nIndentations are crucial in Python. If you don’t indent your code correctly, you will get a typical beginner error.\nPay attention do not mix tabs and spaces in your code.\n\n\n\n\n\n\n\n\nExample - Wrong Indentation\n\n\n\n\n\n\nTry it out:\n\nprint(\"correct indentation\")\n    print(\"wrong indentation\")\n\n\n  Cell In[3], line 2\n    print(\"wrong indentation\")\n    ^\nIndentationError: unexpected indent\n\n\n\n\nAll the lines in the block must have the same indentation:\n\n    print(\"correct indentation\") \n    print(\"correct indentation\")\n    print(\"correct indentation\")\n\ncorrect indentation\ncorrect indentation\ncorrect indentation",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#error-messages",
    "href": "course/chapters/Python/Basics.html#error-messages",
    "title": "Python Start",
    "section": "Error messages",
    "text": "Error messages\nWhen you run a command that has an error, Python will print an error message.\nThe so-called stack trace. It is a list of error messages that Python prints when an error occurs.\nIt gives you information about the error and the location of the error in your code.\nThe stack track is read from bottom to top.\nThe last line contains the error message and the line number where the error occurred.\n\n\n\n\n\n\nTip\n\n\n\nOften the error messages are not very clear. You can search for the error message in the internet. Stackoverflow has a lot of answers to common errors. Or you can ask some AI e.g. ChatGPT.\n\n\n\n\n\n\n\n\nExample - Cryptic Error Message\n\n\n\nWhat does the error message tell you? If you are not sure ask the internet or your favorite AI.\n\n\n\na = [1,2,3]\nprint(a[3])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[5], line 2\n      1 a = [1,2,3]\n----&gt; 2 print(a[3])\n\nIndexError: list index out of range\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe error message tells you that you are trying to access an index that is out of range.\n\n\n\n\n\n\n\n\n\nPython counts from 0.\n\n\n\nThe first element is at index 0, the second element is at index 1, and so on.\ne.g. weekdays=[\"Monday\", \"Tuesday\", \"Wednesday\"] \n\n\n\nelements\n“Monday”\n“Tuesday”\n“Wednesday”\n\n\n\n\nindex\n0\n1\n2\n\n\n\nelement Tuesday is at index 1.  The length of the list is N.  The last element is at index N-1.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#comments",
    "href": "course/chapters/Python/Basics.html#comments",
    "title": "Python Start",
    "section": "Comments",
    "text": "Comments\nComments are important. They help you and others to understand your code.  You can use the # symbol to write comments.\nDocstrings are used to document the code for example with pydoc. They are using triple quotes `““” ““““`.\n\n# This is a comment\n\n\"\"\"\nThis is a documentation.\nYou can document your code for example by pydoc\n\"\"\"\n\n'\\nThis is a documentation.\\nYou can document your code for example by pydoc\\n'",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html",
    "href": "course/chapters/Python/Intermediate.html",
    "title": "Python Intermediate",
    "section": "",
    "text": "I/O (Input/Output)\nDifficulty level:\nOften we need with user input, files, system and paths. In this chapter we will cover these topics.\nYou can use the print() function to print a message to the screen.  You can use the input() function to get input from the user.  You can use the open() function to open a file.  You can use the write() function to write to a file.  You can use the read() function to read from a file.  You can use the close() function to close a file.  You can use the with statement to open a file and automatically close it when you are done.  You can use the os module to work with files and directories.  You can use the sys module to work with command line arguments.  You can use the argparse module to work also with command line arguments.\nThis should print “Hello World!” to the console\nprint(\"Hello World!\")\n\nHello World!\nThis should ask the user to enter a number and print it to the console\nprint(input(\"Enter a number: \"))\nThis should write “Hello World!” to the file “file.txt”\nopen(\"file.txt\", \"w\").write(\"Hello World!\") \n\n12\nThis should read the file “file.txt” and print the content to the console\nprint(open(\"file.txt\").read()) \n\nHello World!\nThis should print “Hello World!” to the console without a newline\nprint(\"Hello World without newline.\", end=\"\") \nprint(\"Next print statement.\")\n\nHello World without newline.Next print statement.\nThis should read the file “file.txt” and print the content to the console\nwith open(\"file.txt\", \"r\") as file: print(file.read()) \n\nHello World!\nWrite a file with the content “Hello World!” and close it\nfile = open(\"file.txt\", \"w\")\nfile.write(\"Hello World!\")\nfile.close()",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#if-statement",
    "href": "course/chapters/Python/Intermediate.html#if-statement",
    "title": "Python Intermediate",
    "section": "if statement",
    "text": "if statement\n\nx = 0\nif x &lt; 0:\n    print(\"x &lt; 0\")\nelif x &gt; 0:\n    print(\"x &gt; 0\")\nelse:\n    print(\"x = 0\")\n\nx = 0",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#breakcontinuepass",
    "href": "course/chapters/Python/Intermediate.html#breakcontinuepass",
    "title": "Python Intermediate",
    "section": "break,continue,pass",
    "text": "break,continue,pass\n\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nfor i in range(10):\n    if i == 5:\n        continue\n    print(i)\n\n0\n1\n2\n3\n4\n6\n7\n8\n9\n\n\n\nfor i in range(10):\n    if i == 5:\n        pass\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#for-loops",
    "href": "course/chapters/Python/Intermediate.html#for-loops",
    "title": "Python Intermediate",
    "section": "For Loops",
    "text": "For Loops\n\nfor i in range(5): #from 0 to 4 \n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nfor i in range(1,10,2): # start 1, stop 10 excluded, step 2\n    print(i)\n\n1\n3\n5\n7\n9\n\n\n\nl = list(range(0,10))\n\n\nl\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nfor i in l:  # using list\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#while-loops",
    "href": "course/chapters/Python/Intermediate.html#while-loops",
    "title": "Python Intermediate",
    "section": "While Loops",
    "text": "While Loops\n\nx = 0\nwhile x  &lt; 4:\n    print(l[x])\n    x = x + 1\n\n0\n1\n2\n3\n\n\n\nprint(\"while loop with continue and break statement\")\nn = 0\nwhile(n &lt; 10):\n    n+=1\n    if n == 5:\n        continue\n    if n == 7:\n        print(\"The loop reached 7 and will break now.\")\n        break\n    print(n)\n\nwhile loop with continue and break statement\n1\n2\n3\n4\n6\nThe loop reached 7 and will break now.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#functions",
    "href": "course/chapters/Python/Intermediate.html#functions",
    "title": "Python Intermediate",
    "section": "Functions",
    "text": "Functions\nFunctions are defined using the def keyword.  You can use the return keyword to return a value from a function.  The parameters of a function are defined in the parentheses.  Multiple parameters are separated by commas.  You can use default values for the parameter e.g. b=5.  Multiple return values are separated by commas.  They are stored in a tuple. \n\ndef summation(a,b=5):\n    return a+b, a-b\n\n\nsummation(4,2)\n\n(6, 2)\n\n\n\nsum, sub = summation(4)\nprint(sum)\nprint(sub)\n\n9\n-1\n\n\n\nx = 3\n\ndef multiple_return_value(x,a,b):\n    n = x+a\n    m = x-b\n    return [n,m]\nprint(multiple_return_value(x,5,10)[0],multiple_return_value(x,5,10)[1])\n\n8 -7",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html#style-guideline-for-writing-python-code",
    "href": "course/chapters/Python/Intermediate.html#style-guideline-for-writing-python-code",
    "title": "Python Intermediate",
    "section": "Style guideline for writing python code",
    "text": "Style guideline for writing python code\nFor writing a readable code, it is important to follow a style guideline.  The most common style guideline for Python is PEP 8.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html",
    "href": "course/chapters/Python/BasicsModules.html",
    "title": "Basic Modules",
    "section": "",
    "text": "Numpy and Pandas\nDifficulty level:",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html#numpy",
    "href": "course/chapters/Python/BasicsModules.html#numpy",
    "title": "Basic Modules",
    "section": "Numpy",
    "text": "Numpy\n\nNumpy is the most important library for Python.\nThe standard data types in Python are very slow and not very efficient for data analysis.\nNumpy is based mainly on C an C++.\nThis allows Numpy to be faster than plain Python.\nWith Numpy a new data type is introduced numpy arrays.\nNumpy arrays are multidimensional arrays that are much faster than Python lists.\nThe libary also includes many mathematical functions and methods for linear algebra.\n\nMore information can be found at the Numpy website.\nLoad the required libraries\n\nimport numpy as np\n\n\nNumpy Arrays\nAn array can be described as multidimensional lists. For example a matrix is a 2D array.\n\nmat = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(mat)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\nThe elements of an array can be accessed using the index of the element.\n\nprint(mat[0, 0])   # 1 element at row 0, column 0\n\n\nprint(mat[1, 2])  # 6 element at row 1, column 2\n\n\nprint(mat[:, 0])  # [1 4 7] all elements in column 0\n\n\nprint(mat[1, :])  # [4 5 6] all elements in row 1\n\n1\n6\n[1 4 7]\n[4 5 6]\n\n\n\nempty_mat = np.empty((3,3),dtype=float)\nprint(empty_mat)\n\n[[4.65458638e-310 0.00000000e+000 0.00000000e+000]\n [0.00000000e+000 0.00000000e+000 0.00000000e+000]\n [0.00000000e+000 0.00000000e+000 0.00000000e+000]]\n\n\n\nones_mat = np.ones((3, 3))\nprint(ones_mat)\n\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n\n\n\nzeros_mat = np.zeros((3, 3))\nprint(zeros_mat)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\narr = np.arange(1, 10, 2) # an array from 1 to 10 with a step of 2\nprint(arr)\n\n[1 3 5 7 9]\n\n\n\narr2 = np.linspace(0,100,5) # an array from 0 to 100 with 5 elements\nprint(arr2)\n\n[  0.  25.  50.  75. 100.]\n\n\n\nrand_mat = np.random.rand(3,3) # a 3x3 matrix with random numbers between 0 and 1\nprint(rand_mat)\n\n[[0.17517898 0.61758872 0.66864968]\n [0.48826749 0.92380655 0.69572028]\n [0.75517286 0.66864823 0.66819642]]\n\n\nOften you need to know the shape of an array. The shape of an array is a tuple that contains the number of elements in each dimension.\n\nprint(mat.shape) # (3, 3) 3 rows and 3 columns\nprint(mat.size)  # 9 total number of elements\nprint(mat.ndim)  # 2 number of dimensions\nprint(mat.dtype) # int64 data type of the elements\n\n(3, 3)\n9\n2\nint64",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html#pandas",
    "href": "course/chapters/Python/BasicsModules.html#pandas",
    "title": "Basic Modules",
    "section": "Pandas",
    "text": "Pandas\n\nPandas is a library for data manipulation and analysis.\nIt is built on top of Numpy.\nWith Pandas you are working with dataframes and not with arrays like in Numpy.\nDataframes are two-dimensional labeled data structures with columns of potentially different types.\nIt is like a table in a database or a spreadsheet. Pandas has a lot of methods to manipulate dataframes.\nYou can select subsets of the data, filter, sort, group, merge, join, etc.\nYou can statistically analyze the data, export the data to different file formats, but also plot the data with the help of matplotlib.\n\nMore information can be found under Pandas website.\n\nimport pandas as pd\n\n\nPanda DataFrames\nPanda DataFrames are two-dimensional labeled data structures with columns of potentially different types like a table.\n\ndata = {\n    \"Name\": [\"Water\", \"Oxygen\", \"Hydrogen\", \"Carbon Dioxide\", \"Methane\", \"Ammonia\", \"Nitrogen\", \"Sulfur Dioxide\"],\n    \"Formula\": [\"H2O\", \"O2\", \"H2\", \"CO2\", \"CH4\", \"NH3\", \"N2\", \"SO2\"],\n    \"Molar Mass (g/mol)\": [18.015, 32.00, 2.016, 44.01, 16.04, 17.03, 28.013, 64.07]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n             Name Formula  Molar Mass (g/mol)\n0           Water     H2O              18.015\n1          Oxygen      O2              32.000\n2        Hydrogen      H2               2.016\n3  Carbon Dioxide     CO2              44.010\n4         Methane     CH4              16.040\n5         Ammonia     NH3              17.030\n6        Nitrogen      N2              28.013\n7  Sulfur Dioxide     SO2              64.070\n\n\n\n\nPanda DataSeries\nPanda DataSeries are one-dimensional labeled arrays.\n\nseries_data = pd.Series([1, 2, 3, 4, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nprint(series_data)\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html",
    "href": "course/chapters/Python/AdvancedModules.html",
    "title": "Advanced Modules",
    "section": "",
    "text": "Advanced Modules\nDifficulty level:",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#scipy",
    "href": "course/chapters/Python/AdvancedModules.html#scipy",
    "title": "Advanced Modules",
    "section": "Scipy",
    "text": "Scipy\nScipy is a library that builds on Numpy. It is specialized in scientific computing. It includes modules for statistical calculations, optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and more.\nMore information can be found at the Scipy website.\nLater on we will see how to use Scipy for statistical calculations and for analysis of spectra.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#scikit-learn",
    "href": "course/chapters/Python/AdvancedModules.html#scikit-learn",
    "title": "Advanced Modules",
    "section": "Scikit-learn",
    "text": "Scikit-learn\nScikit-learn is a library for machine learning. It includes modules for classification, regression, clustering, dimensionality reduction, model selection and preprocessing.\nMore information can be found at the Scikit-learn website.\nLater on we will see how to use Scikit-learn for classification and regression.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#seaborn",
    "href": "course/chapters/Python/AdvancedModules.html#seaborn",
    "title": "Advanced Modules",
    "section": "Seaborn",
    "text": "Seaborn\nSeaborn is a library for data visualization. It is based on Matplotlib and provides a high-level interface for drawing attractive and informative statistical graphics.\nMore information can be found at the Seaborn website.\nLater on we will see how to use Seaborn for data visualization.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html",
    "href": "course/chapters/DHP/DataCollection.html",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "",
    "text": "More information\nDifficulty level:",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#what-is-data",
    "href": "course/chapters/DHP/DataCollection.html#what-is-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "What is data?",
    "text": "What is data?\nData is a collection of\n\nnumbers\nwords\nmeasurements\nobservations\n\nIf you work with data, please have the FAIR principles in mind:\n\nFindable\nAccessible\nInteroperable\nReusable\n\nsee also the UIBK FAIR Principles",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-get-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-get-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you get data?",
    "text": "How do you get data?\nAt the beginning of every data science project, you need to collect data.\nData can be collected from various sources, such as:\n\nexperimental measurements\ncalculations and simulations\nsurveys\nexisting databases\nweb scraping\nAPIs\n\netc.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-store-the-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-store-the-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you store the data?",
    "text": "How do you store the data?\nThis data can be stored in different locations, such as:\n\nlocal files\ndatabases\ncloud storages\ncluster storages\nweb servers\netc.\n\nIf you measure data yourself, either your measurement device will store the data in a specific format, or you will have to store it yourself. Sometimes you have to transform the data into a different format to be able to work with it.\nIf you use data from external sources, you have to make sure that you have legal rights to use the data. Check the data license and terms of use.\nFurther, make sure how the data is stored and how you can access it.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-access-external-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-access-external-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you access external data?",
    "text": "How do you access external data?\nTo access data from external sources, you can use different tools and libraries.\nFor example, if you want to access data from a chemical database e.g. \n\nMaterials Project, you can use the pymatgen library. (Requires API key, which can be obtained by registering on the website.)\nRCSB PDB Protein Data Bank, you can use the rcsb library.\nPubChem, you can use the pubchempy library.\n\n… and many more.\nIf you use APIs please read the documentation of the API to understand how to access the data. And make sure to respect the API usage policy and database terms of use.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#which-data-formats-do-you-use",
    "href": "course/chapters/DHP/DataCollection.html#which-data-formats-do-you-use",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "Which data formats do you use?",
    "text": "Which data formats do you use?\nData can be stored in various data formats, such as:\n\nPlain Text files (e.g. CSV, DAT, TXT)\nText files with structure (e.g. JSON, XML)\nSpreadsheet files\nBinary files (e.g. HDF5, Parquet, NetCDF,Feather, Pickle, npy, etc.)\nDatabases\nchemical and molecular data formats (e.g. XYZ, CIF, PDB, etc.) etc.\n\n\nPlain Text Files\nA text file often contains a header with the names of the columns and then the data in rows. Columns can be separated by different delimiters (spaces, ,, ;, tabs, …).  For example, a file with data from an experiment could look like this:\nTime/s  Temperature/°C\n0         20\n10        21\n...     ...\n\n\nJSON or XML\nJSON stands for JavaScript Object Notation. Data is structured in a key-value format, so that both humans and machines can read it easily.  For example, a JSON file with the same data as above could look like this:\n{\n  \"data\": [\n    {\"Time\": 0, \"Temperature\": 20},\n    {\"Time\": 10, \"Temperature\": 21},\n    ...\n  ]\n}\nXML stands for Extensible Markup Language. It is also designed to be both human-readable and machine-readable.  For example, an XML file with the same data as above could look like this:\n&lt;data&gt;\n  &lt;measurement&gt;\n    &lt;Time&gt;0&lt;/Time&gt;\n    &lt;Temperature&gt;20&lt;/Temperature&gt;\n  &lt;/measurement&gt;\n  &lt;measurement&gt;\n    &lt;Time&gt;10&lt;/Time&gt;\n    &lt;Temperature&gt;21&lt;/Temperature&gt;\n  &lt;/measurement&gt;\n  ...\n\n\nBinary Files\nData is stored in a binary format and can be read with specific libraries. It is often used for large datasets, as it is more efficient than plain text files. The computer is able to read and write binary files faster than text files.  Some common binary file formats are:\n\nHDF5: Hierarchical Data Format, used for large datasets\nParquet: Columnar storage format, used for big data\nNetCDF: Network Common Data Form, array-orriented, often for geoscience data\nFeather: a fast column-based serialization for data frames, initially designed for R and Python, helps to share data between languages\nPickle: Python-specific format, used for serializing Python objects\nnpy: Numpy-specific format, used for saving numpy arrays\n\nHere is a list of comparison of binary file formats: Comparison of data serialization formats\n\n\nDatabases\nDatabases are designed for big data storage. The advantage of databases is that they can be queried and updated easily. There are different types of databases, such as:\n\nSQL databases (e.g. SQLite, MySQL, PostgreSQL)\nNoSQL databases (e.g. MongoDB)\nGraph databases\nTime series databases\n\n\n\nChemical and Molecular Data Formats\nThere are specific data formats for chemical and molecular data, such as:\n\nXYZ: Cartesian coordinates of atoms\nCIF: Crystallographic Information File, used for crystallographic data\nPDB: Protein Data Bank, used for protein structures\nMOL: Molecule file format, used for chemical structures\nSDF: Structure Data File, used for chemical structures\nSMILES: Simplified Molecular Input Line Entry System, used for chemical structures\nInChI: International Chemical Identifier, used for chemical structures\nnmrML: Nuclear Magnetic Resonance Markup Language, used for NMR data\nNMReDATA: Nuclear Magnetic Resonance Electronic Data Aggregation, used for NMR data\nJCAMP-DX: Joint Committee on Atomic and Molecular Physical Data, used for spectroscopy data\nmzML: Mass Spectrometry Markup Language, used for mass spectrometry data\naniml: Analytical Information Markup Language, used for analytical data\nFASTA: used for DNA and protein sequences\n\netc.\n\n\nOther Data Formats\n\nImages: Images can be stored in different formats, such as JPEG, PNG, TIFF, BMP, GIF, etc.\nAudio: Audio files can be stored in different formats, such as MP3, WAV, FLAC, etc.\nVideo: Video files can be stored in different formats, such as MP4, AVI, MOV, etc.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#what-type-of-data-do-you-have",
    "href": "course/chapters/DHP/DataCollection.html#what-type-of-data-do-you-have",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "What type of data do you have?",
    "text": "What type of data do you have?\nNext what do you have to consider is the types of your data.\nDepending on the type of data analysis could be different.\n\nNumerical Data\nNumerical data is data that is expressed with numbers. It can be further divided into two types:\n\nDiscrete: Data that can only take certain values (e.g. integers)\nContinuous: Data that can take any value within a certain range (e.g. real numbers)\n\nFor example,\n\nmeasured temperature over time: continuous\nnumber of chemical substances, which are measured: discrete\n\n\n\nCategorical Data\nCategorial means that data is divided into categories. It can be further divided into two types:\n\nOrdinal: Data that has a specific order or ranking\nNominal: Data that has no specific order or ranking\n\nFor example,\n\nblood type: nominal (A, B, AB, O)\nchemical function group: nominal (alcohol, ketone, aldehyde, carboxylic acid, etc.)\npurity of a substance: ordinal (low, medium, high)\nhardness of a material: ordinal (soft, medium, hard)\n\n\n\n\n\n\n\n\nQuiz",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html",
    "href": "course/chapters/DHP/SimpleDataImport.html",
    "title": "Simple Data Import",
    "section": "",
    "text": "Data Reading\nDifficulty level:",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#how-do-you-read-the-data",
    "href": "course/chapters/DHP/SimpleDataImport.html#how-do-you-read-the-data",
    "title": "Simple Data Import",
    "section": "How do you read the data?",
    "text": "How do you read the data?\nDepending on the data format, you can use different libraries to read the data.\n\nReading Plain Text Files\nYou can use the pandas or numpy library to read CSV files.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#pandas",
    "href": "course/chapters/DHP/SimpleDataImport.html#pandas",
    "title": "Simple Data Import",
    "section": "Pandas",
    "text": "Pandas\nPandas is read function is quite fast and can read large files. The advantage is that different data types can be read in the same file. The reading functions return a DataFrame object.\nPandas has different functions to read different file formats.\n\npandas.read_csv() function is can read CSV files.\npandas.read_table() function is can read general delimiter files.\npandas.read_fwf() function is can read fixed-width files.\n\nMostly used function is pandas.read_csv() because you can specify the delimiter, header, and other options.\n\nimport pandas as pd\n\ndata = pd.read_csv(temperature_data)\ndata\n\n\n\n\n\n\n\n\ntime;temperature\n\n\n\n\n0\n1;303.073024218\n\n\n1\n2;302.951624807\n\n\n2\n3;302.831229733\n\n\n3\n4;302.73615227\n\n\n4\n5;302.708880354\n\n\n...\n...\n\n\n44635\n44636;296.102947663\n\n\n44636\n44637;296.173110138\n\n\n44637\n44638;296.140000813\n\n\n44638\n44639;296.169289777\n\n\n44639\n44640;296.287904152\n\n\n\n\n44640 rows × 1 columns\n\n\n\nThe data has a different delimiter than the default comma. You can specify the delimiter using the sep parameter.\n\ndata = pd.read_csv(temperature_data, sep=';')\ndata\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n0\n1\n303.073024\n\n\n1\n2\n302.951625\n\n\n2\n3\n302.831230\n\n\n3\n4\n302.736152\n\n\n4\n5\n302.708880\n\n\n...\n...\n...\n\n\n44635\n44636\n296.102948\n\n\n44636\n44637\n296.173110\n\n\n44637\n44638\n296.140001\n\n\n44638\n44639\n296.169290\n\n\n44639\n44640\n296.287904\n\n\n\n\n44640 rows × 2 columns\n\n\n\nNow the data is read correctly. The header is already taken from the first row. If you want to specify the header, you can use the header parameter.\n\nimport pandas as pd\ndf = pd.read_csv('file.csv', header=None) # No header\ndf = pd.read_csv('file.csv', header=0) # Header is in the first row\ndf = pd.read_csv('file.csv', header=1) # Header is in the second row\n\n\ndata = pd.read_csv(temperature_data, sep=';', header=0)\ndata\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n0\n1\n303.073024\n\n\n1\n2\n302.951625\n\n\n2\n3\n302.831230\n\n\n3\n4\n302.736152\n\n\n4\n5\n302.708880\n\n\n...\n...\n...\n\n\n44635\n44636\n296.102948\n\n\n44636\n44637\n296.173110\n\n\n44637\n44638\n296.140001\n\n\n44638\n44639\n296.169290\n\n\n44639\n44640\n296.287904\n\n\n\n\n44640 rows × 2 columns\n\n\n\nIf your data contains whitespace, you can use the skipinitialspace parameter to remove initial whitespaces.\n\ndata = pd.read_csv(temperature_dat, skipinitialspace=True, sep=\" \")\ndata\n\n\n\n\n\n\n\n\n1\n303.073024218\n\n\n\n\n0\n2\n302.951625\n\n\n1\n3\n302.831230\n\n\n2\n4\n302.736152\n\n\n3\n5\n302.708880\n\n\n4\n6\n302.647462\n\n\n...\n...\n...\n\n\n44634\n44636\n296.102948\n\n\n44635\n44637\n296.173110\n\n\n44636\n44638\n296.140001\n\n\n44637\n44639\n296.169290\n\n\n44638\n44640\n296.287904\n\n\n\n\n44639 rows × 2 columns\n\n\n\nNow the data has no header. You can specify the header using the names parameter.\n\ndata = pd.read_csv(temperature_dat, sep=' ', skipinitialspace=True,header=1,names=['t', 'T'])\n# important to set header=None, otherwise the first line is used as header\ndata\n\n\n\n\n\n\n\n\nt\nT\n\n\n\n\n0\n3\n302.831230\n\n\n1\n4\n302.736152\n\n\n2\n5\n302.708880\n\n\n3\n6\n302.647462\n\n\n4\n7\n302.513749\n\n\n...\n...\n...\n\n\n44633\n44636\n296.102948\n\n\n44634\n44637\n296.173110\n\n\n44635\n44638\n296.140001\n\n\n44636\n44639\n296.169290\n\n\n44637\n44640\n296.287904\n\n\n\n\n44638 rows × 2 columns\n\n\n\nYou see that also not .csv files can be read with the read_csv() function.\nThe read_csv() function has a lot of parameters. Look in the documentation. You can see which parameters you can set https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nFor example,\n\ndelimiter parameter can be used to specify the delimiter instead of sep. Both are the same. Default is ,.\nheader parameter can be used to specify the header row. Default is inferred from the file.\nskipinitialspace parameter can be used to remove initial whitespaces. Default is False.\nnames parameter can be used to specify the column names.\nskiprows parameter can be used to skip rows at the beginning of the file.\nskipfooter parameter can be used to skip rows at the end of the file.\nnrows parameter can be used to read only a specific number of rows.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\nna_values parameter can be used to specify the missing values.\nkeep_default_na parameter can be used to specify if the default missing values should be kept. Default is True.\nna_filter parameter can be used to recognize missing values without NA or NaN values. Default is True.\ntrue_values parameter can be used to specify the values that should be recognized as True.\nfalse_values parameter can be used to specify the values that should be recognized as False.\nparse_dates parameter can be used to parse dates. Default is False.\n\n\nSome examples are:\n\ndata = pd.read_csv(temperature_data, sep=';', header=0, names=['t', 'T'], skiprows=1)\n\nNow the first row is skipped, only 44638 rows are read instead of 44639.\nMissing value examples:\n\ndata = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=False)\nprint(data.loc[20:26]) #print some rows to see the NaN values\n\n     t              T\n20  21  302.020507467\n21  22               \n22  23  301.845408096\n23  24  301.833550446\n24  25  301.785933229\n25  26  301.846169501\n26  27  301.779994697\n\n\nIf the na_filteris set to False, the missing values are not recognized. But if it set on True, the missing values are recognized.\n\ndata = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=True)\nprint(data.loc[20:26]) #print some rows to see the NaN values\n\n     t           T\n20  21  302.020507\n21  22         NaN\n22  23  301.845408\n23  24  301.833550\n24  25  301.785933\n25  26  301.846170\n26  27  301.779995",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#numpy",
    "href": "course/chapters/DHP/SimpleDataImport.html#numpy",
    "title": "Simple Data Import",
    "section": "Numpy",
    "text": "Numpy\nNumpy has two main functions to read text files. - numpy.loadtxt() function is used to read text files. - numpy.genfromtxt() function is used to read text files with missing values.\nIn comparison to the pandas library, the numpy library is slower and can not read different data types in the same file. So you can not read a file with strings and numbers in the same file.\nIf you try to read a file with a header row, you will get an error.\n\nimport numpy as np\ndata = np.loadtxt(temperature_data, delimiter=';')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nValueError: could not convert string to float: 'time'\n\nThe above exception was the direct cause of the following exception:\n\nValueError                                Traceback (most recent call last)\nCell In[10], line 2\n      1 import numpy as np\n----&gt; 2 data = np.loadtxt(temperature_data, delimiter=';')\n\nFile /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1046, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1043     data = _preprocess_comments(data, comments, encoding)\n   1045 if read_dtype_via_object_chunks is None:\n-&gt; 1046     arr = _load_from_filelike(\n   1047         data, delimiter=delimiter, comment=comment, quote=quote,\n   1048         imaginary_unit=imaginary_unit,\n   1049         usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n   1050         converters=converters, dtype=dtype,\n   1051         encoding=encoding, filelike=filelike,\n   1052         byte_converters=byte_converters)\n   1054 else:\n   1055     # This branch reads the file into chunks of object arrays and then\n   1056     # casts them to the desired actual dtype.  This ensures correct\n   1057     # string-length and datetime-unit discovery (like `arr.astype()`).\n   1058     # Due to chunking, certain error reports are less clear, currently.\n   1059     if filelike:\n\nValueError: could not convert string 'time' to float64 at row 0, column 1.\n\n\n\nYou can specify the header row using the skiprows parameter. If you want to skip one row, the skiprows=1 parameter is set at 1.\n\ndata = np.loadtxt(temperature_data, delimiter=';', skiprows=1)\ndata\n\narray([[1.00000000e+00, 3.03073024e+02],\n       [2.00000000e+00, 3.02951625e+02],\n       [3.00000000e+00, 3.02831230e+02],\n       ...,\n       [4.46380000e+04, 2.96140001e+02],\n       [4.46390000e+04, 2.96169290e+02],\n       [4.46400000e+04, 2.96287904e+02]], shape=(44640, 2))\n\n\nNow the data is read correctly. You can see that the data is read as a numpy array and not as a DataFrame. This can be a disadvantage if you want to use the data as a DataFrame but an advantage if you want to use numpy functions to process the data.\n\nIf you have data with whitespace, you do not need to specify the delimiterparamter because the default is whitespace.\n\ndata = np.loadtxt(temperature_dat)\ndata\n\narray([[1.00000000e+00, 3.03073024e+02],\n       [2.00000000e+00, 3.02951625e+02],\n       [3.00000000e+00, 3.02831230e+02],\n       ...,\n       [4.46380000e+04, 2.96140001e+02],\n       [4.46390000e+04, 2.96169290e+02],\n       [4.46400000e+04, 2.96287904e+02]], shape=(44640, 2))\n\n\ngenfromtxt() gives you more flexibility to read files with missing values.\nFirst using the loadtxt() function, you get an error because of the missing values.\n\ndata = np.loadtxt(temperature_nan_dat)\ndata\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 data = np.loadtxt(temperature_nan_dat)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:1046, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1043     data = _preprocess_comments(data, comments, encoding)\n   1045 if read_dtype_via_object_chunks is None:\n-&gt; 1046     arr = _load_from_filelike(\n   1047         data, delimiter=delimiter, comment=comment, quote=quote,\n   1048         imaginary_unit=imaginary_unit,\n   1049         usecols=usecols, skiplines=skiplines, max_rows=max_rows,\n   1050         converters=converters, dtype=dtype,\n   1051         encoding=encoding, filelike=filelike,\n   1052         byte_converters=byte_converters)\n   1054 else:\n   1055     # This branch reads the file into chunks of object arrays and then\n   1056     # casts them to the desired actual dtype.  This ensures correct\n   1057     # string-length and datetime-unit discovery (like `arr.astype()`).\n   1058     # Due to chunking, certain error reports are less clear, currently.\n   1059     if filelike:\n\nValueError: the number of columns changed from 2 to 1 at row 22; use `usecols` to select a subset and avoid this error\n\n\n\nIf you try to read the file with an empty entrance at row 22, you wil get still an error with the genfromtxt() function.\n\ndata = np.genfromtxt(temperature_nan_dat)\ndata\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 data = np.genfromtxt(temperature_nan_dat)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:2333, in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\n   2331 # Raise an exception ?\n   2332 if invalid_raise:\n-&gt; 2333     raise ValueError(errmsg)\n   2334 # Issue a warning ?\n   2335 else:\n   2336     warnings.warn(errmsg, ConversionWarning, stacklevel=2)\n\nValueError: Some errors were detected !\n    Line #22 (got 1 columns instead of 2)\n\n\n\nBut why? What do you think is the reason for the error?\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThe reason is that the genfromtxt() function expects the same number of columns in each row. The delimiter is set default to whitespace. But if you have a missing value, the function expects a value. An error is raised because at row 22 the function is detecting only one column due to the missing value.\n\n\n\nHow can you solve this problem?\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou can NOT solve this problem with the genfromtxt() function if you have missing values and delimiter is whitespace. Either you have to fill the missing value with a value or you have to use the pandas library.\n\n\n\nIf you have not whitespace as delimiter, you can use the genfromtxt() function with missing values.\n\ndata = np.genfromtxt(temperature_nan_data,delimiter=';')\ndata[20:26] # print some rows to see the NaN values\n\narray([[ 21.        , 302.02050747],\n       [ 22.        ,          nan],\n       [ 23.        , 301.8454081 ],\n       [ 24.        , 301.83355045],\n       [ 25.        , 301.78593323],\n       [ 26.        , 301.8461695 ]])\n\n\nThe different parameters that can be set are for loadtxt() function:\n(see documentation https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html\n\ndelimiter parameter can be used to specify the delimiter. Default is whitespace.\nskiprows parameter can be used to skip rows at the beginning of the file.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\ncomments parameter can be used to specify the comment character. Default is #.\nmax_rows parameter can be used to read only a specific number of rows after skipping rows.\nunpack parameter can be used to unpack the columns, so each column is returned as a separate array.\n\nand for genfromtxt() function\n(see documentation https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html#numpy.genfromtxt)\n\ndelimiter parameter can be used to specify the delimiter. Default is whitespace.\nskip_header parameter can be used to skip rows at the beginning of the file.\nskip_footer parameter can be used to skip rows at the end of the file.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\ncomments parameter can be used to specify the comment character. Default is #.\nmax_rows parameter can be used to read only a specific number of rows after skipping rows.\nunpack parameter can be used to unpack the columns, so each column is returned as a separate array.\nmissing_values parameter can be used to specify which values should be recognized as missing values.\nfilling_values parameter can be used to specify the filling values for the missing values.\nusemask parameter can be used to return a masked array with missing values.\nnames parameter can be used to specify the column names. If names=True, the column names are read from the first row.\nreplace_space parameter can be used to replace spaces in the column names. Default is _.\n\netc.\n\n\n\n\n\n\nImportant\n\n\n\nThe pandas library is faster and more flexible than the numpy library. Choose wisely which library you want to use. It depends on the data format, the data type and what kind of processing you want to do.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#numpy-1",
    "href": "course/chapters/DHP/SimpleDataImport.html#numpy-1",
    "title": "Simple Data Import",
    "section": "Numpy",
    "text": "Numpy\nNumpy has one function to write text files.\n\nnumpy.savetxt() function is used to write text files.\n\n\nnp.savetxt('file.txt', data)\n\n\n\n\n\n\n\nProblems\n\n\n\nIf you have problems with importing data please check the following:\n\nHave first a look at the data file.\nCheck the delimiter.\nCheck the header.\nCheck the missing values.\nCheck the data type.\nCheck the encoding.\nCheck the file format.\nCheck the file path.\nCheck the file name.\n\nYou can use AI tool to get a code snippet for reading the data.\n\ne.g. give the AI tool the one or two line of the as an example and ask for the code snippet.\ne.g. beware do that only if you do not have sensitive data.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataInspection.html",
    "href": "course/chapters/DHP/SimpleDataInspection.html",
    "title": "Simple Data Inspection",
    "section": "",
    "text": "Inspection of Data\nDifficulty level:\nAfter you load your data you have to inspect it to:",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simple Data Inspection</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataInspection.html#inspection-of-data",
    "href": "course/chapters/DHP/SimpleDataInspection.html#inspection-of-data",
    "title": "Simple Data Inspection",
    "section": "",
    "text": "check if no data is consistent, no missing values\ncheck if the data is in the correct format\ncheck if the data is in the correct range\ncheck if the data is in the correct distribution\nget first insights into the data\n\n\nNumpy\n\nOverview of the Data\n\nimport numpy as np\ndata = np.loadtxt(temperature_data, delimiter=';', skiprows=1)\n\nFirst of all you can use the print() function to get a quick overview of the data.\n\nprint(data)\n\n[[1.00000000e+00 3.03073024e+02]\n [2.00000000e+00 3.02951625e+02]\n [3.00000000e+00 3.02831230e+02]\n ...\n [4.46380000e+04 2.96140001e+02]\n [4.46390000e+04 2.96169290e+02]\n [4.46400000e+04 2.96287904e+02]]\n\n\nIn numpy you can use the shape attribute to get the shape of the data.\n\nprint(data.shape)\n\n(44640, 2)\n\n\nThe data has 44640 rows and 2 columns.\n\n\nData Type Conversion\nYou can use the dtype attribute to get the data type of the data.\n\nprint(data.dtype)\n\nfloat64\n\n\nThe data type is float64.\nYou can transform the data to a float64 data type if it is not already in this format.\n\ndata = data.astype('float64')\n\n\n\nMissing Data and Corrupted Data\nYou can use the isnan() function to check if there are missing values in the data.\n\nprint(np.isnan(data).sum())\n\n0\n\n\n\n\n\nPandas\n\nOverview of the Data\nYou can use the head() function to get a quick overview of the first rows of the data.\n\nimport pandas as pd\n\ndata = pd.read_csv(temperature_data,sep=';')\ndata.head()\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n0\n1\n303.073024\n\n\n1\n2\n302.951625\n\n\n2\n3\n302.831230\n\n\n3\n4\n302.736152\n\n\n4\n5\n302.708880\n\n\n\n\n\n\n\nThe describe() function gives you a quick overview of the data distribution.\n\ndata.describe()\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\ncount\n44640.000000\n44640.000000\n\n\nmean\n22320.500000\n297.937566\n\n\nstd\n12886.602345\n6.080222\n\n\nmin\n1.000000\n278.814670\n\n\n25%\n11160.750000\n293.842755\n\n\n50%\n22320.500000\n297.875930\n\n\n75%\n33480.250000\n301.879560\n\n\nmax\n44640.000000\n316.006103\n\n\n\n\n\n\n\nThe describefunction shows you the count, mean, standard deviation, minimum, 25%, 50%, 75% and maximum values of the data.\nIn this case, data has 44640 data points, The mean of the temperature is 298(6) K. The minimum temperature is 279 K and the maximum temperature is 316 K. Further the 25% quantile is 294 K, the 50% quantile is 298 K and the 75% quantile is 302 K. The measurement was taken from 1 to 44640 seconds which is 12 hours and 24 minutes. We suppose that is the correct time range which was to be expected.\nThis gives you a quick overview of the data distribution.\n\n\nMissing Data and Corrupted Data\nTo check if there is missing data in the data set you can use the isna() function.\n\ndata.isna().sum()\n\ntime           0\ntemperature    0\ndtype: int64\n\n\nNo missing data is found in this case.\nYou can check the data type using dtypes function to check if the data is in the correct format.\n\ndata.dtypes\n\ntime             int64\ntemperature    float64\ndtype: object\n\n\n\ndata.dtypes\n\ntime             int64\ntemperature    float64\ndtype: object\n\n\nYou see that time is an int64 and temperature is a float64. For the analysis, you might want to convert the time to a float64 as well.\n\ndata['time'] = data['time'].astype('float64')\n\n\ndata['time'] = data['time'].astype('float64')\ndata.dtypes\n\ntime           float64\ntemperature    float64\ndtype: object\n\n\nIf we have missing data we can use the fillna() function to fill the missing data with a specific value.\n\ndata.fillna(0)\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n0\n1.0\n303.073024\n\n\n1\n2.0\n302.951625\n\n\n2\n3.0\n302.831230\n\n\n3\n4.0\n302.736152\n\n\n4\n5.0\n302.708880\n\n\n...\n...\n...\n\n\n44635\n44636.0\n296.102948\n\n\n44636\n44637.0\n296.173110\n\n\n44637\n44638.0\n296.140001\n\n\n44638\n44639.0\n296.169290\n\n\n44639\n44640.0\n296.287904\n\n\n\n\n44640 rows × 2 columns\n\n\n\n\ndata_missing = pd.read_csv(temperature_nan_data, header=None, skipinitialspace=True, sep=' ', names=['time', 'temperature'])\ndata_missing.head()\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n0\n1\n303.073024\n\n\n1\n2\n302.951625\n\n\n2\n3\n302.831230\n\n\n3\n4\n302.736152\n\n\n4\n5\n302.708880\n\n\n\n\n\n\n\nOne value is missing in the temperature column. We fill it with 0.\nFirst let check where the data is missing.\n\ndata[data['temperature'].isna()]\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n\n\n\n\n\nAt index 21 at time 22 s the temperature is missing.\n\n\n\n\n\n\nNote\n\n\n\nThe handling of missing data is a complex topic. First of all you have to check why the data is missing. Is it a measurement error, a data processing error etc.\nYou have to decide if you want to fill the missing data with a specific value, drop the row or column or interpolate the missing data. The decision depends on the data and the analysis you want to perform. Dropping Data is always a delicate decision because you loose information. Sometimes it is not good scientific practice to drop data. For more information there a lot of research in this topic https://doi.org/10.1076/edre.7.4.353.8937\n\n\nThe time step can be estimated by the difference between the time steps of the previous and the next data point.\n\ndata['time'].diff()\n\n0        NaN\n1        1.0\n2        1.0\n3        1.0\n4        1.0\n        ... \n44635    1.0\n44636    1.0\n44637    1.0\n44638    1.0\n44639    1.0\nName: time, Length: 44640, dtype: float64\n\n\nAnd we can summarize it via:\n\ndata['time'].diff().value_counts()\n\ntime\n1.0    44639\nName: count, dtype: int64\n\n\n\ndata_missing['time'].diff().value_counts()\n\ntime\n1.0    44639\nName: count, dtype: int64\n\n\nget difference between temperature values\n\ndata_missing[10:30].diff()\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n10\nNaN\nNaN\n\n\n11\n1.0\n0.000728\n\n\n12\n1.0\n-0.062319\n\n\n13\n1.0\n-0.130198\n\n\n14\n1.0\n-0.060203\n\n\n15\n1.0\n0.003234\n\n\n16\n1.0\n0.048911\n\n\n17\n1.0\n-0.042025\n\n\n18\n1.0\n0.021264\n\n\n19\n1.0\n0.033401\n\n\n20\n1.0\n0.054579\n\n\n21\n1.0\nNaN\n\n\n22\n1.0\nNaN\n\n\n23\n1.0\n-0.011858\n\n\n24\n1.0\n-0.047617\n\n\n25\n1.0\n0.060236\n\n\n26\n1.0\n-0.066175\n\n\n27\n1.0\n-0.080531\n\n\n28\n1.0\n-0.029080\n\n\n29\n1.0\n0.026428\n\n\n\n\n\n\n\nThe time step is constantly 1 second. The difference between the temperature of the previous and the next data point is at \\(~10^{-2}\\) order. We can assume that in this case the data is consistent enough and we can fill the missing data with the mean of the previous and the next data point.\n\ndata['temperature'].fillna((data['temperature'].shift() + data['temperature'].shift(-1))/2, inplace=True)\n\n\ndata_missing['temperature'].fillna((data_missing['temperature'].shift() + data_missing['temperature'].shift(-1))/2, inplace=True)\ndata_missing[20:25]\n\n\n\n\n\n\n\n\ntime\ntemperature\n\n\n\n\n20\n21\n302.020507\n\n\n21\n22\n301.932958\n\n\n22\n23\n301.845408\n\n\n23\n24\n301.833550\n\n\n24\n25\n301.785933\n\n\n\n\n\n\n\nNow can analysis or plot the data.\n\n\nData Type Conversion\nYou get the data type of the data with the dtypes function.\n\ndata.dtypes\n\ntime           float64\ntemperature    float64\ndtype: object\n\n\nYou can convert the data type with the astype() function.\n\ndata['time'] = data['time'].astype('float64')",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simple Data Inspection</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataInspection.html#example-data-import-and-inspection",
    "href": "course/chapters/DHP/SimpleDataInspection.html#example-data-import-and-inspection",
    "title": "Simple Data Inspection",
    "section": "Example: Data Import and Inspection",
    "text": "Example: Data Import and Inspection",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Simple Data Inspection</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html",
    "href": "course/chapters/Plots/DataVisualization.html",
    "title": "Introduction into Data Visualization",
    "section": "",
    "text": "Data Visualization\nDifficulty level:\nVisualization of data helps us to understand the data better and communicate the results. It is easier to understand a graph than a table of numbers.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html#which-technique-should-you-use",
    "href": "course/chapters/Plots/DataVisualization.html#which-technique-should-you-use",
    "title": "Introduction into Data Visualization",
    "section": "Which technique should you use?",
    "text": "Which technique should you use?\n\nHow many variables do you have and how many do you want to compare at once?\nWhat type of data do you have?\nWhat type of relationship are you trying to represent?\nDo you want to analyze the distribution of the data?\nDo you want to analyze the correlation between the variables?\nDo you want to show ranking or ordering of the data?\nDo you want to a trend over time?\nDo you want to show the composition of the data?\nDo you want to predict the future values of the data?\nDo you want to show the relationship between the variables?\n\n\nAdditional resources\nA good handbook for data visualization in Python is the book “Python Data Science Handbook” by Jake VanderPlas",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html#how-you-should-visualize-data",
    "href": "course/chapters/Plots/DataVisualization.html#how-you-should-visualize-data",
    "title": "Introduction into Data Visualization",
    "section": "How you should visualize data?",
    "text": "How you should visualize data?\n\nA graphic should have a suitable font size.\nChoose colors that are easy to distinguish also for colorblind people (e.g. colorbrewer.\nThe axis ticks and limits should be chosen in a way that the data is easy to read.\nAxis should be labeled and physical units should be not forgotten.\nThe graphic should be not overloaded with information.\nHave always in mind who is the target audience of your graphic and where the graphic will be presented e.g. in a scientific paper, in a presentation or in a poster.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlots.html",
    "href": "course/chapters/Plots/SimplePlots.html",
    "title": "Basic Data Visualization Techniques",
    "section": "",
    "text": "Basic Data Visualization Technique\nDifficulty level:\nThe most popular data visualization libraries in Python is Matplotlib. Let`s start with the basic data visualization techniques using Matplotlib.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Basic Data Visualization Techniques</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlots.html#basic-data-visualization-technique",
    "href": "course/chapters/Plots/SimplePlots.html#basic-data-visualization-technique",
    "title": "Basic Data Visualization Techniques",
    "section": "",
    "text": "1. Generate some x-y data points.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n\n\n\n2. Plot the data points.\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\nTo add more graphs to the same figure, use plt.plot() multiple times before plt.show(). If you want to create a new figure, use plt.figure() before plt.plot().\n\nplt.plot(x,y)\nplt.plot(x,2*y)\nplt.show()\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Adjust the plot.\nThe plot() function takes the following arguments:\n\nx-axis data points\ny-axis data points\ncolor: hex, or color name (e.g., ‘red’, ‘blue’,‘black’), abbreviated (e.g., ‘r’, ‘b’,‘k’)\nlinestyle: ‘-’, ‘–’, ‘-.’, ‘:’ or “solid”, “dashed”, “dashdot”, “dotted”\nmarker: ‘o’, ‘x’, ‘+’, ’*‘, ’s’, ‘d’, ‘^’, ‘v’, ‘&gt;’, ‘&lt;’, ‘p’, ‘h’\nlinewidth - width of the line\nalpha - transparency of the line\nmarkerfacecolor - color of the marker face\nmarkersize - size of the marker\nlabel - label for the data points\n\nYou have to call plt.legend() to show the labels.\n\nplt.figure()\nplt.plot(x, y, color='red', linestyle='dashed', linewidth=2, marker='o', \n            markerfacecolor='blue', markersize=5,\n            label='sin(x)')\nplt.plot(x, 2*y, color='darkgrey', linestyle='dotted', linewidth=2, marker='x',\n            markerfacecolor='grey', markersize=5,\n            label='2*sin(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4. Adust the figure\nThis plot figure can be adjusted by changing the figure size, title, labels, and so on.\n\nplt.xlabel(): Set the x-axis label of the current axis.\nplt.ylabel(): Set the y-axis label of the current axis.\nplt.title(): Set a title for the axes.\nplt.legend(): Place a legend on the axes.\nplt.grid(): Configure the grid lines.\nplt.xlim(): Get or set the x-limits of the current axes.\nplt.ylim(): Get or set the y-limits of the current axes.\nplt.xticks(): Get or set the current tick locations and labels of the x-axis.\nplt.yticks(): Get or set the current tick locations and labels of the y-axis.\nplt.figure(): Create a new figure.\nplt.show(): Display a figure.\n\n\nplt.figure(figsize=(4, 4), dpi=100)\n# Create a figure with a specific size and resolution\nplt.plot(x, y, color='red', linestyle='dashed', linewidth=2, marker='o', \n            markerfacecolor='blue', markersize=5,\n            label='sin(x)')\nplt.plot(x, 2*y, color='darkgrey', linestyle='dotted', linewidth=2, marker='x',\n            markerfacecolor='grey', markersize=5,\n            label='2*sin(x)')\nplt.xlim([0, 10]) # set the x-axis limits\nplt.ylim([-3, 3]) # set the y-axis limits\nplt.xticks(np.arange(0, 11, 2)) # set the x-axis ticks\nplt.yticks(np.arange(-3, 4, 1)) # set the y-axis ticks\nplt.xlabel('x') # set the x-axis label\nplt.ylabel('y') # set the y-axis label\nplt.title('Sine and Double Sine Functions') # set the title of the plot\nplt.grid(linewidth=0.1)# set the grid linewidth\nplt.legend(loc='upper left') # set the location of the legend: upper left, upper right, lower left, lower right\nplt.show()",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Basic Data Visualization Techniques</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlots.html#creating-multiple-plots",
    "href": "course/chapters/Plots/SimplePlots.html#creating-multiple-plots",
    "title": "Basic Data Visualization Techniques",
    "section": "Creating multiple plots",
    "text": "Creating multiple plots\nYou can create multiple plots in the same figure by using the subplot() function.\n\nplt.subplot(2, 1, 1)\nplt.plot(x, y, color='red', linestyle='dashed', linewidth=2, marker='o', \n            markerfacecolor='blue', markersize=5,\n            label='sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Function')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(x, 2*y, color='darkgrey', linestyle='dotted', linewidth=2, marker='x',\n            markerfacecolor='grey', markersize=5,\n            label='2*sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Double Sine Function')\nplt.legend()   \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(4,2),dpi=72,layout='constrained', facecolor='lightyellow')\n# Create a figure with a specific layout and background color\n# constrained layout automatically adjusts the subplots to fit the figure\nfig.suptitle('Figure') # set the title of the figure object\nsubL, subR = fig.subfigures(1, 2) # create two subfigures\nsubL.set_facecolor('thistle') # set the background color of the left subfigure\nsub_subL = subL.subplots(2, 1, sharex=True) # create two subplots in the left subfigure\nsub_subL[1].set_xlabel('x [m]')\nsubL.suptitle('Left subfigure') # set the title of the left subfigure\nsubR.set_facecolor('lightskyblue') # set the background color of the right subfigure\nsub_subR = subR.subplots(1, 2, sharey=True) \nsub_subR[0].set_title('Axes 1') # set the title of the first subplot in the right subfigure\nsub_subR[1].set_title('Axes 2') # set the title of the second subplot in the right subfigure\nsubR.suptitle('Right subfigure')\n\nText(0.5, 0.98, 'Right subfigure')\n\n\n\n\n\n\n\n\n\n\nSubfigures and Gridspec\nYou can also create subplots and gridspecs to create more complex layouts.\n\nfrom matplotlib.gridspec import GridSpec\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig, ax = plt.subplots(3,2, figsize=(3,4), dpi=96, sharex=True, sharey=True, constrained_layout=True,gridspec_kw={'hspace': 0.2, 'wspace': 0.2})\n\nfor i in range(3):\n    for j in range(2):\n        ax[i,j].plot(x, y*(i+1)*(j+1))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig = plt.figure(figsize=(4, 6), dpi=100)\ngs = GridSpec(3, 2, figure=fig, hspace=0.4, wspace=0.3)\n\nfor i in range(3):\n    for j in range(2):\n        ax = fig.add_subplot(gs[i, j])\n        ax.plot(x, y*(i+1)*(j+1))\n        ax.set_title(f'Plot {i+1}, {j+1}')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nExercises\nDownload it locally and try to solve the exercises. \nSimple Plot Example\nOr open it in Google Colab:\nSimple Plot Example",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Basic Data Visualization Techniques</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html",
    "href": "course/chapters/Plots/SimplePlotTypes.html",
    "title": "Simple Plot Types",
    "section": "",
    "text": "Types of Plots\nDifficulty level:\nThere exists are lot of different visualization techniques.\nHow you should visualize the data depends on the data and the question you want to answer.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html#types-of-plots",
    "href": "course/chapters/Plots/SimplePlotTypes.html#types-of-plots",
    "title": "Simple Plot Types",
    "section": "",
    "text": "Distributions can be visualized with a boxplot, a violinplot, a histogram or a density plot.\nRelationships between two variables can be visualized with a scatterplot, a lineplot, a regplot or a jointplot.\nDescriptions of the data can be visualized with a barplot, network plot or a pie chart.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html#relationship-plots",
    "href": "course/chapters/Plots/SimplePlotTypes.html#relationship-plots",
    "title": "Simple Plot Types",
    "section": "Relationship plots",
    "text": "Relationship plots\n\nScatter Plot\nScatter plot shows the relationship of observable over the abscissa e.g. time vs temperature as discrete function.\nThe scatter() function creates a scatter plot.\n\nplt.scatter()\n\n\n\n\n\n\n\nNote\n\n\n\nThe marker size can be adjusted with the s parameter.\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.random.rand(100)*np.sin(x)\n\nplt.scatter(x, y, color='grey', marker='o', label='sin(x)',s=5)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLine Plot\nLine plot shows the relationship of observable over the abscissa e.g. time vs radioactivity decay as continuous function.\nThe plot() function creates a line plot but can also be used to create scatter plots.\n\n\n\n\n\n\nNote\n\n\n\nThe marker size can be adjusted with the markersize parameter.\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.random.rand(100)+np.sin(x)\n\nplt.plot(x, y, color='grey', marker='o', label='sin(x)',markersize=5,linewidth=2,linestyle='--')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nErrorbars\nErrorbars can be added to the plot with the errorbar() function and the yerr parameter.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.random.rand(100)*298\nyerr = np.random.rand(100)*10\n\nplt.errorbar(x, y, yerr=yerr, color='grey', marker='o', label='sin(x)',markersize=5,linewidth=2,elinewidth=3,errorevery=(10, 3), capsize=5, ecolor='black')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe yerr parameter can be used to set the error bars for the y-axis.\nThe xerr parameter can be used to set the error bars for the x-axis.\nThe parameter errorevery can be used to show only every 3th error bar - starting from the 10th error bar.\nThe capsize parameter can be used to set the size of the error bar caps.\nThe elinewidth parameter can be used to set the width of the error bar line.\nThe ecolor parameter can be used to set the color of the error bar line.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html#distribution-plots",
    "href": "course/chapters/Plots/SimplePlotTypes.html#distribution-plots",
    "title": "Simple Plot Types",
    "section": "Distribution Plots",
    "text": "Distribution Plots\n\nHistogram Plot\n[Histogram] (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) shows the distribution of a single variable e.g. count of a mass of individuals in a population. The data is divided into bins and the number of data points in each bin is plotted.\nThe histogram visualizes the skewness, kurtosis and outliers of the data.\nThe hist() function creates a histogram with bins number of bins.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(1000)\n\nplt.hist(x, bins=30, color='grey', edgecolor='black')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\n\n\n\n2D histograms can be created with the hist2d() function.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(1000)\ny = np.random.randn(1000)\nplt.hist2d(x, y, bins=30, cmap='Greys')\nplt.colorbar()\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe cmap parameter is used to set the color map of the histogram.\nThe colorbar() function adds a color bar to the plot.\n\n\n\nBoxplot\nBox plot shows the distribution of a numerical variable for different categories. It shows the minimum, first quartile, median, third quartile and maximum of your data. Outliers can be identified. An example of this e.g. is the distribution of cancer cell survival time for different treatment groups.\nThe boxplot() function creates a boxplot.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nstd = 2\ndata = [np.random.normal(0, std, 100) for std in range(1, 4)]\n\n\nplt.boxplot(data, patch_artist=True, notch=True, showmeans=True, meanline=True, showfliers=True, showbox=True, showcaps=True, orientation='horizontal',sym='C0+',\n            boxprops=dict(facecolor='darkgrey', color='black'),\n            medianprops=dict(color='grey'),\n            whiskerprops=dict(color='black'),\n            capprops=dict(color='black'),\n            meanprops=dict(color='black', linewidth=2))\nplt.yticks([1, 2, 3], ['A', 'B', 'C'])\nplt.title('Boxplot')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe patch_artist parameter is used to fill the box with color.\nThe notch parameter is used to create a notch in the box.\nThe showmeans parameter is used to show the mean of the data.\nThe meanline parameter is used to show the mean line.\nThe showfliers parameter is used to show the outliers of the data.\nThe showbox parameter is used to show the box of the data.\nThe showcaps parameter is used to show the caps of the data.\nThe orientation parameter is used to set the orientation of the boxplot. The default is vertical. If you want to create a horizontal boxplot, set it to horizontal.\nThe sym parameter is used to set the symbol of the outliers. The default is o. If you want to use a different symbol, set it to r+ or any other symbol you want to use.\nThe boxprops parameter is used to set the properties of the box. The facecolor parameter is used to set the color of the box. The color parameter is used to set the color of the box outline.\nThe medianprops parameter is used to set the properties of the median line. The color parameter is used to set the color of the median line.\nThe whiskerprops parameter is used to set the properties of the whiskers. The color parameter is used to set the color of the whiskers.\nThe meanprops parameter is used to set the properties of the mean line. The color parameter is used to set the color of the mean line.\nThe capprops parameter is used to set the properties of the caps. The color parameter is used to set the color of the caps.\n\n\n\nProportion Plots",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html#bar-plot",
    "href": "course/chapters/Plots/SimplePlotTypes.html#bar-plot",
    "title": "Simple Plot Types",
    "section": "Bar Plot",
    "text": "Bar Plot\nBar plot shows the relationship of a categorical variable with a numerical variable e.g. cancer cell survival time for different treatment groups. The height of the bar is proportional to the value of the investigated variable.\nThe bar() function creates a bar plot.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\n\nx = ['A', 'B', 'C', 'D']\ny = [10, 20, 30, 40]\nyerr = np.random.rand(4)*10\n\nplt.bar(x, y,yerr=yerr, color='grey')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()\n\n\n\n\n\n\n\n\nYou can draw a star sign to indicate the significance of the data.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlotTypes.html#pie-chart",
    "href": "course/chapters/Plots/SimplePlotTypes.html#pie-chart",
    "title": "Simple Plot Types",
    "section": "Pie Chart",
    "text": "Pie Chart\nPie chart show the proportion of different categories in a single variable e.g. the content of different amino acids in a protein.\nThe pie() function creates a pie chart.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = [10, 20, 30, 40]\nlabels = ['A', 'B', 'C', 'D']\n\nplt.pie(x, labels=labels, autopct='%1.1f%%', startangle=90, colors=['lightgrey', 'grey', 'darkgrey', 'dimgrey'], explode=(0.1, 0, 0, 0), shadow=True)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/Seaborn.html",
    "href": "course/chapters/Plots/Seaborn.html",
    "title": "Seaborn Plots",
    "section": "",
    "text": "Plotting with Seaborn\nDifficulty level:\nSeaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn is built on top of Matplotlib and closely integrated with pandas data structures.",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Seaborn Plots</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/Seaborn.html#plotting-with-seaborn",
    "href": "course/chapters/Plots/Seaborn.html#plotting-with-seaborn",
    "title": "Seaborn Plots",
    "section": "",
    "text": "KDE Plot\nKernel density plot show the distribution of a single variable e.g. distribution of a mass of individuals in a population. The data is smoothed by a kernel density and the density of the data is plotted continuously.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create a dataset\ndata = np.random.normal(loc=0, scale=1, size=1000)\ndf = pd.DataFrame(data, columns=[\"Data\"])\n\n# Create a KDE plot\nsns.kdeplot(data=df[\"Data\"], color=\"blue\", shade=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nViolin Plot\n\nViolin plot: Show the distribution of a numerical variable for different categories. It is similar to a box plot but it also shows the probability density of the data at different values. An example of this e.g. is the distribution of cancer cell survival time for different treatment groups.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create a dataset\ndata = np.random.normal(loc=0, scale=1, size=1000)\ndf = pd.DataFrame(data, columns=[\"Data\"])\n\n# Create a Violin plot\nsns.violinplot(data=df[\"Data\"], color=\"blue\")\nplt.show()",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Seaborn Plots</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/Seaborn.html#correlation-plots",
    "href": "course/chapters/Plots/Seaborn.html#correlation-plots",
    "title": "Seaborn Plots",
    "section": "Correlation plots",
    "text": "Correlation plots\n\nRegression plot: Show a regression model between two variables e.g. the relationship between the concentration and the time.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create a dataset\n\ndata = np.random.normal(loc=0, scale=1, size=1000)\ndf = pd.DataFrame(data, columns=[\"Data\"])\n\n# Create a Regression plot\nsns.regplot(x=np.arange(0, len(df)), y=df[\"Data\"], color=\"blue\")\nplt.show()\n\n\n\n\n\n\n\n\n\nHeatmap shows the relationship of two categorical variables with a numerical variable e.g. cancer cell survival time for different treatment groups and different cancer types. The color of the cell is proportional to the value of the investigated variable.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create a dataset\ndata = np.random.normal(loc=0, scale=1, size=(100, 100))\ndf = pd.DataFrame(data)\n\n# Create a Heatmap\nsns.heatmap(data=df, cmap=\"coolwarm\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAdditional resources\n\nSeaborn Gallery\nSeaborn API\nSeaborn Tutorial\nSeaborn Documentation",
    "crumbs": [
      "Data Visualization 1 ",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Seaborn Plots</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseA_1.html",
    "href": "course/exercises/exercises/exerciseA_1.html",
    "title": "Exercise A 1",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exercise A 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseA_1.html#temperature-in-synthesis-reactor-part-1",
    "href": "course/exercises/exercises/exerciseA_1.html#temperature-in-synthesis-reactor-part-1",
    "title": "Exercise A 1",
    "section": "Temperature in Synthesis Reactor Part 1",
    "text": "Temperature in Synthesis Reactor Part 1\nIn this exercise, we will repeat the first part of the course.\n\nWe will learn how to read data from a file.\nWe will learn how to inspect the data.\nWe will learn how to make quickly some plots.\n\n\nData\nThe experiment:\n\nWe have multiple synthesis reactors in which we are running a series of experiments.\nEach reactor has a temperature sensor which monitors the temperature inside of it every minute for one month.\nOur task is to check if the temperatures in the reactors are stable.\nThe data for reactor 1 is stored in Temperatures_1.dat.\nThe data for reactor 2 is stored in Temperatures_2.dat.\nThe data for reactor 3 is stored in Temperatures_3.dat.\n\n\n\nData Path:\ndata_path = \"https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/Temperature/\"\n\n\nTask\n\nLook at the plain data files. In which format are the data stored?\nRead the data from the files.\nInspect the data. Are there any missing values? Are the data in the correct format? Is the data measured really over one month?\nTransform the data into days.\nLook at the distribution of the data. Is there some striking pattern?\nMake a plot of the temperatures trend over time for reactor 1, 2 and 3.\n\n\n\nQuestions\n\nWhat do you think?\nAre the temperatures stable in the reactors?\nWhich reactor has the most stable temperature?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exercise A 1</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataManipulation.html",
    "href": "course/chapters/SEDA/DataManipulation.html",
    "title": "Data Manipulation",
    "section": "",
    "text": "Data Manipulation\nDifficulty level:",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataManipulation.html#numpy",
    "href": "course/chapters/SEDA/DataManipulation.html#numpy",
    "title": "Data Manipulation",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nArray manipulation\nOften data has to be manipulated before it can be analyzed.  Numpy has many methods for array manipulation. \nFor example,\n\nthe shape of an array can be changed,\nmultiple arrays can be concatenated,\nthe elements of an array can be sorted\nnon valid values NaN can be removed,\nlinear algebra operations can be performed,\n\netc.\n\nSort arrays:\n\nunsorted_arr = np.array([[3, 1, 5, 2, 4], [5, 2, 0, 8, 1], [3, 2, 9, 4 , 5]])\nprint(\"unsortd_arr: \\n\", unsorted_arr)\nsorted_arr = np.sort(unsorted_arr,axis = 0) # sorted along the column\nprint(\"sorted_arr along columns: \\n\" , sorted_arr)\nsorted_arr = np.sort(unsorted_arr,axis = 1) # sorted along the row\nprint(\"sorted_arr along rows: \\n\" , sorted_arr)\n\nunsortd_arr: \n [[3 1 5 2 4]\n [5 2 0 8 1]\n [3 2 9 4 5]]\nsorted_arr along columns: \n [[3 1 0 2 1]\n [3 2 5 4 4]\n [5 2 9 8 5]]\nsorted_arr along rows: \n [[1 2 3 4 5]\n [0 1 2 5 8]\n [2 3 4 5 9]]\n\n\n\n\nConcatenate arrays:\n\na = np.array([1, 2, 3]) \nb = np.array([4, 5, 6])\nc = np.concatenate((a, b))\nprint(c)\n\n[1 2 3 4 5 6]\n\n\n\n\nReshape arrays:\n\na = np.array([[1, 2],[3, 4],[5, 6]])\nb = np.array([[7, 8],[9, 10],[11, 12]])\nc = np.vstack((a, b)) # vertical stack\nd = np.hstack((a, b)) # horizontal stack\nprint(\"vertical stack: \\n\", c)\nprint(\"horizontal stack: \\n\", d)\nprint(\"flatten array: \\n\", c.flatten())\n\nvertical stack: \n [[ 1  2]\n [ 3  4]\n [ 5  6]\n [ 7  8]\n [ 9 10]\n [11 12]]\nhorizontal stack: \n [[ 1  2  7  8]\n [ 3  4  9 10]\n [ 5  6 11 12]]\nflatten array: \n [ 1  2  3  4  5  6  7  8  9 10 11 12]\n\n\n\n\nLinear algebra operations e.g.:\n\nnp.log(a) returns the natural logarithm of an array\nnp.log10(a) returns the base 10 logarithm of an array\nnp.log2(a) returns the base 2 logarithm of an array\nnp.log1p(a) returns the natural logarithm of an array plus one\nnp.exp(a) returns the exponential of an array\nnp.sqrt(a) returns the square root of an array\nnp.abs(a) returns the absolute value of an array\nnp.sum(a) returns element-wise sum of two arrays\nnp.add(a) returns sum of all elements arrays\nnp.subtract(a,b) returns the difference of two arrays\nnp.multiply(a,b) returns the product of two arrays\nnp.divide(a,b) returns the division of two arrays\nnp.dot(a,b) returns the dot product of two arrays\nnp.cross(a,b) returns the cross product of two arrays\nnp.linalg.inv(a) returns the inverse of a matrix\nnp.linalg.det(a) returns the determinant of a matrix\nnp.linalg.eig(a) returns the eigenvalues and eigenvectors of a matrix\nnp.linalg.solve(a,b) returns the solution of a linear system of equations\nnp.convolv(a,b) returns the convolution of two arrays\n\n\na = np.array([[1, 2],[3, 4],[5, 6]])\nb = np.array([[7, 8],[9, 10],[11, 12]])\nc = np.sum(a) # sum of all elements\nprint(\"Summ of all elements: \", c)\nc = np.add(a,b) # element wise addition\nprint(\"Element wise addition: \", c)\n\nSumm of all elements:  21\nElement wise addition:  [[ 8 10]\n [12 14]\n [16 18]]\n\n\n\n\nQuestion:\nWhat does the following code snippet do?\n\ntemperatures = np.array([20, 30, 40, 50, 60, 70, 80, 90, 100])\nwindow = 3\nnp.convolve(temperatures,np.ones(window) / window,\n                            mode='valid')\n\n\nIf you are not sure, look in the numpy documentation, ask AI, search in the internet or try it out in a code cell.\n\n\n\n\n\n\nAnswer:\n\n\n\n\n\nThe code snippet calculates the moving average of the temperatures with a window size of 3.",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataManipulation.html#pandas",
    "href": "course/chapters/SEDA/DataManipulation.html#pandas",
    "title": "Data Manipulation",
    "section": "Pandas",
    "text": "Pandas\n\nimport pandas as pd\ndata = pd.DataFrame({\n    \"Name\": [\"Water\", \"Sulfuric Acid\", \"Ethanol\", \"Acetone\", \"Ammonia\", \"Benzene\", \"Methanol\", \"Glycerol\"],\n    \"Formula\": [\"H2O\", \"H2SO4\", \"C2H5OH\", \"C3H6O\", \"NH3\", \"C6H6\", \"CH3OH\", \"C3H8O3\"],\n    \"Molecular Weight (g/mol)\": [18.015, 98.079, 46.07, 58.08, 17.03, 78.11, 32.04, 92.09],\n    \"Viscosity (mPa·s)\": [1.002, 24.0, 1.2, 0.32, 0.26, 0.65, 0.544, 1412],\n    \"pH (Acidity)\": [7, 1, 7.33, 7, 11.6, 7, 7.4, 5.5],\n    \"Chemical Type\": [\"Inorganic\", \"Acid\", \"Alcohol\", \"Ketone\", \"Base\", \"Aromatic Hydrocarbon\", \"Alcohol\", \"Polyol\"],\n    \"Concentration (M)\": [55.5, 18.0, 1.0, 0.8, 0.5, 0.1, 1.5, 1.2]})\n\n\nprint(\"head of data: \\n\",data.head()) # print the first 5 rows\nprint(\"\\n\")\nprint(\"Number of data set: \\n\", data.shape[0]) # number of data set\nprint(\"\\n\")\nprint(\"Column Name: \\n\", data[\"Name\"]) # print the column \"Name\"\nprint(\"\\n\")\nprint(\"2. Row: \\n\", data.iloc[1]) # print the second row\n\nhead of data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0          7.00     Inorganic               55.5  \n1          1.00          Acid               18.0  \n2          7.33       Alcohol                1.0  \n3          7.00        Ketone                0.8  \n4         11.60          Base                0.5  \n\n\nNumber of data set: \n 8\n\n\nColumn Name: \n 0            Water\n1    Sulfuric Acid\n2          Ethanol\n3          Acetone\n4          Ammonia\n5          Benzene\n6         Methanol\n7         Glycerol\nName: Name, dtype: object\n\n\n2. Row: \n Name                        Sulfuric Acid\nFormula                             H2SO4\nMolecular Weight (g/mol)           98.079\nViscosity (mPa·s)                    24.0\npH (Acidity)                          1.0\nChemical Type                        Acid\nConcentration (M)                    18.0\nName: 1, dtype: object\n\n\n\nSelect subsets of the data:\nOften you need only a specific part of your data with pandasit is quite easy to filter the data after specific properties.\n\n# filter the rows where concentration is greater than 5\nprint(\"Filter the rows where concentration is greater than 5: \\n\", data[data[\"Concentration (M)\"] &gt; 5]) \nprint(\"\\n\")\n# get the chemical type with a concentration higher than 5\nprint(\"Get the chemical type with a concentration higher than 5: \\n\", \n      data.loc[data[\"Concentration (M)\"]&gt;5,\"Chemical Type\"])\nprint(\"\\n\")\n\nFilter the rows where concentration is greater than 5: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0           7.0     Inorganic               55.5  \n1           1.0          Acid               18.0  \n\n\nGet the chemical type with a concentration higher than 5: \n 0    Inorganic\n1         Acid\nName: Chemical Type, dtype: object\n\n\n\n\n\n\nManipulate data\nYou can\n\ndf.copy(): copy the data\ndf.drop(columns=[...]): drop columns\ndf.drop(index=[...]): drop rows\ndf.drop_duplicates(): drop duplicates\ndf.fillna(...): fill NaNs\ndf.dropna(): drop NaNs\ndf.replace(...): replace values\npd.concat([df1, df2]): concatenate data (concatenate along the rows)\npd.concat([df1, df2], axis=1): concatenate data (concatenate along the columns)\npd.merge(df1, df2): merge data in the sense of inner, outer, left, right join, see pandas documentation\ndf.agg(...): aggregate the data\ndf.transform(...): transform the data\ndf.value_counts(): count the values\ndf.apply(...): apply a function to the data\ndf.groupby(...): group the data and perform operations on the groups\ndf.sort_values(...): sort the data\ndf.shift(...): shift the data\ndf.diff(...): get the difference between the data\ndf.ptc_shift(...): get the percentage change between the data\ndf.to_numpy(): convert the dataframe to a numpy array\npd.DataFrame(arr): convert the numpy array to a dataframe etc.\n\n\nCopy and concatenate data:\n\ndata2 = data.copy()\ndata2[\"Concentration 2 (M)\"] = data2[\"Concentration (M)\"] + 10 # increase the concentration by 10 M\nprint(\"data2: \\n\" , data2)\n\n\ndata3 = pd.concat([data, data2]) # concatenate the dataframes\nprint(\"concated data: \\n\", data3)\n\ndata2: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \n0          7.00             Inorganic               55.5                 65.5  \n1          1.00                  Acid               18.0                 28.0  \n2          7.33               Alcohol                1.0                 11.0  \n3          7.00                Ketone                0.8                 10.8  \n4         11.60                  Base                0.5                 10.5  \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1  \n6          7.40               Alcohol                1.5                 11.5  \n7          5.50                Polyol                1.2                 11.2  \nconcated data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \n0          7.00             Inorganic               55.5                  NaN  \n1          1.00                  Acid               18.0                  NaN  \n2          7.33               Alcohol                1.0                  NaN  \n3          7.00                Ketone                0.8                  NaN  \n4         11.60                  Base                0.5                  NaN  \n5          7.00  Aromatic Hydrocarbon                0.1                  NaN  \n6          7.40               Alcohol                1.5                  NaN  \n7          5.50                Polyol                1.2                  NaN  \n0          7.00             Inorganic               55.5                 65.5  \n1          1.00                  Acid               18.0                 28.0  \n2          7.33               Alcohol                1.0                 11.0  \n3          7.00                Ketone                0.8                 10.8  \n4         11.60                  Base                0.5                 10.5  \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1  \n6          7.40               Alcohol                1.5                 11.5  \n7          5.50                Polyol                1.2                 11.2  \n\n\n\n # create a Panda Series\nacidity = pd.Series([\"basic\", \"acid\", \"basic\", \"neutral\",\n                    \"acid\", \"acid\", \"basic\", \"basic\"], name=\"Aciditiy Type\")\nprint(\"acidity: \\n\",acidity)\n# concatenate the dataframes\ndata4 = pd.concat([data3, acidity], axis=1) \nprint(\"concated data: \\n\", data4)\n\nacidity: \n 0      basic\n1       acid\n2      basic\n3    neutral\n4       acid\n5       acid\n6      basic\n7      basic\nName: Aciditiy Type, dtype: object\nconcated data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \\\n0          7.00             Inorganic               55.5                  NaN   \n1          1.00                  Acid               18.0                  NaN   \n2          7.33               Alcohol                1.0                  NaN   \n3          7.00                Ketone                0.8                  NaN   \n4         11.60                  Base                0.5                  NaN   \n5          7.00  Aromatic Hydrocarbon                0.1                  NaN   \n6          7.40               Alcohol                1.5                  NaN   \n7          5.50                Polyol                1.2                  NaN   \n0          7.00             Inorganic               55.5                 65.5   \n1          1.00                  Acid               18.0                 28.0   \n2          7.33               Alcohol                1.0                 11.0   \n3          7.00                Ketone                0.8                 10.8   \n4         11.60                  Base                0.5                 10.5   \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1   \n6          7.40               Alcohol                1.5                 11.5   \n7          5.50                Polyol                1.2                 11.2   \n\n  Aciditiy Type  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n\n\n\n\nDrop columns and rows:\n\ndata4 = data4.drop(columns=[\"Concentration 2 (M)\"]) # drop the column \"Concentration 2 (M)\"\nprint(\"data4: \\n\", data4)\n\ndata5 = data4.drop(index=[0, 1]) # drop the rows with index 0 and 1\nprint(\"data5: \\n\", data5)\n\ndata6 = data5.drop(columns=[\"Aciditiy Type\"]) # drop the column \"Aciditiy Type\"\nprint(\"data6: \\n\", data6)\n\ndata4: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic               55.5         basic  \n1          1.00                  Acid               18.0          acid  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \n0          7.00             Inorganic               55.5         basic  \n1          1.00                  Acid               18.0          acid  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \ndata5: \n        Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n2   Ethanol  C2H5OH                     46.07              1.200   \n3   Acetone   C3H6O                     58.08              0.320   \n4   Ammonia     NH3                     17.03              0.260   \n5   Benzene    C6H6                     78.11              0.650   \n6  Methanol   CH3OH                     32.04              0.544   \n7  Glycerol  C3H8O3                     92.09           1412.000   \n2   Ethanol  C2H5OH                     46.07              1.200   \n3   Acetone   C3H6O                     58.08              0.320   \n4   Ammonia     NH3                     17.03              0.260   \n5   Benzene    C6H6                     78.11              0.650   \n6  Methanol   CH3OH                     32.04              0.544   \n7  Glycerol  C3H8O3                     92.09           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \ndata6: \n        Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n2   Ethanol  C2H5OH                     46.07              1.200   \n3   Acetone   C3H6O                     58.08              0.320   \n4   Ammonia     NH3                     17.03              0.260   \n5   Benzene    C6H6                     78.11              0.650   \n6  Methanol   CH3OH                     32.04              0.544   \n7  Glycerol  C3H8O3                     92.09           1412.000   \n2   Ethanol  C2H5OH                     46.07              1.200   \n3   Acetone   C3H6O                     58.08              0.320   \n4   Ammonia     NH3                     17.03              0.260   \n5   Benzene    C6H6                     78.11              0.650   \n6  Methanol   CH3OH                     32.04              0.544   \n7  Glycerol  C3H8O3                     92.09           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  \n2          7.33               Alcohol                1.0  \n3          7.00                Ketone                0.8  \n4         11.60                  Base                0.5  \n5          7.00  Aromatic Hydrocarbon                0.1  \n6          7.40               Alcohol                1.5  \n7          5.50                Polyol                1.2  \n2          7.33               Alcohol                1.0  \n3          7.00                Ketone                0.8  \n4         11.60                  Base                0.5  \n5          7.00  Aromatic Hydrocarbon                0.1  \n6          7.40               Alcohol                1.5  \n7          5.50                Polyol                1.2  \n\n\n\n\nDrop duplicates, fill NaNs and drop NaNs\n\ndata5 = data4.drop_duplicates() # drop the duplicates\nprint(\"data5: \\n\", data5)\n\ndata6 = data5.fillna(0) # fill the NaNs with 0\nprint(\"data6: \\n\", data6)\n\ndata7 = data5.dropna() # drop the NaNs\nprint(\"data7: \\n\", data7)\n\ndata5: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic               55.5         basic  \n1          1.00                  Acid               18.0          acid  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \ndata6: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic               55.5         basic  \n1          1.00                  Acid               18.0          acid  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \ndata7: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic               55.5         basic  \n1          1.00                  Acid               18.0          acid  \n2          7.33               Alcohol                1.0         basic  \n3          7.00                Ketone                0.8       neutral  \n4         11.60                  Base                0.5          acid  \n5          7.00  Aromatic Hydrocarbon                0.1          acid  \n6          7.40               Alcohol                1.5         basic  \n7          5.50                Polyol                1.2         basic  \n\n\n\n\nReplace values:\n\n# replace the values\ndata6[\"Aciditiy Type\"].replace({\"acid\": \"Acidic\", \"basic\": \"Basic\", \"neutral\": \"Neutral\"}, inplace=True)\nprint(\"data6: \\n\", data6)\n\ndata6: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic               55.5         Basic  \n1          1.00                  Acid               18.0        Acidic  \n2          7.33               Alcohol                1.0         Basic  \n3          7.00                Ketone                0.8       Neutral  \n4         11.60                  Base                0.5        Acidic  \n5          7.00  Aromatic Hydrocarbon                0.1        Acidic  \n6          7.40               Alcohol                1.5         Basic  \n7          5.50                Polyol                1.2         Basic  \n\n\n\n\n\nCount the values\n\n# count the chemicals with different concentration\nprint(\"Count the chemicals with the different concentrations \\n\", data[\"Concentration (M)\"].value_counts())\n\nCount the chemicals with the different concentrations \n Concentration (M)\n55.5    1\n18.0    1\n1.0     1\n0.8     1\n0.5     1\n0.1     1\n1.5     1\n1.2     1\nName: count, dtype: int64\n\n\n\n\nAppply a function to the data\n\nprint(\"Apply a function to the data: \\n\", data[\"Concentration (M)\"].apply(lambda x: x*2))\n\nApply a function to the data: \n 0    111.0\n1     36.0\n2      2.0\n3      1.6\n4      1.0\n5      0.2\n6      3.0\n7      2.4\nName: Concentration (M), dtype: float64\n\n\n\n\nTransform data\n\n# transform the data\ndata6[\"Concentration (M)\"] = data6[\"Concentration (M)\"].transform(lambda x: x*2)\nprint(\"data6: \\n\", data6)\n\ndata6: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M) Aciditiy Type  \n0          7.00             Inorganic              111.0         Basic  \n1          1.00                  Acid               36.0        Acidic  \n2          7.33               Alcohol                2.0         Basic  \n3          7.00                Ketone                1.6       Neutral  \n4         11.60                  Base                1.0        Acidic  \n5          7.00  Aromatic Hydrocarbon                0.2        Acidic  \n6          7.40               Alcohol                3.0         Basic  \n7          5.50                Polyol                2.4         Basic  \n\n\n\n\nSort data\n\nprint(\"sorted after concentration: \",data.sort_values(by=\"Concentration (M)\", ascending=True))\n\nsorted after concentration:              Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n5        Benzene    C6H6                    78.110              0.650   \n4        Ammonia     NH3                    17.030              0.260   \n3        Acetone   C3H6O                    58.080              0.320   \n2        Ethanol  C2H5OH                    46.070              1.200   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n6       Methanol   CH3OH                    32.040              0.544   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n0          Water     H2O                    18.015              1.002   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  \n5          7.00  Aromatic Hydrocarbon                0.1  \n4         11.60                  Base                0.5  \n3          7.00                Ketone                0.8  \n2          7.33               Alcohol                1.0  \n7          5.50                Polyol                1.2  \n6          7.40               Alcohol                1.5  \n1          1.00                  Acid               18.0  \n0          7.00             Inorganic               55.5  \n\n\n\n\nGroup data\nIn pandas you can group data by a specific column and perform operations on the groups.\n\n# sum of concentration by filter\nprint(\"sum of concentration: \\n\", data[\"Concentration (M)\"].sum()) \n# sum of concentration group by type\nprint(\"Sum of concentration group by type: \\n\", data.groupby(\"Chemical Type\")[\"Concentration (M)\"].sum())\n\nsum of concentration: \n 78.6\nSum of concentration group by type: \n Chemical Type\nAcid                    18.0\nAlcohol                  2.5\nAromatic Hydrocarbon     0.1\nBase                     0.5\nInorganic               55.5\nKetone                   0.8\nPolyol                   1.2\nName: Concentration (M), dtype: float64\n\n\n\n\nTransform Pandas DataFrame to Numpy Array and viceversa\n\nprint(\"Names: \", data5.columns)\ndf = data5.copy()\nprint(len(df))\n# convert the dataframe to a numpy array\narr = df.to_numpy() \nprint(\"arr: \\n\", arr)\n# convert the numpy array to a dataframe\ndf2 = pd.DataFrame(arr, columns=[\"Name\", \"Formula\", \"Molecular Weight (g/mol)\",\"Viscosity (mPa·s)\",\"pH (Acidity)\",\"Chemical Type\",\"Concentration (M)\", \"Aciditiy Type\"])\nprint(\"df2: \\n\", df2)\n\nNames:  Index(['Name', 'Formula', 'Molecular Weight (g/mol)', 'Viscosity (mPa·s)',\n       'pH (Acidity)', 'Chemical Type', 'Concentration (M)', 'Aciditiy Type'],\n      dtype='object')\n8\narr: \n [['Water' 'H2O' 18.015 1.002 7.0 'Inorganic' 55.5 'basic']\n ['Sulfuric Acid' 'H2SO4' 98.079 24.0 1.0 'Acid' 18.0 'acid']\n ['Ethanol' 'C2H5OH' 46.07 1.2 7.33 'Alcohol' 1.0 'basic']\n ['Acetone' 'C3H6O' 58.08 0.32 7.0 'Ketone' 0.8 'neutral']\n ['Ammonia' 'NH3' 17.03 0.26 11.6 'Base' 0.5 'acid']\n ['Benzene' 'C6H6' 78.11 0.65 7.0 'Aromatic Hydrocarbon' 0.1 'acid']\n ['Methanol' 'CH3OH' 32.04 0.544 7.4 'Alcohol' 1.5 'basic']\n ['Glycerol' 'C3H8O3' 92.09 1412.0 5.5 'Polyol' 1.2 'basic']]\ndf2: \n             Name Formula Molecular Weight (g/mol) Viscosity (mPa·s)  \\\n0          Water     H2O                   18.015             1.002   \n1  Sulfuric Acid   H2SO4                   98.079              24.0   \n2        Ethanol  C2H5OH                    46.07               1.2   \n3        Acetone   C3H6O                    58.08              0.32   \n4        Ammonia     NH3                    17.03              0.26   \n5        Benzene    C6H6                    78.11              0.65   \n6       Methanol   CH3OH                    32.04             0.544   \n7       Glycerol  C3H8O3                    92.09            1412.0   \n\n  pH (Acidity)         Chemical Type Concentration (M) Aciditiy Type  \n0          7.0             Inorganic              55.5         basic  \n1          1.0                  Acid              18.0          acid  \n2         7.33               Alcohol               1.0         basic  \n3          7.0                Ketone               0.8       neutral  \n4         11.6                  Base               0.5          acid  \n5          7.0  Aromatic Hydrocarbon               0.1          acid  \n6          7.4               Alcohol               1.5         basic  \n7          5.5                Polyol               1.2         basic",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DescriptiveStatistics.html",
    "href": "course/chapters/SEDA/DescriptiveStatistics.html",
    "title": "Descriptive Statistics and Analysis",
    "section": "",
    "text": "Descriptive Statistics and Analysis\nDifficulty level:\nDescriptive statistics summarize and analyze datasets by providing measures of central tendency, dispersion, and correlation. These statistics help in understanding the underlying patterns in chemical and materials science data.",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Descriptive Statistics and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DescriptiveStatistics.html#descriptive-statistics-and-analysis",
    "href": "course/chapters/SEDA/DescriptiveStatistics.html#descriptive-statistics-and-analysis",
    "title": "Descriptive Statistics and Analysis",
    "section": "",
    "text": "Measures of Central Tendency: Mean, median, and mode provide insights into the average or most common values in the dataset.\nVariability: Variance, standard deviation, and interquartile range (IQR) measure how spread out the data is.\nCorrelation & Covariance: Determine relationships between two or more variables, useful for identifying dependencies in experimental data.\nSummary Statistics: Pandas provides built-in functions to calculate descriptive statistics for dataframes, including mean, median, standard deviation, and more. Further you can get a fast overview of the data using the describe() function.\n\n\nNumpy\n\nimport numpy as np\n\n\nnp.mean(a) returns the mean of the elements of an array\nnp.average(a) returns the weighted average of the elements of an array\nnp.median(a) returns the median of the elements of an array\nnp.max(a) returns the maximum of the elements of an array\nnp.min(a) returns the minimum of the elements of an array\nnp.std(a) returns the standard deviation of the elements of an array\nnp.var(a) returns the variance of the elements of an array\nnp.covar(a) returns the covariance of the elements of an array\nnp.percentile(a,p) returns the p-th percentile of the elements of an array\nnp.quantile(a,q) returns the q-th quantile of the elements of an array\nnp.corrcoef(a) returns the correlation coefficient of the elements of an array\nnp.corrcoef(a,b) returns the correlation coefficient of two arrays\nnp.histogram(a, [,bins,range,density,weights]) returns the histogram of the elements of an array\nnp.histogram2d(a,b, [,bins,range,density,weights]) returns the 2D histogram of the elements of an array\nnp.histogramdd(a, [,bins,range,density,weights]) returns the nD histogram of the elements of an array\n\n\narr = np.array([1, 2, 3, 4, 5, 4, 7, 8, 9])\nprint(arr)\nprint(\"Mean: \", np.mean(arr))\nprint(\"Weighted average: \", np.average(arr, weights=[1, 2, 3, 4, 5, 4, 3, 2, 1]))\nprint(\"Median: \", np.median(arr))\nprint(\"Standard deviation: \", np.std(arr))\nprint(\"Variance: \", np.var(arr))\nprint(\"Correlation coefficient: \", np.corrcoef(arr))\nprint(\"Percentile: \", np.percentile(arr, 0.25))\nprint(\"Quantile: \", np.quantile(arr, 0.75))\nprint(\"IQR: \", np.percentile(arr, 0.75) - np.percentile(arr, 0.25))\n\n[1 2 3 4 5 4 7 8 9]\nMean:  4.777777777777778\nWeighted average:  4.68\nMedian:  4.0\nStandard deviation:  2.57240820062005\nVariance:  6.617283950617284\nCorrelation coefficient:  1.0\nPercentile:  1.02\nQuantile:  7.0\nIQR:  0.040000000000000036\n\n\n\nnp.argmin(a) returns the index of the minimum element of an array\nnp.argmax(a) returns the index of the maximum element of an array\nnp.where(a) returns the indices of the elements of an array that are non-zero\nnp.argwhere(a) returns the indices of the elements of an array that are non-zero\nnp.nonzero(a) returns the indices of the elements of an array that are non-zero\nnp.searchsorted(a,v) returns the index of the first element of an array  that is greater than or equal to a value\nnp.extract(condition,a) returns the elements of an array that satisfy a condition\n\n\narr = np.array([1, 2, 3, 4, 5, 4, 7, 8, 9])\nprint(arr)\nprint(\"Index of the maximum element: \", np.argmax(arr))\nprint(\"Index of the minimum element: \", np.argmin(arr))\nprint(\"Maximum element: \", np.max(arr))\nprint(\"Minimum element: \", np.min(arr))\nprint(\"Find the index of element \\\"4\\\": \", np.where(arr == 4))\nprint(\"Find the index of element \\\"&gt;4\\\": \", np.argwhere(arr &gt; 4),\n       \" and the elements are: \", arr[np.where(arr &gt; 4)])\n\n[1 2 3 4 5 4 7 8 9]\nIndex of the maximum element:  8\nIndex of the minimum element:  0\nMaximum element:  9\nMinimum element:  1\nFind the index of element \"4\":  (array([3, 5]),)\nFind the index of element \"&gt;4\":  [[4]\n [6]\n [7]\n [8]]  and the elements are:  [5 7 8 9]\n\n\n\n\n\nPandas\n\nimport pandas as pd\ndata = pd.DataFrame({\n    \"Name\": [\"Water\", \"Sulfuric Acid\", \"Ethanol\", \"Acetone\", \"Ammonia\", \"Benzene\", \"Methanol\", \"Glycerol\"],\n    \"Formula\": [\"H2O\", \"H2SO4\", \"C2H5OH\", \"C3H6O\", \"NH3\", \"C6H6\", \"CH3OH\", \"C3H8O3\"],\n    \"Molecular Weight (g/mol)\": [18.015, 98.079, 46.07, 58.08, 17.03, 78.11, 32.04, 92.09],\n    \"Viscosity (mPa·s)\": [1.002, 24.0, 1.2, 0.32, 0.26, 0.65, 0.544, 1412],\n    \"pH (Acidity)\": [7, 1, 7.33, 7, 11.6, 7, 7.4, 5.5],\n    \"Chemical Type\": [\"Inorganic\", \"Acid\", \"Alcohol\", \"Ketone\", \"Base\", \"Aromatic Hydrocarbon\", \"Alcohol\", \"Polyol\"],\n    \"Concentration (M)\": [55.5, 18.0, 1.0, 0.8, 0.5, 0.1, 1.5, 1.2]})\n\n\n\nRepeation how to get an overview of the data\n\ndata.head(): Returns the first 5 rows of the dataframe\n\n\nprint(\"head of data: \\n\",data.head()) # print the first 5 rows\n\nhead of data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0          7.00     Inorganic               55.5  \n1          1.00          Acid               18.0  \n2          7.33       Alcohol                1.0  \n3          7.00        Ketone                0.8  \n4         11.60          Base                0.5  \n\n\n\ndata.shape[0]: Returns the number of rows in the dataframe\n\n\nprint(\"Number of data set: \\n\", data.shape[0]) # number of data set\n\nNumber of data set: \n 8\n\n\n\ndata.shape[1]: Returns the number of columns in the dataframe\n\n\nprint(\"Number of data set: \\n\", data.shape[0]) # number of data set\n\nNumber of data set: \n 8\n\n\n\ndata[\"Name\"]: Returns the column “Name” of the dataframe\n\n\nprint(\"Name of chemicals: \\n\", data[\"Name\"]) # print the column \"Name\"\n\nName of chemicals: \n 0            Water\n1    Sulfuric Acid\n2          Ethanol\n3          Acetone\n4          Ammonia\n5          Benzene\n6         Methanol\n7         Glycerol\nName: Name, dtype: object\n\n\n\ndata.iloc[1]: Returns the second row of the dataframe\n\n\nprint(\"2. Row: \\n\", data.iloc[1]) # print the second row\n\n2. Row: \n Name                        Sulfuric Acid\nFormula                             H2SO4\nMolecular Weight (g/mol)           98.079\nViscosity (mPa·s)                    24.0\npH (Acidity)                          1.0\nChemical Type                        Acid\nConcentration (M)                    18.0\nName: 1, dtype: object\n\n\n\ndata[data[\"Concentration (M)\"] &gt; 5]: Returns the rows where the concentration is greater than 5\ndata.loc[data[\"Concentration (M)\"]&gt;5,\"Chemical Type\"]: Returns the chemical type with a concentration higher than 5\n\n\nprint(\"Filter the rows where concentration is greater than 5: \\n\", data[data[\"Concentration (M)\"] &gt; 5])\nprint(\"Get the chemical type with a concentration higher than 5: \\n\", \n      data.loc[data[\"Concentration (M)\"]&gt;5,\"Chemical Type\"])\n\nFilter the rows where concentration is greater than 5: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0           7.0     Inorganic               55.5  \n1           1.0          Acid               18.0  \nGet the chemical type with a concentration higher than 5: \n 0    Inorganic\n1         Acid\nName: Chemical Type, dtype: object\n\n\n\n\nDescriptive Statistics\n\ndata.info(): Returns the information of the dataframe\n\n\nprint(\"Information of data: \\n\", data.info()) # information of data\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8 entries, 0 to 7\nData columns (total 7 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   Name                      8 non-null      object \n 1   Formula                   8 non-null      object \n 2   Molecular Weight (g/mol)  8 non-null      float64\n 3   Viscosity (mPa·s)         8 non-null      float64\n 4   pH (Acidity)              8 non-null      float64\n 5   Chemical Type             8 non-null      object \n 6   Concentration (M)         8 non-null      float64\ndtypes: float64(4), object(3)\nmemory usage: 580.0+ bytes\nInformation of data: \n None\n\n\nStatistical Summary\n\ndata.describe(): Returns the statistical summary of the dataframe\n\n\nprint(\"Statistical summary: \\n\", data.describe()) # statistical summary\n\nStatistical summary: \n        Molecular Weight (g/mol)  Viscosity (mPa·s)  pH (Acidity)  \\\ncount                  8.000000           8.000000      8.000000   \nmean                  54.939250         179.997000      6.728750   \nstd                   32.052446         497.871465      2.905421   \nmin                   17.030000           0.260000      1.000000   \n25%                   28.533750           0.488000      6.625000   \n50%                   52.075000           0.826000      7.000000   \n75%                   81.605000           6.900000      7.347500   \nmax                   98.079000        1412.000000     11.600000   \n\n       Concentration (M)  \ncount           8.000000  \nmean            9.825000  \nstd            19.411318  \nmin             0.100000  \n25%             0.725000  \n50%             1.100000  \n75%             5.625000  \nmax            55.500000  \n\n\nStatistical Measures\n\ndata[\"Concentration (M)\"].mean(): Returns the mean of the column “Concentration (M)”\ndata[\"Concentration (M)\"].std(): Returns the standard deviation of the column “Concentration (M)”\ndata[\"Concentration (M)\"].median(): Returns the median of the column “Concentration (M)”\ndata[\"Concentration (M)\"].max(): Returns the maximum of the column “Concentration (M)”\ndata[\"Concentration (M)\"].min(): Returns the minimum of the column “Concentration (M)”\ndata[\"Concentration (M)\"].sum(): Returns the sum of the column “Concentration (M)”\ndata[\"Concentration (M)\"].mode(): Returns the mode of the column “Concentration (M)”\ndata[\"Concentration (M)\"].percentile(q): Returns the q-th percentile of the column “Concentration (M)”\n\n\nprint(\"mean of concentration: \\n\", data[\"Concentration (M)\"].mean()) # mean of concentration\nprint(\"std of concentration: \\n\", data[\"Concentration (M)\"].std()) # standard deviation of concentration\nprint(\"median of concentration: \\n\", data[\"Concentration (M)\"].median()) # median of concentration\nprint(\"max of concentration: \\n\", data[\"Concentration (M)\"].max()) # max of concentration\nprint(\"min of concentration: \\n\", data[\"Concentration (M)\"].min()) # min of concentration\nprint(\"sum of concentration: \\n\", data[\"Concentration (M)\"].sum()) # sum of concentration\nprint(\"mode of concentration: \\n\", data[\"Concentration (M)\"].mode()) # mode of concentration\nprint(\"percentile of concentration: \\n\", data[\"Concentration (M)\"].quantile(0.25)) # 25th percentile of concentration\nprint(\"IQR of concentration: \\n\", data[\"Concentration (M)\"].quantile(0.75) - data[\"Concentration (M)\"].quantile(0.25)) # IQR of concentration\n\nmean of concentration: \n 9.825\nstd of concentration: \n 19.41131849499888\nmedian of concentration: \n 1.1\nmax of concentration: \n 55.5\nmin of concentration: \n 0.1\nsum of concentration: \n 78.6\nmode of concentration: \n 0     0.1\n1     0.5\n2     0.8\n3     1.0\n4     1.2\n5     1.5\n6    18.0\n7    55.5\nName: Concentration (M), dtype: float64\npercentile of concentration: \n 0.7250000000000001\nIQR of concentration: \n 4.9\n\n\n\ndata.groupby(\"Chemical Type\")[\"Concentration (M)\"].mean(): Returns the mean of the column “Concentration (M)” grouped by “Chemical Type”\n\n\nprint(\"Mean of concentration group by type: \\n\", data.groupby(\"Chemical Type\")[\"Concentration (M)\"].mean())\n\nMean of concentration group by type: \n Chemical Type\nAcid                    18.00\nAlcohol                  1.25\nAromatic Hydrocarbon     0.10\nBase                     0.50\nInorganic               55.50\nKetone                   0.80\nPolyol                   1.20\nName: Concentration (M), dtype: float64\n\n\n\ndata.agg({\"Concentration (M)\": [\"mean\", \"std\", \"median\", \"max\", \"min\", \"sum\"]}): Returns the mean, standard deviation, median, maximum, minimum, and sum of the column “Concentration (M)”\n\n\nprint(\"Aggregate the data: \\n\", data.agg({\"Concentration (M)\": [\"mean\", \"std\", \"median\", \"max\", \"min\", \"sum\"]}))\n\nAggregate the data: \n         Concentration (M)\nmean             9.825000\nstd             19.411318\nmedian           1.100000\nmax             55.500000\nmin              0.100000\nsum             78.600000\n\n\nCorrelation and Covariance\n\ndata.corr(): Returns the correlation matrix of the dataframe\ndata.cov(): Returns the covariance matrix of the dataframe\n\n\nprint(\"Correlation matrix: \\n\", data[\"Concentration (M)\"].corr(data[\"Molecular Weight (g/mol)\"]))\nprint(\"Covariance matrix: \\n\", data[\"Concentration (M)\"].cov(data[\"Molecular Weight (g/mol)\"]))\n\nCorrelation matrix: \n -0.29516999429071966\nCovariance matrix: \n -183.6489357142857\n\n\nPivot Tables Pivot tables are used to summarize and aggregate data inside a dataframe. - data.pivot_table(index=\"Chemical Type\", columns=\"Concentration (M)\", values=\"Molecular Weight (g/mol)\", aggfunc=\"mean\"): Returns a pivot table of the dataframe\n\ndata.pivot_table(index=\"Chemical Type\", columns=\"Concentration (M)\", values=\"Molecular Weight (g/mol)\", aggfunc=\"mean\")\n\n\n\n\n\n\n\nConcentration (M)\n0.1\n0.5\n0.8\n1.0\n1.2\n1.5\n18.0\n55.5\n\n\nChemical Type\n\n\n\n\n\n\n\n\n\n\n\n\nAcid\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n98.079\nNaN\n\n\nAlcohol\nNaN\nNaN\nNaN\n46.07\nNaN\n32.04\nNaN\nNaN\n\n\nAromatic Hydrocarbon\n78.11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nBase\nNaN\n17.03\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nInorganic\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n18.015\n\n\nKetone\nNaN\nNaN\n58.08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nPolyol\nNaN\nNaN\nNaN\nNaN\n92.09\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nComparison of Data\n\ndata1 = data.compare(data2): Compares two dataframes and returns the differences\n\n\ndata1 = data.copy()\ndata1[\"Concentration (M)\"].iloc[0] = 0.5\nprint(\"Comparison of data: \\n\", \ndata.compare(data1))\n\nComparison of data: \n   Concentration (M)      \n               self other\n0              55.5   0.5\n\n\nWindow Functions\nWindow functions are used to perform calculations on a subset of the data. It is useful for calculating moving averages, cumulative sums, and other rolling calculations. It helps to smooth out the data and identify trends.\n\ndf.rolling(...): perform rolling window calculations\ndf.rolling(...).mean(): calculate the rolling mean\ndf.rolling(...).mean(): calculate the rolling mean\ndf.rolling(...).sum(): calculate the rolling sum\ndf.rolling(...).std(): calculate the rolling standard deviation\ndf.rolling(...).min(): calculate the rolling minimum\ndf.rolling(...).max(): calculate the rolling maximum\ndf.rolling(...).apply(...): apply a function to the rolling window\ndf.rolling(...).agg(...): aggregate the data in the rolling window\ndf.rolling(...).transform(...): transform the data in the rolling window\ndf.rolling(...).count(): count the data in the rolling window\n\n\n# calculate the rolling mean\nprint(\"rolling mean: \\n\", data[\"Concentration (M)\"].rolling(window=2).mean())\n\nrolling mean: \n 0      NaN\n1    36.75\n2     9.50\n3     0.90\n4     0.65\n5     0.30\n6     0.80\n7     1.35\nName: Concentration (M), dtype: float64",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Descriptive Statistics and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DescriptiveStatistics.html#example-descriptive-statistics",
    "href": "course/chapters/SEDA/DescriptiveStatistics.html#example-descriptive-statistics",
    "title": "Descriptive Statistics and Analysis",
    "section": "Example: Descriptive Statistics",
    "text": "Example: Descriptive Statistics",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Descriptive Statistics and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/ModelTesting.html",
    "href": "course/chapters/SEDA/ModelTesting.html",
    "title": "Model Testing",
    "section": "",
    "text": "Hypothesis Testing\nDifficulty level:\nHypothesis testing is a statistical method for making decisions or inferences about a population based on a sample. It is widely used in chemistry and materials science to validate experimental results and compare datasets.\n# Example: Null hypothesis - The mean is equal to 0\n#H0: μ = 0\n\n# Alternative hypothesis - The mean is not equal to 0\n#H1: μ ≠ 0\n\nfrom scipy import stats\nimport numpy as np\n\ndata = [1, 2, 3, 4, 5]\n\n# Perform a one-sample t-test\nt_stat, p_value = stats.ttest_1samp(data, 0)\n\n# Print results\nprint(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")\n\n# Interpretation\nif p_value &lt; 0.05:\n    print(\"Reject the null hypothesis: The mean is significantly different from 0.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference from 0.\")\n\nT-Statistic: 4.2426, P-Value: 0.0132\nReject the null hypothesis: The mean is significantly different from 0.\nfrom scipy import stats\nimport numpy as np\n# Generate two random samples (e.g., control and treatment groups)\nnp.random.seed(42)\ncontrol = np.random.normal(loc=50, scale=10, size=30)  # Mean=50, Std=10\ntreatment = np.random.normal(loc=55, scale=10, size=30)  # Mean=55, Std=10\n\n# Perform an independent t-test\nt_stat, p_value = stats.ttest_ind(control, treatment)\n\n# Print results\nprint(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")\n\n# Interpretation\nif p_value &lt; 0.05:\n    print(\"Reject the null hypothesis: The groups have significantly different means.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between groups.\")\n\nT-Statistic: -2.3981, P-Value: 0.0197\nReject the null hypothesis: The groups have significantly different means.\nfrom scipy import stats\nimport numpy as np\n# Generate paired data (e.g., before and after measurements)\nnp.random.seed(42)\nbefore = np.random.normal(loc=50, scale=10, size=30)  # Mean=50, Std=10\nafter = before + np.random.normal(loc=5, scale=2, size=30)  # Mean=55, Std=2\n\n# Perform a paired t-test\nt_stat, p_value = stats.ttest_rel(before, after)\n\n# Print results\nprint(f\"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}\")\n\n# Interpretation\nif p_value &lt; 0.05:\n    print(\"Reject the null hypothesis: The means are significantly different.\")\nelse:\n    print(\"Fail to reject the null hypothesis: No significant difference between means.\")\n\nT-Statistic: -13.9936, P-Value: 0.0000\nReject the null hypothesis: The means are significantly different.\nfrom scipy import stats\nimport numpy as np\n\n# Example: Comparing multiple groups\ngroup1 = [10.2, 14.5, 13.3, 9.8, 12.7]\ngroup2 = [9.5, 13.1, 12.9, 8.7, 11.3]\ngroup3 = [11.2, 15.1, 14.3, 10.8, 13.7]\n\n# Perform ANOVA test\nf_stat, p_value = stats.f_oneway(group1, group2, group3)\nprint(f\"F-statistic: {f_stat}, P-value: {p_value}\")\n\nF-statistic: 1.1840438281116243, P-value: 0.3393857967100532\nfrom scipy import stats\nimport numpy as nps\nfrom scipy.stats import chi2_contingency\n\n# Example: Comparing categorical data\nobserved = [[10, 20, 30], [6, 9, 17]]\nchi2, p, dof, expected = chi2_contingency(observed)\nprint(f\"Chi-square: {chi2}, P-value: {p}\")\n\nChi-square: 0.27157465150403504, P-value: 0.873028283380073\nCheck the SciPy documentation for more statistical tests and functions.\nAnd for more information on hypothesis testing, refer to the SciPy Hypothesis Tests.",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Testing</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/ModelTesting.html#example-model-testing",
    "href": "course/chapters/SEDA/ModelTesting.html#example-model-testing",
    "title": "Model Testing",
    "section": "Example: Model Testing",
    "text": "Example: Model Testing",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Model Testing</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataModels.html",
    "href": "course/chapters/SEDA/DataModels.html",
    "title": "Inferential Statistics",
    "section": "",
    "text": "Data Models\nDifficulty level:",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataModels.html#linear-regression",
    "href": "course/chapters/SEDA/DataModels.html#linear-regression",
    "title": "Inferential Statistics",
    "section": "Linear Regression ",
    "text": "Linear Regression \nOften we want to find a model which can explain the data. It is important to understand the data and the model to be able to interpret the results and make predictions.\nThe simplest model is the linear regression model. It assumes that the data has a linear relationship with the target variable. For example, if we have a single feature \\(x\\) and a target variable \\(y\\), the linear regression model can be defined as:\n\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\nwhere \\(y\\) is the target variable, \\(x\\) is the feature, \\(\\beta_0\\) and \\(\\beta_1\\) are the coefficients, and \\(\\epsilon\\) is the error term.\nFor multiple features \\(x_1, x_2, \\ldots, x_n\\), the linear regression model can be defined as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon\n\\]\nwhere \\(y\\) is the target variable, \\(x_1, x_2, \\ldots, x_n\\) are the features, \\(\\beta_0, \\beta_1, \\ldots, \\beta_n\\) are the coefficients, and \\(\\epsilon\\) is the error term.\nIn python there exists serveral libraries which can be used to fit a linear regression model.\n\nUsing scipy\nA easy way is to use scipy library. The scipy library has a function called linregress which can be used to fit a linear regression model. The function returns the slope, intercept, r-value, p-value, and the standard error of the estimate. And with scipy version 1.15.2 also the intercept error.\n\nfrom scipy.stats import linregress\nimport numpy as np\n\nx = [1, 2, 3, 4, 5]\ny = x*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)\n\nresults = linregress(x, y)\n\nslope = results.slope\nintercept = results.intercept\nr_value = results.rvalue\np_value = results.pvalue\nstd_err = results.stderr\nintercept_err = results.intercept_stderr\nprint(\"slope: %f    intercept: %f\" % (slope, intercept))\nprint(\"R-squared: %f\" % r_value**2)\nprint(\"p-value: %f\" % p_value)\nprint(\"standard error: %f\" % std_err)\nprint(\"Intercept error: %f\" %intercept_err)\n# Two-sided inverse Students t-distribution\n# p - probability, df - degrees of freedom\nfrom scipy.stats import t\ntinv = lambda p, df: abs(t.ppf(p/2, df))\nprint(\"95% confidence interval: \" + str(intercept - tinv(0.05, len(x)-2)*intercept_err) + \" to \" + str(intercept + tinv(0.05, len(x)-2)*intercept_err))\n\nslope: 1.231036    intercept: -2.649882\nR-squared: 0.452755\np-value: 0.213236\nstandard error: 0.781393\nIntercept error: 2.591588\n95% confidence interval: -10.897470253074465 to 5.5977058953928935\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor older version scipyonly returned 5 values with fields slope, intercept, rvalue, pvalue and stderr. For compatiblity reasons the return values are 5 elements tuple.\n\nfrom scipy.stats import linregress\nslope, intercept, r, p, se = linregress(x, y)\nprint(\"slope: \", slope)\nprint(\"intercept: \", intercept)\nprint(\"r-value: \", r)\nprint(\"p-value: \", p)\nprint(\"standard error: \", se)\n\nslope:  1.2310362343312684\nintercept:  -2.649882178840785\nr-value:  0.6728706714443222\np-value:  0.21323637259341802\nstandard error:  0.7813930326669261\n\n\nAnd if you want to get the intercept error you can use the following return value as a object:\n\nfrom scipy.stats import linregress\nresults = linregress(x, y)\nprint(\"slope: \", results.slope)\nprint(\"intercept: \", results.intercept)\nprint(\"r-value: \", results.rvalue)\nprint(\"p-value: \", results.pvalue)\nprint(\"standard error: \", results.stderr)\nprint(\"intercept error: \", results.intercept_stderr)\n\nslope:  1.2310362343312684\nintercept:  -2.649882178840785\nr-value:  0.6728706714443222\np-value:  0.21323637259341802\nstandard error:  0.7813930326669261\nintercept error:  2.5915875031541136\n\n\n\n\n\n\nUsing statsmodels\nAnother library is statsmodels. The statsmodels library provides more detailed information about the model, such as the coefficients, standard errors, t-values, p-values, and confidence intervals.\n\nimport statsmodels.api as sm\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\ny = X*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)\n\nX_with_const = sm.add_constant(X)  # Add intercept term\nmodel = sm.OLS(y, X_with_const).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.140\nModel:                            OLS   Adj. R-squared:                 -0.147\nMethod:                 Least Squares   F-statistic:                    0.4882\nDate:                Fri, 11 Apr 2025   Prob (F-statistic):              0.535\nTime:                        10:47:45   Log-Likelihood:                -10.644\nNo. Observations:                   5   AIC:                             25.29\nDf Residuals:                       3   BIC:                             24.51\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -3.1133      2.754     -1.131      0.340     -11.876       5.650\nx1             0.5801      0.830      0.699      0.535      -2.062       3.222\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   2.278\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.492\nSkew:                           0.121   Prob(JB):                        0.782\nKurtosis:                       1.483   Cond. No.                         8.37\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\nUsing scikit-learn\nOne of the most popular libraries is scikit-learn. The following code shows how to fit a linear regression model using scikit-learn:\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\nX = np.linspace(1, 5, 100).reshape(-1, 1)\ny = 2 * X + 1 + np.random.normal(0, 1, 100).reshape(-1, 1)\n\n# Model fitting\nmodel = LinearRegression()\nmodel.fit(X, y)\nprint(\"Parameters:\", model.get_params())\nprint(\"R-squared:\", model.score(X, y))\n\n# Predictions\ny_pred = model.predict(X)\n\n# Plot\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Fit')\nplt.legend()\nplt.show()\n\nParameters: {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}\nR-squared: 0.8639230673135491",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/DataModels.html#non-linear-fits",
    "href": "course/chapters/SEDA/DataModels.html#non-linear-fits",
    "title": "Inferential Statistics",
    "section": "Non-Linear Fits",
    "text": "Non-Linear Fits\nLinear regression may not always be sufficient, especially for complex relationships. Non-linear models provide more flexibility.\n\nPolynomial Regression\nPolynomial regression is a type of linear regression where the relationship between the independent variable \\(x\\) and the dependent variable \\(y\\) is modeled as an \\(n\\)-th degree polynomial.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Sample data\nX = np.linspace(1, 5, 100)\ny = X**2 + np.random.normal(0, 1, 100)\n\nmodel = np.polyfit(X, y, 2)\n\nprint(\"Coefficients:\", model)\n\n# Create a polynomial function\n\nmodel = np.poly1d(model)\n\nX_range = np.linspace(1, 5, 100)\ny_fit = model(X_range)\n\nprint(model)\n\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X_range, y_fit, color='purple', label='Polynomial Fit')\nplt.legend()\n\nCoefficients: [ 1.03224626 -0.1478871   0.19436177]\n       2\n1.032 x - 0.1479 x + 0.1944\n\n\n\n\n\n\n\n\n\n\n\nCurve Fitting with scipy\nscipy provides the curve_fit function to fit a non-linear model to the data. The function requires the model function and the data as input.\n\nfrom scipy.optimize import curve_fit\n\ndef nonlinear_func(x, a, b, c):\n    return a * np.sin(b * x) + c\n\npopt,pov  = curve_fit(nonlinear_func, X.flatten(), y)\nperr = np.sqrt(np.diag(pov))\n\nprint(\"Fitted parameters:\", popt)\nprint(\"Parameter errors:\", perr)\n\nX_range = np.linspace(1, 5, 100)\ny_fit = nonlinear_func(X_range, *popt)\n\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X_range, y_fit, color='purple', label='Non-Linear Fit')\nplt.legend()\nplt.show()\n\nFitted parameters: [4.48522067 1.82133974 9.98870145]\nParameter errors: [0.94678227 0.06746611 0.66789667]",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/AdvancedAnalysis.html",
    "href": "course/chapters/SEDA/AdvancedAnalysis.html",
    "title": "Advanced Statistical Analysis",
    "section": "",
    "text": "Dimensionality Reduction Techniques\nDifficulty level:\nOften data has multi dimensions and it is not easy to analyze it.  Multivariate analysis is a set of statistical techniques used to analyze data with multiple variables. It helps in understanding the relationships between variables and identifying patterns in complex data.\nWhat can be done to analyze multi-dimensional data?\nPrincipal Component Analysis (PCA): reduces the dimensionality of data by transforming variables into uncorrelated components. It retains most of the variance in the data while reducing noise and redundancy. The PCA decomposition is based on the eigenvalues and eigenvectors of the covariance matrix of the data to identify the principal components with the highest variance.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Create a simple 2D dataset\nX = np.array([\n    [2.5, 2.4],\n    [0.5, 0.7],\n    [2.2, 2.9],\n    [1.9, 2.2],\n    [3.1, 3.0],\n    [2.3, 2.7],\n    [2, 1.6],\n    [1, 1.1],\n    [1.5, 1.6],\n    [1.1, 0.9]\n])\n\n# Plot original data\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], color='blue')\nplt.title(\"Original 2D Data\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\n\n# Apply PCA to reduce to 1D\npca = PCA(n_components=1)\nX_pca = pca.fit_transform(X)\nX_projected = pca.inverse_transform(X_pca)\n\n# Plot the projected data\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:, 0], X[:, 1], label='Original', alpha=0.6)\nplt.scatter(X_projected[:, 0], X_projected[:, 1], label='Projected (1D)', alpha=0.6)\nfor orig, proj in zip(X, X_projected):\n    plt.plot([orig[0], proj[0]], [orig[1], proj[1]], 'r--', linewidth=0.5)\nplt.title(\"PCA Projection to 1D\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.legend()\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\nSometimes the data needs to be scaled before applying PCA. The StandardScaler from sklearn can be used to scale the data.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Generate synthetic data\nrng = np.random.RandomState(0)\nn_samples = 100\ncov = [[6, 1], [8, 4]]\nX = rng.multivariate_normal(mean=[0, 0], cov=cov, size=n_samples)\n\n# Standardizing Data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Applying PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n\n# Plot PCA results\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Analysis')\nplt.show()\n\nExplained Variance Ratio: [0.81292552 0.18707448]\nPCR (Principal Component Regression): Combines PCA and regression analysis. It first applies PCA to reduce dimensionality and then performs regression on the principal components. (see for more information https://en.wikipedia.org/wiki/Principal_component_regression and https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n# Generate synthetic data\nrng = np.random.RandomState(0)\nn_samples = 100\ncov = [[6, 1], [8, 4]]\nX = rng.multivariate_normal(mean=[3, 5], cov=cov, size=n_samples)\n\n# generate PCA components\npca = PCA(n_components=2).fit(X)\nIndependent Component Analysis (ICA): Separates a multivariate signal into additive subcomponents.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import FastICA\n\n# Sample dataset\nX = np.random.rand(10, 3)\nica = FastICA(n_components=2)\nX_ica = ica.fit_transform(X)\n\n# Plot ICA results\nplt.scatter(X_ica[:, 0], X_ica[:, 1])\nplt.xlabel('ICA1')\nplt.ylabel('ICA2')\nplt.title('Independent Component Analysis')\nplt.show()\nt-Distributed Stochastic Neighbor Embedding (t-SNE): Visualizes high-dimensional data in lower dimensions.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\n\n# Sample dataset\nX = np.random.rand(10, 3)\ntsne = TSNE(n_components=2,perplexity=3,learning_rate='auto')\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\nplt.xlabel('t-SNE1')\nplt.ylabel('t-SNE2')\nplt.title('t-SNE Analysis')\nplt.show()\nUMAP (Uniform Manifold Approximation and Projection): Reduces the dimensionality of data while preserving local and global structure.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport umap\n\n# Sample dataset\nX = np.random.rand(10, 3)\numap_model = umap.UMAP(n_components=2)\nX_umap = umap_model.fit_transform(X)\n\n# Plot UMAP results\nplt.scatter(X_umap[:, 0], X_umap[:, 1])\nplt.xlabel('UMAP1')\nplt.ylabel('UMAP2')\nplt.title('UMAP Analysis')\nplt.show()\nFactor Analysis: Identifies latent factors that explain the variance in observed variables.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import FactorAnalysis\n\n# Sample dataset\nX = np.random.rand(10, 3)\nfa = FactorAnalysis(n_components=2)\nX_fa = fa.fit_transform(X)\n\n# Plot Factor Analysis results\nplt.scatter(X_fa[:, 0], X_fa[:, 1])\nplt.xlabel('Factor 1')\nplt.ylabel('Factor 2')\nplt.title('Factor Analysis')\nplt.show()\nCanonical Correlation Analysis (CCA): Analyzes the relationship between two sets of variables.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_decomposition import CCA\n\n# Sample dataset\nX = np.random.rand(10, 3)\nY = np.random.rand(10, 3)\ncca = CCA(n_components=2)\nX_c, Y_c = cca.fit_transform(X, Y)\n\n# Plot CCA results\nplt.scatter(X_c[:, 0], Y_c[:, 0])\nplt.xlabel('CCA1')\nplt.ylabel('CCA2')\n\nplt.title('Canonical Correlation Analysis')\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Advanced Statistical Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/AdvancedAnalysis.html#dimensionality-reduction-techniques",
    "href": "course/chapters/SEDA/AdvancedAnalysis.html#dimensionality-reduction-techniques",
    "title": "Advanced Statistical Analysis",
    "section": "",
    "text": "Clustering Techniques\nClustering is an unsupervised learning technique used to group similar data points based on patterns. In chemistry and materials science, clustering helps in categorizing material properties, identifying experimental trends, and classifying samples.\n\nk-Means Clustering: Partitions data into k clusters based on similarity.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Example dataset\nX = np.random.rand(20, 2)\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nlabels = kmeans.labels_\n\n# Plot Clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.title('K-Means Clustering')\nplt.show()\n\n\n\n\n\n\n\n\n\nHierarchical Clustering: Builds a tree of clusters using agglomerating or divisive methods.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Example dataset\nX = np.random.rand(20, 2)\nagg = AgglomerativeClustering(n_clusters=3)\nlabels = agg.fit_predict(X)\n\n# Plot Clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.title('Hierarchical Clustering')\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Advanced Statistical Analysis</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseA_2.html",
    "href": "course/exercises/exercises/exerciseA_2.html",
    "title": "Exercise A 2",
    "section": "",
    "text": "Temperature in Synthesis Reactor Part 2\nIn this exercise, we will repeat not-linear fits.\nLet’s analyze how bad the situation is in reactor 3 is. To do this, we will fit a sigmoid curve to the data set. Try to remember which kind of function we used to fit the non-linear data in the lecture.\nIn our case the sigmoid function is given by:\n\\[\nT(t) = T_0 + \\Delta T \\frac{1}{1 + e^{-a (t - t_0)}}\n\\]\nwith the four fitting parameters being:\nUse your results from Part 1 of this exercise.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Exercise A 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseA_2.html#temperature-in-synthesis-reactor-part-2",
    "href": "course/exercises/exercises/exerciseA_2.html#temperature-in-synthesis-reactor-part-2",
    "title": "Exercise A 2",
    "section": "",
    "text": "temperature baseline $T_0 $,\nthe temperature increase \\(\\Delta T\\),\nthe growth rate \\(a\\),\nthe time at the midpoint of the temperature increase \\(t_0\\)\n\n\n\nTask 1:\n\nFit a sigmoid function to the data set of reactor 3.\nPlot the data and the fitted curve.\n\n\n\nQuestions:\n\nHow much is the tempearture changed in reactor 3?\nHow fast this change appears?\nWhen does the temperature reach the midpoint of the change?\n\n\n\nTask2:\n\nFit a suitable distribution function over the histograms plots of the temperature data\n\n\n\nQuestions:\n\nWhat can you say about the distribution of the temperature data?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Exercise A 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_1.html",
    "href": "course/exercises/exercises/exerciseB_1.html",
    "title": "Exercise B 1",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Exercise B 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_1.html#linear-regression-calibration-ir-spectroscopy",
    "href": "course/exercises/exercises/exerciseB_1.html#linear-regression-calibration-ir-spectroscopy",
    "title": "Exercise B 1",
    "section": "Linear Regression: Calibration IR spectroscopy",
    "text": "Linear Regression: Calibration IR spectroscopy\nIn this exercise, we will repeat linear regressions.\n\nWe will learn how to use np.polyfit to perform linear regression.\nWe will learn how to use scipy.stats.linregress to perform linear regression.\nWe will learn how to use sklearn.linear_model.LinearRegression to perform linear regression.\nWe learn what are the differences between these three methods.\nWe will learn how to use the linear regression to calculate the concentration of a unknown sample.\n\nIn quantitative spectroscopy we measure the (dimensionless) absorbance \\(A\\) of a beam of light at a selected frequency/wavelength to determine the concentration \\(c\\) of a sample. To do this, we first calibrate the device using solutions with known concentration.\n\nData\nIn our exercise we compare the calibrations of two groups of students available in the files ir_spectroscopy1.dat and ir_spectroscopy2.dat.\n\n\nData Path:\ndata_path = \"https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/Linear_regression/\"\n\n\nTask\n\nLoad the data from the files ir_spectroscopy1.dat and ir_spectroscopy2.dat.\nPerform a linear regression on the data of each group.\nCompare the results of the three methods.\nUse the linear regression to predict the concentration of our unknown sample measured at the absorption of \\(A_\\mathrm{sample} = 0.753\\).\n\n\n\nQuestions\n\nWhat is the concentration of the unknown sample?\nAre the differences between the three methods?\nWhich method would you recommend for this type of data?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Exercise B 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_2.html",
    "href": "course/exercises/exercises/exerciseB_2.html",
    "title": "Exercise B 2",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Exercise B 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_2.html#linear-regression-and-correlation-experimental-vs.-theoretical-data",
    "href": "course/exercises/exercises/exerciseB_2.html#linear-regression-and-correlation-experimental-vs.-theoretical-data",
    "title": "Exercise B 2",
    "section": "Linear Regression and Correlation: Experimental vs. Theoretical Data",
    "text": "Linear Regression and Correlation: Experimental vs. Theoretical Data\nIn this exercise, we will repeat linear regressions and use it for demonstration of the correlation between experimental and theoretical data.\n\nWe will learn how to use create symmetrical plots.\nWe will learn to use z-scores outlier test.\n\nIn this exercise we compare experimentally measured electrochemical potentials of anthraquinone (AQ) derivatives with those calculated using theoretical methods. We want to find out how well the calculation can reproduce the experimental data. And check if there are any outliers in the data.\nAnthraquinone is an aromatic organic molecule, with manifold applications. In nature, AQ and its derivatives are found in plants and microorganisms due to its key role in reversible redox reactions. For this reason AQ derivates are also key components in industrial processes and material development (such as battery research at the University of Innsbruck).\n\nData\nThe data for this exercise is taken from a recent collaboration between the Theoretical Chemistry Department and the Institute for Physical chemistry. (Phys.Chem.Chem.Phys. 2022, 24, 16207 – 16219 ).\nThe experimental data is stored in the file experimental.dat and the theoretical data is stored in the file theory.dat.\nThe data contains the following columns:\n\n1-OH : the name of the AQ derivative\n-530.0 : the first redox potential in V\n-1178.5: the second redox potential in V\n\n\n\nData Path:\nhttps://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/Anthraquinone/\n\n\nTask\n\nLoad the data from the files experimental.dat and theory.dat.\nHave fist a look at the data.\nCreate a correlation plot of the experimental and theoretical data.\nPerform a linear regression of the data.\nCalculate the correlation coefficient.\nCheck for outliers in the data using the z-scores method.\n\n\n\nQuestions\n\nHow well do the theoretical data reproduce the experimental data?\nAre there any outliers in the data?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Exercise B 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_3.html",
    "href": "course/exercises/exercises/exerciseB_3.html",
    "title": "Exercise B 3",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Exercise B 3</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_3.html#arrhenious-plot",
    "href": "course/exercises/exercises/exerciseB_3.html#arrhenious-plot",
    "title": "Exercise B 3",
    "section": "Arrhenious Plot",
    "text": "Arrhenious Plot\nIn this exercise, we will repeat linear regressions and how to use the logarithm of the\n\nWe will learn how to use create np.log.\nWe will learn how to select a specific range of data.\nWe will learn how to use scipy.linregress to perform linear regression.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Exercise B 3</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseB_3.html#exercise-b.3-arrhenius-curve-fitting",
    "href": "course/exercises/exercises/exerciseB_3.html#exercise-b.3-arrhenius-curve-fitting",
    "title": "Exercise B 3",
    "section": "Exercise B.3 – Arrhenius Curve Fitting",
    "text": "Exercise B.3 – Arrhenius Curve Fitting\nIn this exercise we have a look at diffusion data (either from experiment or theoretical calculations) and want to obtain the activation energy \\(E_A\\).\nMany dynamical properties in chemistry (such as rate constants, diffusion, etc.) follow an Arrhenian temperature dependency given as\n\\[ y(T) = y_0 e^{-\\frac{E_A}{RT}} \\]\nwith \\(y_0\\) being the pre-exponential factor, R and T are the molar gas constant (8.3145 J mol\\(^{-1}\\) K \\(^{-1}\\)) and temperature, respectively.\nOne potential option to obtain \\(E_A\\) is a non-linear exponential fit, but this is known to be less reliable than its linear counterpart!\nConsequently, Mr. Svante Arrhenius used a linearization of the equation, which in case of the diffusion coefficient looks like this:\n\\[ ln(D) = ln(D_0) - \\frac{E_A}{R}\\frac{1}{T} \\]\nThis corresponds to a linear equation\n\\[ y = a\\cdot x + b\\]\n\\[y = ln(D)\\]\n\\[ x= \\frac{1}{T}\\]\nApplying standard linear regression we obtain\n\\[a=ln(D_0)\\] \\[b = -\\frac{E_A}{R}\\]\nFrom this we can directly access the activation energy \\(E_A\\) via: \\[ E_A = - a \\cdot R \\]\nEasy! :D\nThis time the files are in csv-format (= comma separated values), i.e. the different data columns are separated by comma symbols.\nLuckily, we can again use the command np.loadtext(), but we have to indicate the comma by adding delimiter = “,”.\nMost programs such as Excel, Origin and scientific software can write data sets in this format. If you want to use python in your research, this is most likely the most common file format to input you data sets.\n\nData\nWe have three data sets with diffusion coefficients \\(D\\) in nm \\(^2\\) /ps at different temperatures \\(T\\) in K.\n\nD_vs_T_v1.csv (Diffusion coefficient vs. temperature)\nD_vs_T_v2.csv (Diffusion coefficient vs. temperature)\nD_vs_T_v3.csv (Diffusion coefficient vs. temperature)\n\n\n\nData Path:\nhttps://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/Arrhenius/\n\n\nTask\n\nLoad the data from the files D_vs_T_v1.csv, D_vs_T_v2.csv and D_vs_T_v3.csv into numpy arrays.\nCreate a plot of the diffusion coefficient \\(D\\) vs. temperature \\(T\\).\nCreate a plot of the logarithm of the diffusion coefficient \\(ln(D)\\) vs. \\(1/T\\).\nPerform a linear regression of the data using scipy.stats.linregress only in the linear region of the data.\nCalculate the activation energy \\(E_A\\) from the slope of the linear regression.\n\n\n\nQuestions\n\nWhich data set has the highest activation energy?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Exercise B 3</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/AdvancedPlotTypes.html",
    "href": "course/chapters/PlotTypes/AdvancedPlotTypes.html",
    "title": "Advanced Plot Types",
    "section": "",
    "text": "Multiple Correlation Analysis\nDifficulty level:\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nrs = np.random.RandomState(33)\ndf = pd.DataFrame(data=rs.normal(size=(100, 4)), columns=['A', 'B', 'C', 'D'])\n\n\n# Compute correlation matrix\ncorr_matrix = df.corr()\n\n# Visualizing with a heatmap\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n\nplt.show()",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/AdvancedPlotTypes.html#pca-plot",
    "href": "course/chapters/PlotTypes/AdvancedPlotTypes.html#pca-plot",
    "title": "Advanced Plot Types",
    "section": "PCA Plot",
    "text": "PCA Plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample dataset\nX = np.random.rand(10, 3)\n\n# Standardizing Data\nscaler = StandardScaler()\n\nX_scaled = scaler.fit_transform(X)\n\n# Applying PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Plot PCA results\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Analysis')\nplt.show()",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/AdvancedPlotTypes.html#network-plot",
    "href": "course/chapters/PlotTypes/AdvancedPlotTypes.html#network-plot",
    "title": "Advanced Plot Types",
    "section": "Network Plot",
    "text": "Network Plot\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nG = nx.Graph()\nG.add_node(1)\nG.add_nodes_from([2, 3])\nG.add_edge(1, 2)\nG.add_edges_from([(1, 2), (1, 3)])\nnx.draw(G, with_labels=True)\nplt.show()",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/AdvancedPlotTypes.html#d-plot",
    "href": "course/chapters/PlotTypes/AdvancedPlotTypes.html#d-plot",
    "title": "Advanced Plot Types",
    "section": "3D Plot",
    "text": "3D Plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = np.linspace(-5, 5, 100) \ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.sin(np.sqrt(X**2 + Y**2))\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nplt.show()",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Advanced Plot Types</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/InteractivePlots.html",
    "href": "course/chapters/PlotTypes/InteractivePlots.html",
    "title": "Interactive Plots",
    "section": "",
    "text": "Interactive Plots\nDifficulty level:",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interactive Plots</span>"
    ]
  },
  {
    "objectID": "course/chapters/PlotTypes/InteractivePlots.html#interactive-plots",
    "href": "course/chapters/PlotTypes/InteractivePlots.html#interactive-plots",
    "title": "Interactive Plots",
    "section": "",
    "text": "Matplotlib and ipywidget\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\ndef plot_sine(frequency):\n    plt.plot(x, np.sin(frequency*x))\n    plt.show()\n  \nfrequency_slider = widgets.FloatSlider(value=1, min=0.1, max=10, step=0.1)\nwidgets.interactive(plot_sine, frequency=frequency_slider)\n\n\n\n\n\n\n\nPlotly\nPloty is a library that allows you to create interactive plots and dashboards, see the Plotly website.\n\nimport plotly.express as px\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig = px.line(x=x, y=y, title='Sine function')\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nBokeh\nBokeh is a library that allows you to create interactive plots and dashboards, see the Bokeh website.\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nimport numpy as np\n\noutput_notebook()\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\np = figure(title='Sine function')\np.line(x, y)\nshow(p)\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAltair\nAltair is a library that allows you to create interactive plots and dashboards, see the Altair website.\n\nimport altair as alt\nimport pandas as pd\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\ndf = pd.DataFrame({'x': x, 'y': y})\n\nalt.Chart(df).mark_line().encode(\n    x='x',\n    y='y'\n).properties(\n    title='Sine function'\n)",
    "crumbs": [
      "Data Visualization 2",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Interactive Plots</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseD_1.html",
    "href": "course/exercises/exercises/exerciseD_1.html",
    "title": "Exercise D 1",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise D 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseD_1.html#d-plot",
    "href": "course/exercises/exercises/exerciseD_1.html#d-plot",
    "title": "Exercise D 1",
    "section": "3D Plot",
    "text": "3D Plot\nIn this exercise, we will repeat to plot the 3D data.\n\nData\nThe 3D data is given in the file grid_data.dat. The data contains the following columns: - \\(r_\\mathrm{OH_1}\\) distance from O to H1 atom in Angstrom - \\(r_\\mathrm{OH_2}\\) distance from O to H2 atom in Angstrom - \\(V\\) potential energy in kcal/mol\n\n\nData Path:\ndata_path = \"https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/3D_data/\"\n\n\nTask\n\nLoad the data from the files grid_data.dat.\nCreate a 3D plot of the data using the plot_surface function.\nUse a meaningful color map.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Exercise D 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseD_2.html",
    "href": "course/exercises/exercises/exerciseD_2.html",
    "title": "Exercise D 2",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exercise D 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseD_2.html#interactive-plot",
    "href": "course/exercises/exercises/exerciseD_2.html#interactive-plot",
    "title": "Exercise D 2",
    "section": "Interactive Plot",
    "text": "Interactive Plot\nIn this exercise, we will repeat how to create an interactive plot using ipywidgets.\n\nTask 1\n\nCreate an interactive plot using ipywidgets and matplotlib for the van-Deemeter equation where the user can change the parameters \\(a\\), \\(b\\) and \\(c\\) using sliders.\n\n\n\nTask 1\n\nCreate an interactive plot using ipywidgets and matplotlib for a rate equation together with an Arrhenius Plot where the user can change the parameters \\(k_0\\),and \\(E_a\\) using sliders.",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Exercise D 2</span>"
    ]
  },
  {
    "objectID": "course/chapters/SpecialAnalysisPlots/Spectroscopy.html",
    "href": "course/chapters/SpecialAnalysisPlots/Spectroscopy.html",
    "title": "Analysis of Spectroscopy Data",
    "section": "",
    "text": "Spectroscopy Data Analysis\nDifficulty level:\nSpectroscopy is one of the most used experimental techniques in chemistry e.g.:\nThe analysis of spectroscopy needs often some preprocessing steps e.g.:",
    "crumbs": [
      "Special Analysis and Visualization Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analysis of Spectroscopy Data</span>"
    ]
  },
  {
    "objectID": "course/chapters/SpecialAnalysisPlots/Spectroscopy.html#spectroscopy-data-analysis",
    "href": "course/chapters/SpecialAnalysisPlots/Spectroscopy.html#spectroscopy-data-analysis",
    "title": "Analysis of Spectroscopy Data",
    "section": "",
    "text": "UV/Vis spectroscopy\nIR spectroscopy\nNMR spectroscopy\nMass spectroscopy\n\n\n\nBaseline correction\nSmoothing\nNormalization\nPeak assignment\nPeak fitting\nPeak integration\nPeak deconvolution\n\n\nFeature Detection\nIn spectroscopy data, peaks are often the most important features.\nMaxima and minima are often used to identify the peaks in the data.\nGlobal maxima and minima can be found using the min and max, np.min and np.max or df['column_name'].min() and df['column_name'].max() functions.\nLocal maxima and minima can be found also by using the argrelextrema function from the scipy.signal module.\n\nimport numpy as np\nfrom scipy.signal import argrelextrema\nimport matplotlib.pyplot as plt\n\n# Generate some data transmission data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\ny[30:40] = np.exp(-y[30:40]) + 0.5\n\n# Find local maxima and minima\nlocal_max = argrelextrema(y, np.greater,order=200)[0]\nlocal_min = argrelextrema(y, np.less)[0]\n\n# Plot the data\nplt.plot(x, y, label='Data')\nplt.plot(x[local_min], y[local_min], 'o', color='green', label='Local Minima')\nplt.scatter(x[local_max], y[local_max], color='red', label='Local Maxima')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Local Maxima and Minima')\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nfind_peaks function from the scipy.signal module can be used to find the peaks in the data. It finds the local maxima in the data and returns the indices of the peaks. If you want to find the minima, you can simply invert the data by multiplying it with -1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\n\n# Generate some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\ny[30:40] = np.exp(-y[30:40]) + 0.5\n# Find peaks\npeaks, _ = find_peaks(y, height=0.5, distance=10)\n# Plot the data\nplt.plot(x, y, label='Data')\nplt.plot(x[peaks], y[peaks], 'o', color='red', label='Peaks')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Peaks in Data')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nYou can also use the height, threshold, distance, prominence and width parameters to filter the peaks to enhance the peak detection.\n\nheight: Minimum height of the peaks\nthreshold: Minimum vertical distance to its neighboring samples\ndistance: Minimum horizontal distance (in samples) between neighboring peaks\nprominence: Minimum prominence of the peaks\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\n\n# Generate some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\ny[30:40] = np.exp(-y[30:40]) + 0.5\n# Find peaks\npeaks, _ = find_peaks(y, height=0.5, distance=10, prominence=0.5, width=1)\n# Plot the data\nplt.plot(x, y, label='Data')    \nplt.plot(x[peaks], y[peaks], 'o', color='red', label='Peaks')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Peaks in Data')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSmoothing\nOften data is noisy and needs to be smoothed before further analysis.\n\nMoving average\nWeighted moving average\nGaussian filter\nSavitzky-Golay filter\n\nMoving Average\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Generate some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\n\n# Apply the moving average\nwindow_size = 5\n\nsmoothed_data =np.convolve(y, np.ones(window_size)/window_size, mode='same')\n\n# Plot the data\nplt.plot(x, y, label='Data')\nplt.plot(x, smoothed_data, label='Smoothed Data', color='red')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Moving Average')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nGaussian Filter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\n# Generate some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\n# Apply the Gaussian filter\nsigma = 2\nsmoothed_data = gaussian_filter1d(y, sigma=sigma)\n# Plot the data\nplt.plot(x, y, label='Data')\n\nplt.plot(x, smoothed_data, label='Smoothed Data', color='red')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Gaussian Filter')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nSavitzky-Golay Filter\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import savgol_filter\n# Generate some data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, x.size)\n# Apply the Savitzky-Golay filter\nwindow_size = 5\npoly_order = 2\nsmoothed_data = savgol_filter(y, window_size, poly_order)\n# Plot the data\nplt.plot(x, y, label='Data')\nplt.plot(x, smoothed_data, label='Smoothed Data', color='red')\nplt.ylabel('Y-axis')\nplt.xlabel('X-axis')\nplt.title('Savitzky-Golay Filter')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBaseline Correction\nA baseline correction is often needed to remove background noise from the signal and\nThe baseline correction can be done by different methods e.g.:\n\nPolynomial fitting\nSpline fitting\nMinimum value fitting\nMoving average etc.\n\nThere exists also a package called pybaselines which can be used to correct the baseline of the data.\n\n!pip install pybaselines\n\nCollecting pybaselines\n  Downloading pybaselines-1.2.0-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: numpy&gt;=1.20 in /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages (from pybaselines) (2.2.4)\nRequirement already satisfied: scipy&gt;=1.6 in /opt/hostedtoolcache/Python/3.12.9/x64/lib/python3.12/site-packages (from pybaselines) (1.15.2)\nDownloading pybaselines-1.2.0-py3-none-any.whl (209 kB)\nInstalling collected packages: pybaselines\nSuccessfully installed pybaselines-1.2.0\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter1d\nfrom pybaselines import Baseline\nfrom pybaselines.utils import gaussian\n\n\n# Generate some data\nx = np.linspace(20, 1000, 1000)\nsignal = (\n    + gaussian(x, 6, 240, 5)\n    + gaussian(x, 8, 350, 11)\n    + gaussian(x, 15, 400, 18)\n    + gaussian(x, 6, 550, 6)\n    + gaussian(x, 13, 700, 8)\n    + gaussian(x, 9, 800, 9)\n    + gaussian(x, 9, 880, 7)\n)\nbaseline = 5 + 6 * np.exp(-(x - 40) / 30) + gaussian(x, 5, 1000, 300)\nnoise = np.random.default_rng(0).normal(0, 0.1, len(x))\ny = signal + baseline + noise\n\n# Use the Baseline class to fit the baseline\nbaseline_fitter = Baseline(x_data=x)\n# The baseline_fitter object can be used to fit the baseline with different methods in that case the Asymmetrically Reweighted Penalized Least Squares (ARPLS)\nstiff_baseline = baseline_fitter.arpls(y, lam=5e5)[0]\n# Correct the data by subtracting the baseline\ndata_corrected = y - stiff_baseline\n\n# Create subplots\nfig, axs = plt.subplots(3, 1, figsize=(8, 12))\naxs[0].plot(x, y, label='Data')\naxs[0].plot(x, signal, label='Signal')\naxs[0].plot(x, baseline, label='Baseline')\naxs[0].set_ylabel('Y-axis')\naxs[0].set_xlabel('X-axis')\naxs[0].set_title('Data with Baseline')\naxs[0].legend()\naxs[1].plot(x, y, label='Data')\naxs[1].plot(x, stiff_baseline, label='Stiff Baseline')\naxs[1].set_ylabel('Y-axis')\naxs[1].set_xlabel('X-axis')\naxs[1].set_title('Stiff Baseline')\naxs[1].legend()\naxs[2].plot(x, y, label='Data')\naxs[2].plot(x, data_corrected, label='Corrected Data')\naxs[2].set_ylabel('Y-axis')\naxs[2].set_xlabel('X-axis')\naxs[2].set_title('Corrected Data')\naxs[2].legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Special Analysis and Visualization Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Analysis of Spectroscopy Data</span>"
    ]
  },
  {
    "objectID": "course/chapters/SpecialAnalysisPlots/ImageAnalysis.html",
    "href": "course/chapters/SpecialAnalysisPlots/ImageAnalysis.html",
    "title": "Analysis of Image Data",
    "section": "",
    "text": "Image Data Analysis\nDifficulty level:",
    "crumbs": [
      "Special Analysis and Visualization Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Analysis of Image Data</span>"
    ]
  },
  {
    "objectID": "course/chapters/SpecialAnalysisPlots/Ternary.html",
    "href": "course/chapters/SpecialAnalysisPlots/Ternary.html",
    "title": "Ternary Plot",
    "section": "",
    "text": "Difficulty level:   \n\nTernary Plot\nA ternary plot is a type of plot that is used to visualize the composition of three components.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ternary\n\n# Create a figure\nfig, tax = ternary.figure(scale=1.0)\n\n# Draw Boundary and Gridlines\ntax.boundary(linewidth=2.0)\ntax.gridlines(multiple=0.2, color=\"blue\")\n\n# Set Axis labels and Title\nfontsize = 10\ntax.set_title(\"Ternary Plot\", fontsize=fontsize)\ntax.left_axis_label(\"Component A\", fontsize=fontsize)\ntax.right_axis_label(\"Component B\", fontsize=fontsize)\ntax.bottom_axis_label(\"Component C\", fontsize=fontsize)\n\n# Plot some data\npoints = np.random.dirichlet((1, 1, 1), 10)\ntax.scatter(points, marker='s', color='red', label=\"Data\")\n\n# Legend\ntax.legend()\nplt.show()",
    "crumbs": [
      "Special Analysis and Visualization Techniques",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Ternary Plot</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseC_1.html",
    "href": "course/exercises/exercises/exerciseC_1.html",
    "title": "Exercise C 1",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Exercise C 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseC_1.html#spectroscopic-data-analysis",
    "href": "course/exercises/exercises/exerciseC_1.html#spectroscopic-data-analysis",
    "title": "Exercise C 1",
    "section": "Spectroscopic data analysis",
    "text": "Spectroscopic data analysis\nIn this exercise, we will repeat how spectroscopic data can be analyzed effectively via Python SciPylibrary.\n\nWe will learn denoising and smoothing of data (SM).\nWe will learn how to correct the baseline of data (BC).\nWe will learn how to detect peaks in data (PD).\n\nThese kind of analysis methods can be relevant when depicting X-ray diffraction patterns, spectrograms obtained from various techniques (e.g. IR/Raman, UV/Vis, X-ray, NMR, …), chromatograms and electropherograms and many more.\nOften the experimental data is subject to noise or has a notable offset in the baseline.\nSciPy offers a number of straightforward tools to quickly enhance the data to make it ready for plotting.\nLet’s start by reading in some X-ray diffraction data.\n\nData\nThe data is a simple X-ray diffraction pattern of a crystalline material. The experimental data is stored in XRD_experimental.datand the theoretical data in XRD_theory.dat.\nFirst column represents the 2-theta angle in degrees, the second column represents the intensity in arbitrary units.\n\n\nData Path:\ndata_path = 'https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/X-ray_Diffraction/'\n\n\nTask\n\nLoad the data from the files XRD_experimental.dat and XRD_theory.dat.\nHave fist a look at the data.\nNormalize the data to the maximum intensity to 100.\nSmooth the experimental data using a Gaussian filter with a standard deviation of 2.0.\nFind the peaks in the experimental data using the find_peaks function from scipy.signal.\nPlot the smoothed experimental data.\nAdd a baseline correction to the experimental data using the minimum_fiter1d function from scipy.ndimage.\nPlot the baseline corrected experimental data.\nFind the peaks in the baseline corrected data uing the find_peaks function from scipy.signal.\nCompare the smoothed experimental data with the smoothed and baseline corrected experimental data via a correlation plot and linear regression.\nCompare both experimental smoothed and baseline corrected data and theoretical data via a correlation plot and linear regression.\n\n\n\nQuestions\n\nWhat effect does the Gaussian filter have on the data?\nHow does the baseline correction affect the data?\nHow well does the smoothed data correlate with the theoretical data?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Exercise C 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseC_2.html",
    "href": "course/exercises/exercises/exerciseC_2.html",
    "title": "Exercise C 2",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Exercise C 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exerciseC_2.html#chromotography-and-spectroscopy-integration",
    "href": "course/exercises/exercises/exerciseC_2.html#chromotography-and-spectroscopy-integration",
    "title": "Exercise C 2",
    "section": "Chromotography and Spectroscopy Integration",
    "text": "Chromotography and Spectroscopy Integration\nIn this exercise, we will repeat how spectroscopic data can be integrated via Python SciPylibrary.\n\nWe will learn denoising and smoothing of data (SM).\nWe will learn how to correct the baseline of data (BC).\nWe will learn how to detect peaks in data (PD).\nWe will learn how to integrate the area under the curve (AUC).\n\nIn this example we are using the featues of finde-peaks() to determine the left and right integration limits to integrate peaks of an example chromatogram.\nTo achieve this, we first have to reduce the noise and apply a baseline correction.\nLet’s start by loading the file!\n\nData\nThe chromatographic data is stored in a file called Chromatogram.csv.\nFirst column represents the time in min and the second column represents the intensity in arbitrary units (a.u.).\n\n\nData Path:\ndata_path = \"https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/Chromatography/\"\n\n\nTask\n\nLoad the data from the files Chromatogram.csv.\nApply Gaussian smoothing and minimum filtered baseline correction.\nDetect the peaks and integrate the area under the curve.\nPlot the chromatogram with the detected peaks and the integrated area under the curve.\n\n\n\nQuestions\n\nWhat is the area under the curve for each peak?",
    "crumbs": [
      "Exercises",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Exercise C 2</span>"
    ]
  },
  {
    "objectID": "course/exercises/playground.html",
    "href": "course/exercises/playground.html",
    "title": "Exercise Playground",
    "section": "",
    "text": "Playground for Python\nYou can use the load_wine dataset from the sklearn library to practice data manipulation and visualization. Below is a code snippet that demonstrates how to load the dataset, perform some basic data analysis, and visualize it using matplotlib.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nThe load_wine dataset contains information about different types of wines, including their chemical properties and quality ratings. You can use this dataset to practice various data manipulation techniques, such as filtering, grouping, and aggregating data.\ndata = load_wine()\ndata\n\n{'data': array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n         1.065e+03],\n        [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n         1.050e+03],\n        [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n         1.185e+03],\n        ...,\n        [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n         8.350e+02],\n        [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n         8.400e+02],\n        [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n         5.600e+02]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2]),\n 'frame': None,\n 'target_names': array(['class_0', 'class_1', 'class_2'], dtype='&lt;U7'),\n 'DESCR': '.. _wine_dataset:\\n\\nWine recognition dataset\\n------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 178\\n:Number of Attributes: 13 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - Alcohol\\n    - Malic acid\\n    - Ash\\n    - Alcalinity of ash\\n    - Magnesium\\n    - Total phenols\\n    - Flavanoids\\n    - Nonflavanoid phenols\\n    - Proanthocyanins\\n    - Color intensity\\n    - Hue\\n    - OD280/OD315 of diluted wines\\n    - Proline\\n    - class:\\n        - class_0\\n        - class_1\\n        - class_2\\n\\n:Summary Statistics:\\n\\n============================= ==== ===== ======= =====\\n                                Min   Max   Mean     SD\\n============================= ==== ===== ======= =====\\nAlcohol:                      11.0  14.8    13.0   0.8\\nMalic Acid:                   0.74  5.80    2.34  1.12\\nAsh:                          1.36  3.23    2.36  0.27\\nAlcalinity of Ash:            10.6  30.0    19.5   3.3\\nMagnesium:                    70.0 162.0    99.7  14.3\\nTotal Phenols:                0.98  3.88    2.29  0.63\\nFlavanoids:                   0.34  5.08    2.03  1.00\\nNonflavanoid Phenols:         0.13  0.66    0.36  0.12\\nProanthocyanins:              0.41  3.58    1.59  0.57\\nColour Intensity:              1.3  13.0     5.1   2.3\\nHue:                          0.48  1.71    0.96  0.23\\nOD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\nProline:                       278  1680     746   315\\n============================= ==== ===== ======= =====\\n\\n:Missing Attribute Values: None\\n:Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners:\\n\\nForina, M. et al, PARVUS -\\nAn Extendible Package for Data Exploration, Classification and Correlation.\\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science.\\n\\n.. dropdown:: References\\n\\n    (1) S. Aeberhard, D. Coomans and O. de Vel,\\n    Comparison of Classifiers in High Dimensional Settings,\\n    Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Technometrics).\\n\\n    The data was used with many others for comparing various\\n    classifiers. The classes are separable, though only RDA\\n    has achieved 100% correct classification.\\n    (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\\n    (All results using the leave-one-out technique)\\n\\n    (2) S. Aeberhard, D. Coomans and O. de Vel,\\n    \"THE CLASSIFICATION PERFORMANCE OF RDA\"\\n    Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\\n    Mathematics and Statistics, James Cook University of North Queensland.\\n    (Also submitted to Journal of Chemometrics).\\n',\n 'feature_names': ['alcohol',\n  'malic_acid',\n  'ash',\n  'alcalinity_of_ash',\n  'magnesium',\n  'total_phenols',\n  'flavanoids',\n  'nonflavanoid_phenols',\n  'proanthocyanins',\n  'color_intensity',\n  'hue',\n  'od280/od315_of_diluted_wines',\n  'proline']}\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf['target'] = data.target\ndf['targe_name'] = df['target'].map({0: 'Class 0', 1: 'Class 1', 2: 'Class 2'})\ndf.style \\\n  .format(precision=2) \\\n  .highlight_max(axis=0, color='lightgreen') \\\n  .highlight_min(axis=0, color='lightcoral') \\\n  .set_caption(\"Wine Dataset Features\") \\\n  .set_table_styles(\n    [{'selector': 'th.col0', 'props': [('color', 'blue'), ('font-weight', 'bold')]}]\n  ) \\\n  .set_properties(**{'text-align': 'left'}) \\\n  .set_table_attributes('style=\"width: 100%; border: 1px solid black;\"') \\\n  .set_table_styles(\n    [{'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black')]}],\n    axis=0\n  )\ndisplay(df)\n\n\n\n\n\n\n\n\nalcohol\nmalic_acid\nash\nalcalinity_of_ash\nmagnesium\ntotal_phenols\nflavanoids\nnonflavanoid_phenols\nproanthocyanins\ncolor_intensity\nhue\nod280/od315_of_diluted_wines\nproline\ntarget\ntarge_name\n\n\n\n\n0\n14.23\n1.71\n2.43\n15.6\n127.0\n2.80\n3.06\n0.28\n2.29\n5.64\n1.04\n3.92\n1065.0\n0\nClass 0\n\n\n1\n13.20\n1.78\n2.14\n11.2\n100.0\n2.65\n2.76\n0.26\n1.28\n4.38\n1.05\n3.40\n1050.0\n0\nClass 0\n\n\n2\n13.16\n2.36\n2.67\n18.6\n101.0\n2.80\n3.24\n0.30\n2.81\n5.68\n1.03\n3.17\n1185.0\n0\nClass 0\n\n\n3\n14.37\n1.95\n2.50\n16.8\n113.0\n3.85\n3.49\n0.24\n2.18\n7.80\n0.86\n3.45\n1480.0\n0\nClass 0\n\n\n4\n13.24\n2.59\n2.87\n21.0\n118.0\n2.80\n2.69\n0.39\n1.82\n4.32\n1.04\n2.93\n735.0\n0\nClass 0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n173\n13.71\n5.65\n2.45\n20.5\n95.0\n1.68\n0.61\n0.52\n1.06\n7.70\n0.64\n1.74\n740.0\n2\nClass 2\n\n\n174\n13.40\n3.91\n2.48\n23.0\n102.0\n1.80\n0.75\n0.43\n1.41\n7.30\n0.70\n1.56\n750.0\n2\nClass 2\n\n\n175\n13.27\n4.28\n2.26\n20.0\n120.0\n1.59\n0.69\n0.43\n1.35\n10.20\n0.59\n1.56\n835.0\n2\nClass 2\n\n\n176\n13.17\n2.59\n2.37\n20.0\n120.0\n1.65\n0.68\n0.53\n1.46\n9.30\n0.60\n1.62\n840.0\n2\nClass 2\n\n\n177\n14.13\n4.10\n2.74\n24.5\n96.0\n2.05\n0.76\n0.56\n1.35\n9.20\n0.61\n1.60\n560.0\n2\nClass 2\n\n\n\n\n178 rows × 15 columns\nWhat you can do with this dataset:",
    "crumbs": [
      "Playground",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exercise Playground</span>"
    ]
  },
  {
    "objectID": "course/exercises/playground.html#first-information",
    "href": "course/exercises/playground.html#first-information",
    "title": "Exercise Playground",
    "section": "First information",
    "text": "First information\n\nprint(\"Number of samples:\", df.shape[0])\nprint(\"Number of features:\", df.shape[1] - 2)  # Exclude target columns\nprint(\"Number of classes:\", df['target'].nunique())\nprint(\"Class distribution:\")\nprint(df['target'].value_counts())\n\nNumber of samples: 178\nNumber of features: 13\nNumber of classes: 3\nClass distribution:\ntarget\n1    71\n0    59\n2    48\nName: count, dtype: int64",
    "crumbs": [
      "Playground",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exercise Playground</span>"
    ]
  },
  {
    "objectID": "course/exercises/playground.html#statistical-summary",
    "href": "course/exercises/playground.html#statistical-summary",
    "title": "Exercise Playground",
    "section": "Statistical summary",
    "text": "Statistical summary\n\ndf.describe().T.style \\\n    .format(precision=2) \\\n    .highlight_max(axis=0, color='lightgreen') \\\n    .highlight_min(axis=0, color='lightcoral') \\\n    .set_caption(\"Wine Dataset Features Statistics\") \\\n    .set_table_styles(\n        [{'selector': 'th', 'props': [('color', 'blue'), ('font-weight', 'bold')]}]\n    ) \\\n    .set_properties(**{'text-align': 'left'}) \\\n    .set_table_attributes('style=\"width: 100%; border: 1px solid black;\"') \\\n    .set_table_styles(\n        [{'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('color', 'black')]}],\n        axis=0\n    )\n\n\n\n\n\n\nTable 37.1: Wine Dataset Features Statistics\n\n\n\n\n\n \ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nalcohol\n178.00\n13.00\n0.81\n11.03\n12.36\n13.05\n13.68\n14.83\n\n\nmalic_acid\n178.00\n2.34\n1.12\n0.74\n1.60\n1.87\n3.08\n5.80\n\n\nash\n178.00\n2.37\n0.27\n1.36\n2.21\n2.36\n2.56\n3.23\n\n\nalcalinity_of_ash\n178.00\n19.49\n3.34\n10.60\n17.20\n19.50\n21.50\n30.00\n\n\nmagnesium\n178.00\n99.74\n14.28\n70.00\n88.00\n98.00\n107.00\n162.00\n\n\ntotal_phenols\n178.00\n2.30\n0.63\n0.98\n1.74\n2.35\n2.80\n3.88\n\n\nflavanoids\n178.00\n2.03\n1.00\n0.34\n1.21\n2.13\n2.88\n5.08\n\n\nnonflavanoid_phenols\n178.00\n0.36\n0.12\n0.13\n0.27\n0.34\n0.44\n0.66\n\n\nproanthocyanins\n178.00\n1.59\n0.57\n0.41\n1.25\n1.56\n1.95\n3.58\n\n\ncolor_intensity\n178.00\n5.06\n2.32\n1.28\n3.22\n4.69\n6.20\n13.00\n\n\nhue\n178.00\n0.96\n0.23\n0.48\n0.78\n0.96\n1.12\n1.71\n\n\nod280/od315_of_diluted_wines\n178.00\n2.61\n0.71\n1.27\n1.94\n2.78\n3.17\n4.00\n\n\nproline\n178.00\n746.89\n314.91\n278.00\n500.50\n673.50\n985.00\n1680.00\n\n\ntarget\n178.00\n0.94\n0.78\n0.00\n0.00\n1.00\n2.00\n2.00",
    "crumbs": [
      "Playground",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Exercise Playground</span>"
    ]
  },
  {
    "objectID": "course/exercises/recap_python.html",
    "href": "course/exercises/recap_python.html",
    "title": "Exercise Recap Python",
    "section": "",
    "text": "Quiz",
    "crumbs": [
      "Repetition",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exercise Recap Python</span>"
    ]
  },
  {
    "objectID": "course/exercises/recap_python.html#exercise-recap-python",
    "href": "course/exercises/recap_python.html#exercise-recap-python",
    "title": "Exercise Recap Python",
    "section": "Exercise Recap Python",
    "text": "Exercise Recap Python\nOpen it locally:\nExercise Recap Python\nOpen it in Colab: Exercise Recap Python",
    "crumbs": [
      "Repetition",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Exercise Recap Python</span>"
    ]
  },
  {
    "objectID": "informations/license.html",
    "href": "informations/license.html",
    "title": "License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Stefanie Kröll and Thomas Hofer\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>License</span>"
    ]
  },
  {
    "objectID": "informations/privacy.html",
    "href": "informations/privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Hosting: GitHub Pages\nThis website does not collect, store or process any personally identifiable information and does not use cookies or other tracking technologies.\nThis website is hosted on GitHub Pages. GitHub says at",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Privacy Policy</span>"
    ]
  },
  {
    "objectID": "informations/privacy.html#hosting-github-pages",
    "href": "informations/privacy.html#hosting-github-pages",
    "title": "Privacy Policy",
    "section": "",
    "text": "Github-Docs &gt; GitHub Pages &gt; Get started &gt; About GitHub Pages (https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages):\n\n\n“When a GitHub Pages site is visited, the visitor’s IP address is logged and stored for security purposes, regardless of whether the visitor has signed into GitHub or not. For more information about GitHub’s security practices, see GitHub Privacy Statement”",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Privacy Policy</span>"
    ]
  },
  {
    "objectID": "informations/impressum.html",
    "href": "informations/impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "This Data Analysis and Visualization for Chemists and Material Scientists tutorial is created by Stefanie Kröll, MSc and a part of the exercises are created by Assoz.-Prof. Dr. Thomas Hofer as part of the lecture course “VU Data Science and Visualization Primer for Chemists and Material Scientists” at University of Innsbruck, Austria.\n\n\nContact Information\nWrite to us if you have any questions or suggestions:\nPlease contact us via email:\ns.kroell@student.uibk.ac.at\n\n\nPrivacy Policy\nExternal Link Policy and Disclaimers\n\n\n\nAcknowledgements\nThis tutorial is based on the following sources:\n\nScientific Computing for Chemists with Python\nAn introduction to Python for Chemistry",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Impressum</span>"
    ]
  },
  {
    "objectID": "informations/disclaimer.html",
    "href": "informations/disclaimer.html",
    "title": "Disclaimers",
    "section": "",
    "text": "Disclaimer:\nAll information provided on this website is offered “as is” without any warranties, expressed or implied, including, but not limited to, warranties of merchantability, fitness for a particular purpose, or non-infringement. While efforts are made to ensure the accuracy of the information, the website owner makes no guarantee of its correctness or completeness. Use the information at your own discretion and risk.\n\n\n\n\n\n\nExternal Link Policy\n\n\n\nIt is imperative to note that the external links present on this website are intended to serve as a reference point, providing insights into software programs, packages, tools and tutorials that can be utilised. The primary function of these links is to facilitate further information on the subject matter. It is crucial to acknowledge that the website author is not held responsible for the content or information found on external websites. Furthermore, the website author does not exercise any control over the content of such external websites. It is, therefore, incumbent upon the user to exercise due diligence when engaging with external links. The user uses the links at his own risk and is subject to the terms of services of the respective providers.\n\n\nFor some code snippets, examples and rephrasing sentences, the help of AI tools like ChatGPT, Copilot and DeepL was used.",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Disclaimers</span>"
    ]
  }
]