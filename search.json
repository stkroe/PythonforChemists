[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "",
    "text": "Who is the course for?\nç# Overview\nWelcome to the Data Analysis and Visualization Course for Chemists and Material Scientists! This course is designed for better understanding how Python can be used for data analysis and visualization in the fields of chemistry and materials science. Before diving into the course content, please take a moment to review the following important informations.\nThis course has been designed for persons who are looking to undertake a specialised data visualization and analyzing course in Python with a particular focus the field of chemistry and materials science. The course tries to be interactivley with practical examples.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "index.html#what-can-you-expect-from-this-course",
    "href": "index.html#what-can-you-expect-from-this-course",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "What can you expect from this course",
    "text": "What can you expect from this course\n\nThis course wants to show you the advantage of using a programming language for data analyzing and visualization in chemistry and material science in comparision to gui-based softwares.\nThe course will give you a very brief introduction to Python and its libraries.\nThe main focus will be on the libraries numpy, pandas, pandas, scipy, matplotlib and seaborn for data analyzing and visualization.\nThe course is organized interactivly. You will get the chance to practice with exercises.\nUpon successful completion of this course, participants will have acquired a comprehensive understanding of the fundamental components of Python and the key packages necessary for the analysis and presentation of their own research data.\nYou can test your knowledge by an exam example at the end of this course.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "index.html#what-can-you-not-expect-from-this-course",
    "href": "index.html#what-can-you-not-expect-from-this-course",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "What can you NOT expect from this course",
    "text": "What can you NOT expect from this course\n\nYou will not get a deep explanation of the Python language. Please consider full Python tutorials for get a deep overview of Python.\nYou will not learn object-oriented programming in Python.\nWe will not go into details of the libraries.\nThis is not a statistics course.\n\nThe aim of this course should be that you get an idea how Python can be use for data analyzing and visualization in chemistry and material science field.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "index.html#how-this-course-is-structured",
    "href": "index.html#how-this-course-is-structured",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "How this course is structured",
    "text": "How this course is structured\nThis course is divided into three possible paths to accomplish the learning objectives:\n\nBeginner Path: Focuses on first contact with Python, covering basic concepts and introductory modules.\nAdvanced Path: Provides a deeper dive into different libraries and specialized visualization plots.\nChallenging Path: Explores advanced topics and complex data visualization techniques, with more difficult exercises and in-depth analysis.\n\nEach lecture includes examples related to chemistry and material science. At the end of each part, there are exercises to test your understanding and reinforce the concepts learned. These exercises are designed to be practical and relevant to real-world scenarios in chemistry and materials science.\nComprehensive Exam\nAt the very end of the course, there is a comprehensive exam which covers a complete data analyis and visualization example.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "index.html#this-course-within-of-the-vu-data-science-and-visualization-primer-for-chemists-and-material-scientists",
    "href": "index.html#this-course-within-of-the-vu-data-science-and-visualization-primer-for-chemists-and-material-scientists",
    "title": "Data Analysis and Visualization for Chemists and Material Scientists",
    "section": "This course within of the “VU Data Science and Visualization Primer for Chemists and Material Scientists”",
    "text": "This course within of the “VU Data Science and Visualization Primer for Chemists and Material Scientists”\nWe concentrate on the Basic Path during this lecture.\nTimeschedule for a week table\n\n\n\n\n\n\n\n\n\n\n\nTime\nMonday\nTuesday\nWednesday\nThursday\nFriday\n\n\n\n\n13:00-14:00\nIntroduction/Python Basics\nLecture 2\nLecture 3\nLecture 4\nLecture 5\n\n\n14:00-15:00\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nExample 5\n\n\n15:00-16:00\nLecture 1\nExample 2\nExample 3\nExample 4\nF&Q\n\n\n16:00-17:00\nExample 1\nExample 2\nExample 3\nExample 4\nFinal Exam\n\n\n17:00-18:00\nExample 1\nExample 2\nExample 3\nExample 4\nFinal Exam",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Introduction.html",
    "href": "course/chapters/Essentials/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What do you need for data analysis and visualization?\nA very simple approach would be the use of basic spreadsheet programs with graphical user interfaces e.g. Excel, LibreOffice Calc …\nBut for more advanced analysis you need probably specifc plotting and analysis tools:\nThis is a small election of tools, that can be used for data analysis and plotting.\nFor a larger list see these wikipedia lists:",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Introduction.html#what-do-you-need-for-data-analysis-and-visualization",
    "href": "course/chapters/Essentials/Introduction.html#what-do-you-need-for-data-analysis-and-visualization",
    "title": "Introduction",
    "section": "",
    "text": "Scientists produce a lot of data which needs to be analyzed and visualized.\nClear presentation of data is essential for the understanding of the data.\nIt helps to improve your scientific communication and make your results more accessible to others.\nThere are many tools available for data analysis and visualization.\n\n\n\n\nYou can either use programs with GUIs e.g. LabPlot, QtiPlot, Scilab, SciDAVis, Origin …\nor a GUI based program with command line interface e.g. Xmgrace, GNU Octave …\nor command line based tools e.g. Gnuplot, Matlab, Mathematica …\nor programming languages e.g. Python, R, Julia …\n\n\n\n\nList of Numerical Analysis Software\nList of Graphical Software\nList of Statistical Software\nList of Computer Algebra Systems",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html",
    "href": "course/chapters/Essentials/WhatIsPython.html",
    "title": "Python?",
    "section": "",
    "text": "Short History of Python\n(see Wikipedia for more details)\nVan Rossum designed Python as a “Computer Programming Language for Everybody”.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#short-history-of-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#short-history-of-python",
    "title": "Python?",
    "section": "",
    "text": "1989: the beginning of python\n1991: Guido van Rossum, a Dutch programmer and father of Python, implemented and published the first version of Python.\nFun fact: The name Python came from the show Monty Python’s Flying Circus.\n1994: Python 1.0 was released\n2000: Python 2.0 was released\n2008: Python 3.0 was released with new syntax and features\nImportant: python3 to python2 is backward incompatible e.g. print(\"Hello World\") python3 and print \"Hello World\" python2.\npython2 is nowdays outdated. It is not recommended to use it for new projects.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#what-is-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#what-is-python",
    "title": "Python?",
    "section": "What is Python?",
    "text": "What is Python?\n\nPython is an interpreted language compared to compiled languages e.g., C or C++.\nPython is often slower compared to compiled languages.\nPython has dynamic type checking and garbage collection.\nPython supports both object-oriented and functional programming.\nPython is a high-level language, meaning it is closer to human language than the machine language understood by computers.\n\n\nSimple Scheme how Python works internally:\nSo if you execute a python programe, the python interpreter will convert the code into byte code and then the byte code will be executed by the python virtual machine (PVM).\n graph TD\n     A[Python Source Code *.py] --&gt;|Step 1: Compilation| B[Bytecode *.pyc]\n     B --&gt;|Step 2: Interpretation| C[Python Virtual Machine - PVM]\n     C --&gt;|Execution| D[Program Output]",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/WhatIsPython.html#how-i-can-learn-python",
    "href": "course/chapters/Essentials/WhatIsPython.html#how-i-can-learn-python",
    "title": "Python?",
    "section": "How I can learn Python?",
    "text": "How I can learn Python?\nThere are a lot of resources to dive deeper into Python e.g.:\n\nhttps://www.py4e.com/ (Python for Everybody)\nhttps://py-tutorial-de.readthedocs.io/de/python-3.3/ (German Tutorial)\nhttps://www.w3schools.com/python/default.asp\nhttps://jakevdp.github.io/PythonDataScienceHandbook/\nhttps://exercism.org/ (Coding Exercises)\nhttps://www.freecodecamp.org/ (Coding Exercises)\nhttps://realpython.com/tutorials/data-viz/\nhttps://www.youtube.com/watch?v=LHBE6Q9XlzI\n\n\n\n\n\n\n\nImportant\n\n\n\nAs it is with languages, you can only learn it if you practice it.  So, start you own projects and have fun with it!",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python?</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html",
    "href": "course/chapters/Essentials/InstallationGuide.html",
    "title": "Installation Guide",
    "section": "",
    "text": "How to install Python locally?\nHere is a short instruction how to install Python on your local PC or you can use Google Colab to solve all the exercises of this course.\nFundamental Python websites:",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#install-python-interpreter",
    "href": "course/chapters/Essentials/InstallationGuide.html#install-python-interpreter",
    "title": "Installation Guide",
    "section": "1. Install Python Interpreter",
    "text": "1. Install Python Interpreter\nYour preferred searching engine is your friend to find the best way to install Python on your system. Please choose that method which is suitable for you.\nPython can be install by severals ways:\n\ndirectly by Python official site\n\nthe installation guide can be found under python wiki\n\nor via package manager of your os:\n\ne.g.: sudo apt install python (linux debian) or brew install python (macOS)\n\nor via docker, wsl ect.\nor via a conda or mamba python package and environment managers which have a python interpreter on board and are avaiable for Windows, Linux and macOS [my recomendation]",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#python-package-and-environment-manager",
    "href": "course/chapters/Essentials/InstallationGuide.html#python-package-and-environment-manager",
    "title": "Installation Guide",
    "section": "2. Python Package and Environment manager",
    "text": "2. Python Package and Environment manager\nThe advantage of using a python package and environment manager is that you have a python interpreter directly on board but you can also directly create different python enviroments and install and remove python packages.\n\nConda\nThere are differerent condainstaller: (Please pay attention which one is suitable for you (https://docs.anaconda.com/distro-or-miniconda/).\n\n\n\n\n\n\nWarning\n\n\n\nPlease read the Anaconda Terms of Service FAQs and Terms of Service) not every case is free of use.\n\n\n\nAnaconda Distribution is a comprehensive distribution which includes conda and hundreds of preinstalled packages and tools.\nminiconda is the light version of it which contains only conda, python interpreter and few fundamental packages\nminiforge minimal installer for conda and using only the community conda-forge channel\n\n\n\nMamba\nAnother python package and environment managers is mamba. mamba is a reimplementation of conda: - micromamba is a statically linked version of mamba - mambaand it at the moment faster than conda\n\n\n\n\n\n\nTip\n\n\n\nRecommondation: micromamba  \nInstall it like it is explained under the micromamba documentation: - https://mamba.readthedocs.io/en/latest/installation/micromamba-installation.html\n\n\nPlease install in one of the above explained ways Python and use your preferred searching enging to get more information.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#set-up-an-environment",
    "href": "course/chapters/Essentials/InstallationGuide.html#set-up-an-environment",
    "title": "Installation Guide",
    "section": "3. Set up an environment",
    "text": "3. Set up an environment\nIt is often very useful to have different python enviroments for different python projects because of the need of different python package versions.\nYou can use conda or micromamba to create different environments. There exists also other virtual environment manager.\nIn this course the explaination is restricted to micromamba as an example. If you want to use something else there exists tons of information online how to use other programs.\n\nMicromamba: Most important commands are:\nRead for more detail: https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html\nCreating a new virtual environment:\nmicromamba create --name &lt;myenvname&gt;\nInstall new packages:\nmicromamba install &lt;packagename&gt;\nList all environments:\nmicromamba env list\nActivate an environment:\nmicromamba activate &lt;myenvname&gt;\nList all packages of this environment:\nmicromamba list",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/InstallationGuide.html#usefull-packages-for-data-analyse-and-visualization",
    "href": "course/chapters/Essentials/InstallationGuide.html#usefull-packages-for-data-analyse-and-visualization",
    "title": "Installation Guide",
    "section": "4. Usefull packages for Data Analyse and Visualization:",
    "text": "4. Usefull packages for Data Analyse and Visualization:\n\nmatplotlib - data visualization library\nnumpy - numerical library\nscipy - scientific library\npandas - data manipulation library\nseaborn - data visualization library\nscikit-learn - machine learning library\nstatsmodels - statistical library\njupyter-notebook/jupyterlab - interactive computing environment\nipykernel - IPython Kernel for Jupyter\npip - package installer for python instead of conda\n\n\n\n\n\n\n\nTip\n\n\n\nRecommendation  Use yml-file with all needed packages and configurations:\n\n\nSave this in a environment.yml file:\nname: myenv\nchannels:\n - conda-forge\ndependencies:\n - python=3.12\n - pandas\n - numpy\n - matplotlib\n - jupyterlab\n - scikit-learn\n - scipy\n - pip\n - ipykernel\n - seaborn\n - statsmodels\nand create an environment with this specific packages:\nmicromamba env create -f environment.yml\nTest your installation by opening the interactive python mode by typing in your terminal (Linux, macOS) / comand prompt (Windows):\npython\nthen something like this should be opened in your terminal (Linux, macOS) / comand prompt (Windows)\nPython 3.12.7 | packaged by conda-forge | (main, Oct  4 2024, 15:57:01) [Clang 17.0.6 ] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \nthen type:\nprint(\"Hello World!\")\nIf this works your installation was successful!",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Installation Guide</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Editors.html",
    "href": "course/chapters/Essentials/Editors.html",
    "title": "Editors",
    "section": "",
    "text": "Choose an editor\nAfter you installed Python successfully you need a editor for writing your Python programs. Technical you could use everything where you can write text but it is not really purposeful.\nAn editor with syntax highlighting, code completion and debugging is very useful.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Editors</span>"
    ]
  },
  {
    "objectID": "course/chapters/Essentials/Editors.html#difference-between-jupyter-notebooks-and-python-scripts",
    "href": "course/chapters/Essentials/Editors.html#difference-between-jupyter-notebooks-and-python-scripts",
    "title": "Editors",
    "section": "Difference Between Jupyter Notebooks and Python Scripts",
    "text": "Difference Between Jupyter Notebooks and Python Scripts\n\nInteractivity: Jupyter Notebooks allow you to run code in chunks (cells) and see the output immediately, making them ideal for experimentation and visualization. Python scripts (.py files) are typically executed all at once.\nDocumentation: Notebooks support Markdown cells for adding rich text, equations, and images alongside your code. Python scripts are plain text files and require comments for documentation.\nUse Case: Notebooks are great for exploratory data analysis and teaching, while Python scripts are better suited for production code and automation.",
    "crumbs": [
      "Essentials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Editors</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html",
    "href": "course/chapters/Python/Basics.html",
    "title": "Python Start",
    "section": "",
    "text": "Elemental Syntax\nThis will be a very fast and basic start into coding with Python.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#comands",
    "href": "course/chapters/Python/Basics.html#comands",
    "title": "Python Start",
    "section": "Comands",
    "text": "Comands\nEach line of code is a command. The computer reads the code from top to bottom and executes each command in succession order.\nFirst of all some terminology:\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nCommand\nA line of code that tells the computer to do something.\n\n\nSyntax\nThe rules that govern how commands are written.\n\n\nExecute\nTo run a command.\n\n\nComment\nA note in the code that is not executed.\n\n\nIndentation\nThe space at the beginning of a line of code that tells the computer that the line is part of a block of code.\n\n\nError\nA message that tells you that something is wrong with your code.\n\n\nStack trace\nA list of error messages that Python prints when an error occurs.\n\n\nVariable\nA name that stores a value.\n\n\nData type\nThe type of value that a variable stores.\n\n\nDeclaration\nThe process of assigning memory to a variable\n\n\nInteger\nA whole number.\n\n\nFloat\nA number with a decimal point.\n\n\nBoolean\nA value that is either True or False.\n\n\nList\nA collection of items.\n\n\nMutable\nA type of object that can be changed.\n\n\nImmutable\nA type of object that cannot be changed.\n\n\nString\nA sequence of characters.\n\n\nFunction\nA block of code that performs a specific task.\n\n\nModule\nA collection of functions and variables.\n\n\nLibrary\nA collection of modules.\n\n\nPackage\nA collection of related modules.\n\n\nScript\nA file that contains code that can be run independently.\n\n\n\n\n\n\n\n\n\nExample - Hello World\n\n\n\nprint is a command that tells the computer to display the text or variable that follows it.\n\n\nTry it out:\n\nprint(\"Hello World\")\n\nHello World\n\n\nIf you want to span a command over multiple lines you can use \\ or  if you are using parenthesis in your command you can directly use a new line.\n\n result = 1 + 2 + 3 + \\\n          7 + 8 + 9\n numbers = [\n     1, 2, 3,\n     4, 5, 6,\n     7, 8, 9\n ]",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#indentation",
    "href": "course/chapters/Python/Basics.html#indentation",
    "title": "Python Start",
    "section": "Indentation",
    "text": "Indentation\nIn Python code blocks are not structured by brackets or semicolons like C/C++ or Java but by indentation. This means that the code inside a loop or a function is indented by a tab or four spaces.\n\n\n\n\n\n\nWarning\n\n\n\nIndentations are curcial in Python. If you don’t indent your code correctly, you will get an typical beginner error.\nPay attention to not mix tabs and spaces in your code.\n\n\n\n\n\n\n\n\nExample - Wrong Indentation\n\n\n\n\n\n\nTry it out:\n\nprint(\"correct indentation\")\n    print(\"wrong indentation\")\n\n\n  Cell In[3], line 2\n    print(\"wrong indentation\")\n    ^\nIndentationError: unexpected indent\n\n\n\n\nAll the lines in the block must have the same indentation:\n\n    print(\"correct indentation\") \n    print(\"correct indentation\")\n    print(\"correct indentation\")\n\ncorrect indentation\ncorrect indentation\ncorrect indentation",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#error-messages",
    "href": "course/chapters/Python/Basics.html#error-messages",
    "title": "Python Start",
    "section": "Error messages",
    "text": "Error messages\nWhen you run a command that has an error, Python will print an error message.\nThe so called stack trace. It is a list of error messages that Python prints when an error occurs.\nIt gives you information about the error and the location of the error in your code.\nThe strack track is read from bottom to top.\nThe last line contains the error message and the line number where the error occured.\n\n\n\n\n\n\nTip\n\n\n\nOften the error messages are not very clear. You can search for the error message in the internet. Stackoverflow has a lot of answers to common errors. Or you can ask some AI e.g. ChatGPT.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExample - Cryptic Error Message What does the error message tell you? If you are not sure ask the internet or your favorite AI.\n\n\n\na = [1,2,3]\nprint(a[3])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[5], line 2\n      1 a = [1,2,3]\n----&gt; 2 print(a[3])\n\nIndexError: list index out of range\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe error message tells you that you are trying to access an index that is out of range.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#comments",
    "href": "course/chapters/Python/Basics.html#comments",
    "title": "Python Start",
    "section": "Comments",
    "text": "Comments\nComments are important. They help you and others to understand your code.  You can use the ` symbol to write comments.\nDocstrings are used to document the code for example with pydoc. They are using triple quotes `““” ““““`.\n\n# This is a comment\n\n\"\"\"\nThis is a documentation.\nYou can document your code for example by pydoc\n\"\"\"\n\n'\\nThis is a documentation.\\nYou can document your code for example by pydoc\\n'",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#if-statement",
    "href": "course/chapters/Python/Basics.html#if-statement",
    "title": "Python Start",
    "section": "if statement",
    "text": "if statement\n\nx = 0\nif x &lt; 0:\n    print(\"x &lt; 0\")\nelif x &gt; 0:\n    print(\"x &gt; 0\")\nelse:\n    print(\"x = 0\")\n\nx = 0",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#breakcontinuepass",
    "href": "course/chapters/Python/Basics.html#breakcontinuepass",
    "title": "Python Start",
    "section": "break,continue,pass",
    "text": "break,continue,pass\n\nfor i in range(10):\n    if i == 5:\n        break\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nfor i in range(10):\n    if i == 5:\n        continue\n    print(i)\n ```   \n\n```{python} \nfor i in range(10):\n    if i == 5:\n        pass\n    print(i)\n\n\n  File &lt;tokenize&gt;:5\n    ```\n    ^\nIndentationError: unindent does not match any outer indentation level",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#for-loops",
    "href": "course/chapters/Python/Basics.html#for-loops",
    "title": "Python Start",
    "section": "For Loops",
    "text": "For Loops\n\nfor i in range(5): #from 0 to 4 \n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\nfor i in range(1,10,2): # start 1, stop 10 excluded, step 2\n    print(i)\n\n1\n3\n5\n7\n9\n\n\n\nl = list(range(0,10))\n\n\nl\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nfor i in l:  # using list\n    print(i)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#while-loops",
    "href": "course/chapters/Python/Basics.html#while-loops",
    "title": "Python Start",
    "section": "While Loops",
    "text": "While Loops\n\nx = 0\nwhile x  &lt; 4:\n    print(l[x])\n    x = x + 1\n\n0\n1\n2\n3\n\n\n\nprint(\"while loop with continue and break statement\")\nn = 0\nwhile(n &lt; 10):\n    n+=1\n    if n == 5:\n        continue\n    if n == 7:\n        print(\"The loop reached 7 and will break now.\")\n        break\n    print(n)\n\nwhile loop with continue and break statement\n1\n2\n3\n4\n6\nThe loop reached 7 and will break now.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#functions",
    "href": "course/chapters/Python/Basics.html#functions",
    "title": "Python Start",
    "section": "Functions",
    "text": "Functions\nFunctions are defined using the def keyword.  You can use the return keyword to return a value from a function.  The parameters of a function are defined in the parentheses.  Multiple parameters are separated by commas.  You can use default values for the parameter e.g. b=5.  Multiple return values are separated by commas.  They are stored in a tuple. \n\ndef summation(a,b=5):\n    return a+b, a-b\n\n\nsummation(4,2)\n\n(6, 2)\n\n\n\nsum, sub = summation(4)\nprint(sum)\nprint(sub)\n\n9\n-1\n\n\n\nx = 3\n\ndef multiple_return_value(x,a,b):\n    n = x+a\n    m = x-b\n    return [n,m]\nprint(multiple_return_value(x,5,10)[0],multiple_return_value(x,5,10)[1])\n\n8 -7",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Basics.html#style-guideline-for-writing-python-code",
    "href": "course/chapters/Python/Basics.html#style-guideline-for-writing-python-code",
    "title": "Python Start",
    "section": "Style guideline for writing python code",
    "text": "Style guideline for writing python code\nFor writing a readable code, it is important to follow a style guideline.  The most common style guideline for Python is PEP 8.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Python Start</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/Intermediate.html",
    "href": "course/chapters/Python/Intermediate.html",
    "title": "Python Intermediate",
    "section": "",
    "text": "Often we need with user input, files, system and paths. In this chapter we will cover these topics.\n\nI/O (Input/Output)\nYou can use the print() function to print a message to the screen.  You can use the input() function to get input from the user.  You can use the open() function to open a file.  You can use the write() function to write to a file.  You can use the read() function to read from a file.  You can use the close() function to close a file.  You can use the with statement to open a file and automatically close it when you are done.  You can use the os module to work with files and directories.  You can use the sys module to work with command line arguments.  You can use the argparse module to work also with command line arguments. \nThis should print “Hello World!” to the console\n\nprint(\"Hello World!\")\n\nHello World!\n\n\nThis should ask the user to enter a number and print it to the console\n\nprint(input(\"Enter a number: \"))\n\n\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 print(input(\"Enter a number: \"))\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/ipykernel/kernelbase.py:1281, in Kernel.raw_input(self, prompt)\n   1279 if not self._allow_stdin:\n   1280     msg = \"raw_input was called, but this frontend does not support input requests.\"\n-&gt; 1281     raise StdinNotImplementedError(msg)\n   1282 return self._input_request(\n   1283     str(prompt),\n   1284     self._parent_ident[\"shell\"],\n   1285     self.get_parent(\"shell\"),\n   1286     password=False,\n   1287 )\n\nStdinNotImplementedError: raw_input was called, but this frontend does not support input requests.\n\n\n\nThis should write “Hello World!” to the file “file.txt”\n\nopen(\"file.txt\", \"w\").write(\"Hello World!\") \n\n12\n\n\nThis should read the file “file.txt” and print the content to the console\n\nprint(open(\"file.txt\").read()) \n\nHello World!\n\n\nThis should print “Hello World!” to the console without a newline\n\nprint(\"Hello World without newline.\", end=\"\") \nprint(\"Next print statement.\")\n\nHello World without newline.Next print statement.\n\n\nThis should read the file “file.txt” and print the content to the console\n\nwith open(\"file.txt\", \"r\") as file: print(file.read()) \n\nHello World!\n\n\n\n\nSystem\nThere are a lot of modules in Python to work with the system.  You can use the os module to work with files and directories.  You can use the sys module to work with command line arguments.  You can use the argparse module to work also with command line arguments. \nMost important functions are:  - os.getcwd() to get the current working directory.  - os.chdir() to change the current working directory.  - os.listdir() to list the files in a directory.  - os.mkdir() to create a directory.  - os.rmdir() to remove a directory.  - os.remove() to remove a file.  - os.rename() to rename a file.  - os.path.exists() to check if a file or directory exists.  - os.path.isfile() to check if a file exists.  - os.path.isdir() to check if a directory exists.  - os.path.join() to join two paths.  - os.path.basename() to get the base name of a path.  - os.path.dirname() to get the directory name of a path.  - os.path.abspath() to get the absolute path of a path.  - os.path.split() to split a path into a directory and a file.  - os.path.splitext() to split a path into a base name and an extension.  - os.path.getsize() to get the size of a file.  - os.path.getmtime() to get the modification time of a file. \n\nsys.argv to get the command line arguments. \nsys.exit() to exit the program. \nsys.stdin to read from the standard input. \nsys.stdout to write to the standard output. \nsys.stderr to write to the standard error. \nargparse.ArgumentParser() to create a parser. \nadd_argument() to add an argument to the parser. \nparse_args() to parse the command line arguments. \n\nThis should remove the file “file.txt”\n\nimport os\nos.remove(\"file.txt\")\n\nFor sys you can use the sys.argv to get the command line arguments.  sys.argv is a list of the command line arguments.  sys.argv[0] is the name of the script.  sys.argv[1] is the first argument. \n\nimport sys\nfirst_argument = sys.argv[1]\n\nFor more information about the sys module you can visit the official documentation.\nFor Argparse you can use the following code: For python scripts: You can use argparse to parse command line arguments.\n\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser(description=\"This is a description.\")\nparser.add_argument(\"--arg1\", help=\"This is the first argument.\")\nparser.add_argument(\"---arg2\", help=\"This is the second argument.\")\nargs = parser.parse_args()\n\n\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n\n\n\n\nFor more information about Argparse you can visit the official documentation.\n\n\nPaths\nPay attention to the paths in your code. They are different defined in Windows and Linux. In Windows and macOS, you use backslashes / and in Linux, you use forward slashes \\.\nTo avoid this problem you can use the os or pathlib module to make your code platform independent.\n\nimport os\n\npath = os.path.join('folder1', 'folder2', 'folder3', 'data.dat')\nprint(path)\n\nfolder1/folder2/folder3/data.dat\n\n\n\nworking_dir = os.getcwd()\nprint(working_dir)\n\n/home/runner/work/PythonforChemists/PythonforChemists/course/chapters/Python\n\n\n\nfrom pathlib import Path\n\npath = Path('folder1') / 'folder2' / 'folder3' / 'data.dat'\nprint(path)\n\nfolder1/folder2/folder3/data.dat\n\n\n\nworking_dir = Path.cwd()\nprint(working_dir)\n\n/home/runner/work/PythonforChemists/PythonforChemists/course/chapters/Python",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Python Intermediate</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html",
    "href": "course/chapters/Python/BasicsModules.html",
    "title": "Basic Modules",
    "section": "",
    "text": "Numpy and Pandas",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html#numpy",
    "href": "course/chapters/Python/BasicsModules.html#numpy",
    "title": "Basic Modules",
    "section": "Numpy",
    "text": "Numpy\n\nNumpy is the most important library for Python.\nThe standard data types in Python are very slow and not very efficient for data analysis.\nNumpy is based mainly on C an C++.\nThis allows Numpy to be faster than plain Python.\nWith Numpy a new data type is introduced numpy arrays.\nNumpy arrays are multidimensional arrays that are much faster than Python lists.\nThe libary also includes many mathematical functions and methods for linear algebra.\n\nMore information can found at the Numpy website.\nLoad the required libraries\n\nimport numpy as np\n\n\nNumpy Arrays\nAn array can be described as multidimensional lists. For example a matrix is a 2D array.\n\nmat = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(mat)\n\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\nThe elements of an array can be accessed using the index of the element.\n\nprint(mat[0, 0])   # 1 element at row 0, column 0\n\n\nprint(mat[1, 2])  # 6 element at row 1, column 2\n\n\nprint(mat[:, 0])  # [1 4 7] all elements in column 0\n\n\nprint(mat[1, :])  # [4 5 6] all elements in row 1\n\n1\n6\n[1 4 7]\n[4 5 6]\n\n\n\nempty_mat = np.empty((3,3),dtype=float)\nprint(empty_mat)\n\n[[4.69010435e-310 0.00000000e+000 0.00000000e+000]\n [0.00000000e+000 0.00000000e+000 0.00000000e+000]\n [0.00000000e+000 0.00000000e+000 0.00000000e+000]]\n\n\n\nones_mat = np.ones((3, 3))\nprint(ones_mat)\n\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n\n\n\nzeros_mat = np.zeros((3, 3))\nprint(zeros_mat)\n\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\n\n\narr = np.arange(1, 10, 2) # an array from 1 to 10 with a step of 2\nprint(arr)\n\n[1 3 5 7 9]\n\n\n\narr2 = np.linspace(0,100,5) # an array from 0 to 100 with 5 elements\nprint(arr2)\n\n[  0.  25.  50.  75. 100.]\n\n\n\nrand_mat = np.random.rand(3,3) # a 3x3 matrix with random numbers between 0 and 1\nprint(rand_mat)\n\n[[0.59308691 0.89088555 0.77727523]\n [0.81518219 0.8693971  0.99299503]\n [0.9422383  0.36714851 0.3039445 ]]",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/BasicsModules.html#pandas",
    "href": "course/chapters/Python/BasicsModules.html#pandas",
    "title": "Basic Modules",
    "section": "Pandas",
    "text": "Pandas\n\nPandas is a library for data manipulation and analysis.\nIt is built on top of Numpy.\nWith Pandas you are working with dataframes and not with arrays like in Numpy.\nDataframes are two-dimensional labeled data structures with columns of potentially different types.\nIt is like a table in a database or a spreadsheet. Pandas has a lot of methods to manipulate dataframes.\nYou can select subsets of the data, filter, sort, group, merge, join, etc.\nYou can statistically analyze the data, export the data to different file formats, but also plot the data with the help of matplotlib.\n\nMore information can be found under Pandas website.\n\nimport pandas as pd\n\n\nPanda DataFrames\nPanda DataFrames are two-dimensional labeled data structures with columns of potentially different types like a table.\n\ndata = {\n    \"Name\": [\"Water\", \"Oxygen\", \"Hydrogen\", \"Carbon Dioxide\", \"Methane\", \"Ammonia\", \"Nitrogen\", \"Sulfur Dioxide\"],\n    \"Formula\": [\"H2O\", \"O2\", \"H2\", \"CO2\", \"CH4\", \"NH3\", \"N2\", \"SO2\"],\n    \"Molar Mass (g/mol)\": [18.015, 32.00, 2.016, 44.01, 16.04, 17.03, 28.013, 64.07]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n\n             Name Formula  Molar Mass (g/mol)\n0           Water     H2O              18.015\n1          Oxygen      O2              32.000\n2        Hydrogen      H2               2.016\n3  Carbon Dioxide     CO2              44.010\n4         Methane     CH4              16.040\n5         Ammonia     NH3              17.030\n6        Nitrogen      N2              28.013\n7  Sulfur Dioxide     SO2              64.070\n\n\n\n\nPanda DataSeries\nPanda DataSeries are one-dimensional labeled arrays.\n\nseries_data = pd.Series([1, 2, 3, 4, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nprint(series_data)\n\na    1\nb    2\nc    3\nd    4\ne    5\ndtype: int64",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Basic Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html",
    "href": "course/chapters/Python/AdvancedModules.html",
    "title": "Advanced Modules",
    "section": "",
    "text": "Advanced Modules",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#scipy",
    "href": "course/chapters/Python/AdvancedModules.html#scipy",
    "title": "Advanced Modules",
    "section": "Scipy",
    "text": "Scipy\nScipy is a library that builds on Numpy. It is specialized in scientific computing. It includes modules for statistical calculations , optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and more.\nMore information can be found at the Scipy website.\nLater on we will see how to use Scipy for statistical calculations and for anlaysis of spectra.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#scikit-learn",
    "href": "course/chapters/Python/AdvancedModules.html#scikit-learn",
    "title": "Advanced Modules",
    "section": "Scikit-learn",
    "text": "Scikit-learn\nScikit-learn is a library for machine learning. It includes modules for classification, regression, clustering, dimensionality reduction, model selection and preprocessing.\nMore information can be found at the Scikit-learn website.\nLater on we will see how to use Scikit-learn for classification and regression.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/Python/AdvancedModules.html#seaborn",
    "href": "course/chapters/Python/AdvancedModules.html#seaborn",
    "title": "Advanced Modules",
    "section": "Seaborn",
    "text": "Seaborn\nSeaborn is a library for data visualization. It is based on Matplotlib and provides a high-level interface for drawing attractive and informative statistical graphics.\nMore information can be found at the Seaborn website.\nLater on we will see how to use Seaborn for data visualization.",
    "crumbs": [
      "Python Crash Course",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Advanced Modules</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html",
    "href": "course/chapters/DHP/DataCollection.html",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "",
    "text": "More information",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#what-is-data",
    "href": "course/chapters/DHP/DataCollection.html#what-is-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "What is data?",
    "text": "What is data?\nData is a collection of\n\nnumbers\nwords\nmeasurements\nobservations\n\nIf you work with data, please have the FAIR principles in mind:\n\nFindable\nAccessible\nInteroperable\nReusable",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-get-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-get-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you get data?",
    "text": "How do you get data?\nAt the beginning of every data science project, you need to collect data.\nData can be collected from various sources, such as:\n\nexperimental measurements\ncalculations and simulations\nsurveys\nexisting databases\nweb scraping\nAPIs\n\netc.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-store-the-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-store-the-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you store the data?",
    "text": "How do you store the data?",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#how-do-you-access-external-data",
    "href": "course/chapters/DHP/DataCollection.html#how-do-you-access-external-data",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "How do you access external data?",
    "text": "How do you access external data?\nTo access data from exeternal sources, you can use different tools and libraries.\nFor example, if you want to access data from a chemical database e.g. \n\nMaterials Project, you can use the pymatgen library. (Requires API key, which can be obtained by registering on the website.)\nRCSB PDB Protein Data Bank, you can use the rcsb library.\nPubChem, you can use the pubchempy library.\n\n… and many more.\nIf you use APIs please read the documentation of the API to understand how to access the data. And make sure to respect the API usage policy and database terms of use.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#which-data-formats-do-you-use",
    "href": "course/chapters/DHP/DataCollection.html#which-data-formats-do-you-use",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "Which data formats do you use?",
    "text": "Which data formats do you use?\nData can be stored in various data formats, such as:\n\nPlain Text files (e.g. CSV, DAT, TXT)\nText files with structure (e.g. JSON, XML)\nSpreadsheet files\nBinary files (e.g. HDF5, Parquet, NetCDF,Feather, Pickle, npy, etc.)\nDatabases\nchemical and molecular data formats (e.g. XYZ, CIF, PDB, etc.)\netc.\n\n\nPlain Text Files\nA text file often contains a header with the names of the columns and then the data in rows. Columns can be seperated by different delimiters (spaces, ,, ;, tabs, …).  For example, a file with data from an experiment could look like this:\nTime/s  Temperature/°C\n0         20\n10        21\n...     ...\n\n\n\nJSON or XML\nJSON stands for JavaScript Object Notation. Data is structured in a key-value format, so that both humans and machines can read it easily.  For example, a JSON file with the same data as above could look like this:\n{\n  \"data\": [\n    {\"Time\": 0, \"Temperature\": 20},\n    {\"Time\": 10, \"Temperature\": 21},\n    ...\n  ]\n}\nXML stands for Extensible Markup Language. It is also designed to be both human-readable and machine-readable.  For example, an XML file with the same data as above could look like this:\n&lt;data&gt;\n  &lt;measurement&gt;\n    &lt;Time&gt;0&lt;/Time&gt;\n    &lt;Temperature&gt;20&lt;/Temperature&gt;\n  &lt;/measurement&gt;\n  &lt;measurement&gt;\n    &lt;Time&gt;10&lt;/Time&gt;\n    &lt;Temperature&gt;21&lt;/Temperature&gt;\n  &lt;/measurement&gt;\n  ...\n\n\nBinary Files\nData is stored in a binary format and can be read with specific libraries. It is often used for large datasets, as it is more efficient than plain text files. The computer is able to read and write binary files faster than text files.  Some common binary file formats are:\n\nHDF5: Hierarchical Data Format, used for large datasets\nParquet: Columnar storage format, used for big data\nNetCDF: Network Common Data Form, array-orriented, often for geoscience data\nFeather: a fast column-based serialization for data frames, initially designed for R and Python, helps to share data between languages\nPickle: Python-specific format, used for serializing Python objects\nnpy: Numpy-specific format, used for saving numpy arrays\n\nHere is a list of comparison of binary file formats: Comparison of data serialization formats\n\n\nDatabases\nDatabases are designed for big data storage. The advantage of databases is that they can be queried and updated easily. There are different types of databases, such as: - SQL databases (e.g. SQLite, MySQL, PostgreSQL) - NoSQL databases (e.g. MongoDB) - Graph databases - Time series databases\n\n\nChemical and Molecular Data Formats\nThere are specific data formats for chemical and molecular data, such as:\n\nXYZ: Cartesian coordinates of atoms\nCIF: Crystallographic Information File, used for crystallographic data\nPDB: Protein Data Bank, used for protein structures\nMOL: Molecule file format, used for chemical structures\nSDF: Structure Data File, used for chemical structures\nSMILES: Simplified Molecular Input Line Entry System, used for chemical structures\nInChI: International Chemical Identifier, used for chemical structures\nnmrML: Nuclear Magnetic Resonance Markup Language, used for NMR data\nNMReDATA: Nuclear Magnetic Resonance Electronic Data Aggregation, used for NMR data\nJCAMP-DX: Joint Committee on Atomic and Molecular Physical Data, used for spectroscopy data\nmzML: Mass Spectrometry Markup Language, used for mass spectrometry data\naniml: Analytical Information Markup Language, used for analytical data\nFASTA: used for DNA and protein sequences\n\netc.\n\n\nOther Data Formats\n\nImages: Images can be stored in different formats, such as JPEG, PNG, TIFF, BMP, GIF, etc.\nAudio: Audio files can be stored in different formats, such as MP3, WAV, FLAC, etc.\nVideo: Video files can be stored in different formats, such as MP4, AVI, MOV, etc.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/DataCollection.html#what-type-of-data-do-you-have",
    "href": "course/chapters/DHP/DataCollection.html#what-type-of-data-do-you-have",
    "title": "Introduction to Data Science in Chemistry and Materials Science",
    "section": "What type of data do you have?",
    "text": "What type of data do you have?\nNext what do you have to consider is the types of your data.\nDepending on the type of data analysis could be different.\n\nNumerical Data\nNumerical data is data that is expressed with numbers. It can be further divided into two types:\n\nDiscrete: Data that can only take certain values (e.g. integers)\nContinuous: Data that can take any value within a certain range (e.g. real numbers)\n\nFor example,\n\nmeasured temperature over time: continuous\nnumber of chemical substances, which are measured: discrete\n\n\n\nCategorical Data\nCategorial means that data is divided into categories. It can be further divided into two types:\n\nOrdinal: Data that has a specific order or ranking\nNominal: Data that has no specific order or ranking\n\nFor example,\n\nblood type: nominal {A, B, AB, O}\nchemical function group: nominal {alcohol, ketone, aldehyde, carboxylic acid, etc.}\npurity of a substance: ordinal {low, medium, high}\nhardness of a material: ordinal {soft, medium, hard}",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction to Data Science in Chemistry and Materials Science</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html",
    "href": "course/chapters/DHP/SimpleDataImport.html",
    "title": "Simple Data Import",
    "section": "",
    "text": "Data Reading and Writing",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#how-do-you-read-the-data",
    "href": "course/chapters/DHP/SimpleDataImport.html#how-do-you-read-the-data",
    "title": "Simple Data Import",
    "section": "How do you read the data?",
    "text": "How do you read the data?\nDepending on the data format, you can use different libraries to read the data.\n\nReading Plain Text Files\nYou can use the pandas or numpy library to read CSV files.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#pandas",
    "href": "course/chapters/DHP/SimpleDataImport.html#pandas",
    "title": "Simple Data Import",
    "section": "Pandas",
    "text": "Pandas\nPandas is read function is quite fast and can read large files. The advantage is that different data types can be read in the same file. The reading functions return a DataFrame object.\nPandas has different functions to read different file formats.\n\npandas.read_csv() function is can read CSV files.\npandas.read_table() function is can read general delimiter files.\npandas.read_fwf() function is can read fixed-width files.\n\nMostly used function is pandas.read_csv() because you can specify the delimiter, header, and other options.\n\nimport pandas as pd\n\ndata = pd.read_csv(temperature_data)\ndata\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[2], line 3\n      1 import pandas as pd\n----&gt; 3 data = pd.read_csv(temperature_data)\n      4 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nThe data has a different delimiter than the default comma. You can specify the delimiter using the sep parameter.\n\ndata = pd.read_csv(temperature_data, sep=';')\ndata\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 data = pd.read_csv(temperature_data, sep=';')\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nNow the data is read correctly. The header is already taken from the first row. If you want to specify the header, you can use the header parameter.\n\nimport pandas as pd\ndf = pd.read_csv('file.csv', header=None) # No header\ndf = pd.read_csv('file.csv', header=0) # Header is in the first row\ndf = pd.read_csv('file.csv', header=1) # Header is in the second row\n\n\ndata = pd.read_csv(temperature_data, sep=';', header=0)\ndata\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 data = pd.read_csv(temperature_data, sep=';', header=0)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nIf your data contains whitespace, you can use the skipinitialspace parameter to remove initial whitespaces.\n\ndata = pd.read_csv(temperature_dat, skipinitialspace=True, sep=\" \")\ndata\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 data = pd.read_csv(temperature_dat, skipinitialspace=True, sep=\" \")\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nNow the data has no header. You can specify the header using the names parameter.\n\ndata = pd.read_csv(temperature_dat, sep=' ', skipinitialspace=True,header=1,names=['t', 'T'])\n# important to set header=None, otherwise the first line is used as header\ndata\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 data = pd.read_csv(temperature_dat, sep=' ', skipinitialspace=True,header=1,names=['t', 'T'])\n      2 # important to set header=None, otherwise the first line is used as header\n      3 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nYou see that also not .csv files can be read with the read_csv() function.\nThe read_csv() function has a lot of parameters. Look in the documentation. You can see which parameters you can set https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\nFor example, - delimiter parameter can be used to specify the delimiter instead of sep. Both are the same. Default is ,.\n\nheader parameter can be used to specify the header row. Default is inferred from the file.\nskipinitialspace parameter can be used to remove initial whitespaces. Default is False.\nnames parameter can be used to specify the column names.\nskiprows parameter can be used to skip rows at the beginning of the file.\nskipfooter parameter can be used to skip rows at the end of the file.\nnrows parameter can be used to read only a specific number of rows.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\nna_values parameter can be used to specify the missing values.\nkeep_default_na parameter can be used to specify if the default missing values should be kept. Default is True.\nna_filter parameter can be used to recognize missing values without NA or NaN values. Default is True.\ntrue_values parameter can be used to specify the values that should be recognized as True.\nfalse_values parameter can be used to specify the values that should be recognized as False.\nparse_dates parameter can be used to parse dates. Default is False.\n\nSome examples are:\n\ndata = pd.read_csv(temperature_data, sep=';', header=0, names=['t', 'T'], skiprows=1)\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 data = pd.read_csv(temperature_data, sep=';', header=0, names=['t', 'T'], skiprows=1)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nNow the first row is skipped, only 44638 rows are read instead of 44639.\nMissing value examples:\n\ndata = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=False)\nprint(data.loc[20:26]) #print some rows to see the NaN values\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 data = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=False)\n      2 print(data.loc[20:26]) #print some rows to see the NaN values\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nIf the na_filteris set to False, the missing values are not recognized. But if it set on True, the missing values are recognized.\n\ndata = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=True)\nprint(data.loc[20:26]) #print some rows to see the NaN values\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 data = pd.read_csv(temperature_nan_dat, sep=' ', skipinitialspace=True,header=None,names=['t', 'T'],  keep_default_na=True,na_filter=True)\n      2 print(data.loc[20:26]) #print some rows to see the NaN values\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#numpy",
    "href": "course/chapters/DHP/SimpleDataImport.html#numpy",
    "title": "Simple Data Import",
    "section": "Numpy",
    "text": "Numpy\nNumpy has two main functions to read text files. - numpy.loadtxt() function is used to read text files. - numpy.genfromtxt() function is used to read text files with missing values.\nIn comparison to the pandas library, the numpy library is slower and can not read different data types in the same file. So you can not read a file with strings and numbers in the same file.\nIf you try to read a file with a header row, you will get an error.\n\nimport numpy as np\ndata = np.loadtxt(temperature_data, delimiter=';')\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[10], line 2\n      1 import numpy as np\n----&gt; 2 data = np.loadtxt(temperature_data, delimiter=';')\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1022, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1020     fname = os.fspath(fname)\n   1021 if isinstance(fname, str):\n-&gt; 1022     fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1023     if encoding is None:\n   1024         encoding = getattr(fh, 'encoding', 'latin1')\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures.csv not found.\n\n\n\nYou can specify the header row using the skiprows parameter. If you want to skip one row, the skiprows=1 parameter is set at 1.\n\ndata = np.loadtxt(temperature_data, delimiter=';', skiprows=1)\ndata\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 data = np.loadtxt(temperature_data, delimiter=';', skiprows=1)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1022, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1020     fname = os.fspath(fname)\n   1021 if isinstance(fname, str):\n-&gt; 1022     fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1023     if encoding is None:\n   1024         encoding = getattr(fh, 'encoding', 'latin1')\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures.csv not found.\n\n\n\nNow the data is read correctly. You can see that the data is read as a numpy array and not as a DataFrame. This can be a disadvantage if you want to use the data as a DataFrame but an advantage if you want to use numpy functions to process the data.\nIf you have data with whitespace, you do not need to specify the delimiterparamter because the default is whitespace.\n\ndata = np.loadtxt(temperature_dat)\ndata\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 data = np.loadtxt(temperature_dat)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1022, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1020     fname = os.fspath(fname)\n   1021 if isinstance(fname, str):\n-&gt; 1022     fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1023     if encoding is None:\n   1024         encoding = getattr(fh, 'encoding', 'latin1')\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures.dat not found.\n\n\n\ngenfromtxt() gives you more flexibility to read files with missing values.\nFirst using the loadtxt() function, you get an error because of the missing values.\n\ndata = np.loadtxt(temperature_nan_dat)\ndata\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 data = np.loadtxt(temperature_nan_dat)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1395, in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\n   1392 if isinstance(delimiter, bytes):\n   1393     delimiter = delimiter.decode('latin1')\n-&gt; 1395 arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n   1396             converters=converters, skiplines=skiprows, usecols=usecols,\n   1397             unpack=unpack, ndmin=ndmin, encoding=encoding,\n   1398             max_rows=max_rows, quote=quotechar)\n   1400 return arr\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1022, in _read(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\n   1020     fname = os.fspath(fname)\n   1021 if isinstance(fname, str):\n-&gt; 1022     fh = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1023     if encoding is None:\n   1024         encoding = getattr(fh, 'encoding', 'latin1')\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures_nan.dat not found.\n\n\n\nIf you try to read the file with an empty entrance at row 22, you wil get still an error with the genfromtxt() function.\n\ndata = np.genfromtxt(temperature_nan_dat)\ndata\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 data = np.genfromtxt(temperature_nan_dat)\n      2 data\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1990, in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\n   1988     fname = os.fspath(fname)\n   1989 if isinstance(fname, str):\n-&gt; 1990     fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1991     fid_ctx = contextlib.closing(fid)\n   1992 else:\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures_nan.dat not found.\n\n\n\nBut why? What do you think is the reason for the error?\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThe reason is that the genfromtxt() function expects the same number of columns in each row. The delimiter is set default to whitespace. But if you have a missing value, the function expects a value. An error is raised because at row 22 the function is detecting only one column due to the missing value.\n\n\n\nHow can you solve this problem?\n\n\n\n\n\n\nCaution\n\n\n\n\n\nYou can NOT solve this problem with the genfromtxt() function if you have missing values and delimiter is whitespace. Either you have to fill the missing value with a value or you have to use the pandas library.\n\n\n\nIf you have not whitespace as delimiter, you can use the genfromtxt() function with missing values.\n\ndata = np.genfromtxt(temperature_nan_data,delimiter=';')\ndata[20:26] # print some rows to see the NaN values\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 data = np.genfromtxt(temperature_nan_data,delimiter=';')\n      2 data[20:26] # print some rows to see the NaN values\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1990, in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\n   1988     fname = os.fspath(fname)\n   1989 if isinstance(fname, str):\n-&gt; 1990     fid = np.lib._datasource.open(fname, 'rt', encoding=encoding)\n   1991     fid_ctx = contextlib.closing(fid)\n   1992 else:\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:192, in open(path, mode, destpath, encoding, newline)\n    155 \"\"\"\n    156 Open `path` with `mode` and return the file object.\n    157 \n   (...)\n    188 \n    189 \"\"\"\n    191 ds = DataSource(destpath)\n--&gt; 192 return ds.open(path, mode, encoding=encoding, newline=newline)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/numpy/lib/_datasource.py:529, in DataSource.open(self, path, mode, encoding, newline)\n    526     return _file_openers[ext](found, mode=mode,\n    527                               encoding=encoding, newline=newline)\n    528 else:\n--&gt; 529     raise FileNotFoundError(f\"{path} not found.\")\n\nFileNotFoundError: https://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/lectures/lecture1/temperatures_nan.csv not found.\n\n\n\nThe different parameters that can be set are for loadtxt() function:\n(see documentation https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html\n\ndelimiter parameter can be used to specify the delimiter. Default is whitespace.\nskiprows parameter can be used to skip rows at the beginning of the file.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\ncomments parameter can be used to specify the comment character. Default is #.\nmax_rows parameter can be used to read only a specific number of rows after skipping rows.\nunpack parameter can be used to unpack the columns, so each column is returned as a separate array.\n\nand for genfromtxt() function\n(see documentation https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html#numpy.genfromtxt)\n\ndelimiter parameter can be used to specify the delimiter. Default is whitespace.\nskip_header parameter can be used to skip rows at the beginning of the file.\nskip_footer parameter can be used to skip rows at the end of the file.\nusecols parameter can be used to read only specific columns.\ndtype parameter can be used to specify the data type of the columns.\ncomments parameter can be used to specify the comment character. Default is #.\nmax_rows parameter can be used to read only a specific number of rows after skipping rows.\nunpack parameter can be used to unpack the columns, so each column is returned as a separate array.\nmissing_values parameter can be used to specify which values should be recognized as missing values.\nfilling_values parameter can be used to specify the filling values for the missing values.\nusemask parameter can be used to return a masked array with missing values.\nnames parameter can be used to specify the column names. If names=True, the column names are read from the first row.\nreplace_space parameter can be used to replace spaces in the column names. Default is _.\n\netc.\n\n\n\n\n\n\nImportant\n\n\n\nThe pandas library is faster and more flexible than the numpy library. Choose wisely which library you want to use. It depends on the data format, the data type and what kind of processing you want to do.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataImport.html#exercises",
    "href": "course/chapters/DHP/SimpleDataImport.html#exercises",
    "title": "Simple Data Import",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Data Import</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataInspection.html",
    "href": "course/chapters/DHP/SimpleDataInspection.html",
    "title": "Simple Data Inspection",
    "section": "",
    "text": "Inspection of Data\nAfter you load your data you have to inspect it to:",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Inspection</span>"
    ]
  },
  {
    "objectID": "course/chapters/DHP/SimpleDataInspection.html#inspection-of-data",
    "href": "course/chapters/DHP/SimpleDataInspection.html#inspection-of-data",
    "title": "Simple Data Inspection",
    "section": "",
    "text": "check if no data is consistent, no missing values\ncheck if the data is in the correct format\ncheck if the data is in the correct range\ncheck if the data is in the correct distribution\nget first insights into the data\n\n\nPandas\n\nOverview of the Data\nYou can use the head() function to get a quick overview of the first rows of the data.\n\nimport pandas as pd\n\ndata = pd.read_csv(temperature_data,sep=';')\ndata.head()\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[2], line 3\n      1 import pandas as pd\n----&gt; 3 data = pd.read_csv(temperature_data,sep=';')\n      4 data.head()\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nThe describe() function gives you a quick overview of the data distribution.\n\ndata.describe()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 data.describe()\n\nNameError: name 'data' is not defined\n\n\n\nThe describefunction shows you the count, mean, standard deviation, minimum, 25%, 50%, 75% and maximum values of the data.\nIn this case, data has 44640 data points, The mean of the temperature is 298(6) K. The minimum temperature is 279 K and the maximum temperature is 316 K. Further the 25% quantile is 294 K, the 50% quantile is 298 K and the 75% quantile is 302 K. The measurement was taken from 1 to 44640 seconds which is 12 hours and 24 minutes. We suppose that is the correct time range which was to be expected.\nThis gives you a quick overview of the data distribution.\n\n\nMissing Data and Corrupted Data\nTo check if there is missing data in the data set you can use the isna() function.\n\ndata.isna().sum()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 data.isna().sum()\n\nNameError: name 'data' is not defined\n\n\n\nNo missing data is found in this case.\nYou can check the data type using dtypes function to check if the data is in the correct format.\n\ndata.dtypes\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 data.dtypes\n\nNameError: name 'data' is not defined\n\n\n\n\ndata.dtypes\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 data.dtypes\n\nNameError: name 'data' is not defined\n\n\n\nYou see that time is an int64 and temperature is a float64. For the analysis, you might want to convert the time to a float64 as well.\n\ndata['time'] = data['time'].astype('float64')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 data['time'] = data['time'].astype('float64')\n\nNameError: name 'data' is not defined\n\n\n\n\ndata['time'] = data['time'].astype('float64')\ndata.dtypes\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 data['time'] = data['time'].astype('float64')\n      2 data.dtypes\n\nNameError: name 'data' is not defined\n\n\n\nIf we have missing data we can use the fillna() function to fill the missing data with a specific value.\n\ndata.fillna(0)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 data.fillna(0)\n\nNameError: name 'data' is not defined\n\n\n\n\ndata_missing = pd.read_csv(temperature_nan_data, header=None, skipinitialspace=True, sep=' ', names=['time', 'temperature'])\ndata_missing.head()\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 data_missing = pd.read_csv(temperature_nan_data, header=None, skipinitialspace=True, sep=' ', names=['time', 'temperature'])\n      2 data_missing.head()\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:728, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    725     codecs.lookup_error(errors)\n    727 # open URLs\n--&gt; 728 ioargs = _get_filepath_or_buffer(\n    729     path_or_buf,\n    730     encoding=encoding,\n    731     compression=compression,\n    732     mode=mode,\n    733     storage_options=storage_options,\n    734 )\n    736 handle = ioargs.filepath_or_buffer\n    737 handles: list[BaseBuffer]\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:384, in _get_filepath_or_buffer(filepath_or_buffer, encoding, compression, mode, storage_options)\n    382 # assuming storage_options is to be interpreted as headers\n    383 req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n--&gt; 384 with urlopen(req_info) as req:\n    385     content_encoding = req.headers.get(\"Content-Encoding\", None)\n    386     if content_encoding == \"gzip\":\n    387         # Override compression based on Content-Encoding header\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/site-packages/pandas/io/common.py:289, in urlopen(*args, **kwargs)\n    283 \"\"\"\n    284 Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of\n    285 the stdlib.\n    286 \"\"\"\n    287 import urllib.request\n--&gt; 289 return urllib.request.urlopen(*args, **kwargs)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:216, in urlopen(url, data, timeout, cafile, capath, cadefault, context)\n    214 else:\n    215     opener = _opener\n--&gt; 216 return opener.open(url, data, timeout)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:525, in OpenerDirector.open(self, fullurl, data, timeout)\n    523 for processor in self.process_response.get(protocol, []):\n    524     meth = getattr(processor, meth_name)\n--&gt; 525     response = meth(req, response)\n    527 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:634, in HTTPErrorProcessor.http_response(self, request, response)\n    631 # According to RFC 2616, \"2xx\" code indicates that the client's\n    632 # request was successfully received, understood, and accepted.\n    633 if not (200 &lt;= code &lt; 300):\n--&gt; 634     response = self.parent.error(\n    635         'http', request, response, code, msg, hdrs)\n    637 return response\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:563, in OpenerDirector.error(self, proto, *args)\n    561 if http_err:\n    562     args = (dict, 'default', 'http_error_default') + orig_args\n--&gt; 563     return self._call_chain(*args)\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:496, in OpenerDirector._call_chain(self, chain, kind, meth_name, *args)\n    494 for handler in handlers:\n    495     func = getattr(handler, meth_name)\n--&gt; 496     result = func(*args)\n    497     if result is not None:\n    498         return result\n\nFile /opt/hostedtoolcache/Python/3.10.16/x64/lib/python3.10/urllib/request.py:643, in HTTPDefaultErrorHandler.http_error_default(self, req, fp, code, msg, hdrs)\n    642 def http_error_default(self, req, fp, code, msg, hdrs):\n--&gt; 643     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\nOne value is missing in the temperature column. We fill it with 0.\nFirst let check where the data is missing.\n\ndata[data['temperature'].isna()]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 data[data['temperature'].isna()]\n\nNameError: name 'data' is not defined\n\n\n\ndata_missing[data_missing[‘temperature’].isna()]\nAt index 21 at time 22 s the temperature is missing.\n\n\n\n\n\n\nNote\n\n\n\nThe handling of missing data is a complex topic. First of all you have to check why the data is missing. Is it a measurement error, a data processing error etc.\nYou have to decide if you want to fill the missing data with a specific value, drop the row or column or interpolate the missing data. The decision depends on the data and the analysis you want to perform. Droping Data is always a delicate decision because you loose information. Sometimes it is not good scientific practice to drop data. For more information there a lot of research in this topic https://doi.org/10.1076/edre.7.4.353.8937\n\n\nThe time step can be estimated by the difference between the time steps of the previous and the next data point.\n\ndata['time'].diff()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 data['time'].diff()\n\nNameError: name 'data' is not defined\n\n\n\nAnd we can summarize it via:\n\ndata['time'].diff().value_counts()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 data['time'].diff().value_counts()\n\nNameError: name 'data' is not defined\n\n\n\n\ndata_missing['time'].diff().value_counts()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 data_missing['time'].diff().value_counts()\n\nNameError: name 'data_missing' is not defined\n\n\n\nget difference between temperature values\n\ndata_missing[10:30].diff()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 data_missing[10:30].diff()\n\nNameError: name 'data_missing' is not defined\n\n\n\nThe time step is constantly 1 second. The difference between the temperature of the previous and the next data point is at \\(~10^{-2}\\) order. We can assume that in this case the data is consistent enough and we can fill the missing data with the mean of the previous and the next data point.\n\ndata['temperature'].fillna((data['temperature'].shift() + data['temperature'].shift(-1))/2, inplace=True)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 data['temperature'].fillna((data['temperature'].shift() + data['temperature'].shift(-1))/2, inplace=True)\n\nNameError: name 'data' is not defined\n\n\n\n\ndata_missing['temperature'].fillna((data_missing['temperature'].shift() + data_missing['temperature'].shift(-1))/2, inplace=True)\ndata_missing[20:25]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 data_missing['temperature'].fillna((data_missing['temperature'].shift() + data_missing['temperature'].shift(-1))/2, inplace=True)\n      2 data_missing[20:25]\n\nNameError: name 'data_missing' is not defined\n\n\n\nNow can analysis or plot the data.",
    "crumbs": [
      "Data Handling and Preprocessing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Simple Data Inspection</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html",
    "href": "course/chapters/Plots/DataVisualization.html",
    "title": "Introduction into Data Visualization",
    "section": "",
    "text": "Data Visualization",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html#why-we-should-viusalize-data",
    "href": "course/chapters/Plots/DataVisualization.html#why-we-should-viusalize-data",
    "title": "Introduction into Data Visualization",
    "section": "Why we should viusalize data?",
    "text": "Why we should viusalize data?\nVisualization of data helps us to understand the data better. It helps us to understand the relationship between different features and the target variable. You can identify outliers. Further the distribution of the data can be understood and correlation can be identified.\nThere exists are lot of different visualization techniques:\n\nLine plot: Show the relationship of observable over the abscissa e.g. time vs radioactivity decay as continuous function.\nScatter plot: Show the relationship of observable over the abscissa e.g. time vs temperature as discrete function.\n[Histogram] (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html): Show the distribution of a single variable e.g. count of a mass of individuals in a population. The data is divided into bins and the number of data points in each bin is plotted.\nKernel density plot: Show the distribution of a single variable e.g. distribution of a mass of individuals in a population. The data is smoothed by a kernel density and the density of the data is plotted continuously.\nBar plot: Show the relationship of a categorical variable with a numerical variable e.g. cancer cell survival time for different treatment groups. The height of the bar is proportional to the value of the investigated variable.\nHeatmap: Show the relationship of two categorical variables with a numerical variable e.g. cancer cell survival time for different treatment groups and different cancer types. The color of the cell is proportional to the value of the investigated variable.\nNetwork plot: It is used to show the relationship between different nodes. It is used to show the relationship between different entities e.g. protein-protein interaction network PPI.\nBox plot: Show the distribution of a numerical variable for different categories. It shows the minimum, first quartile, median, third quartile and maximum of your data. Outliers can be identified. An example of this e.g. is the distribution of cancer cell survival time for different treatment groups.\nPie chart: Show the proportion of different categories in a single variable e.g. the content of different amino acids in a protein.\nViolin plot: Show the distribution of a numerical variable for different categories. It is similar to a box plot but it also shows the probability density of the data at different values. An example of this e.g. is the distribution of cancer cell survival time for different treatment groups.\nRegression plot: Show a regression model between two variables e.g. the relationship between the concentration and the time.",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html#which-technique-should-you-use",
    "href": "course/chapters/Plots/DataVisualization.html#which-technique-should-you-use",
    "title": "Introduction into Data Visualization",
    "section": "Which technique should you use?",
    "text": "Which technique should you use?\n\nHow many variables do you have and how many do you want to compare at once?\nWhat type of data do you have?\nWhat type of relationship are you trying to represent?\nDo you want to analyze the distribution of the data?\nDo you want to analyze the correlation between the variables?\nDo you want to show ranking or ordering of the data?\nDo you want to a trend over time?\nDo you want to show the composition of the data?\nDo you want to predict the future values of the data?\nDo you want to show the relationship between the variables?\n\n\nAdditional resources\nA good handbook for data visualization in Python &lt;br is the book “Python Data Science Handbook” by Jake VanderPlas:  https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html\nHow you should visualize the data depends on the data and the question you want to answer. For example descriptive statistics can be visualized with a boxplot, a violinplot, a histogram or a density plot. Relationships between two variables can be visualized with a scatterplot, a lineplot, a regplot or a jointplot.",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/DataVisualization.html#how-you-should-visualize-data",
    "href": "course/chapters/Plots/DataVisualization.html#how-you-should-visualize-data",
    "title": "Introduction into Data Visualization",
    "section": "How you should visualize data?",
    "text": "How you should visualize data?\n\nA graphic should have a suitable font size.\nChoose colors that are easy to distinguish also for colorblind people.\nThe axis ticks and limits should be chosen in a way that the data is easy to read.\nAxis should be labeled and physical units should be not forgotten.\nThe graphic should be not overloaded with information.",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction into Data Visualization</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlots.html",
    "href": "course/chapters/Plots/SimplePlots.html",
    "title": "Basic Data Visualization Techniques",
    "section": "",
    "text": "Basic Data Visualization Technique\nThe most popular data visualization libraries in Python is Matplotlib. Let`s start with the basic data visualization techniques using Matplotlib.",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Data Visualization Techniques</span>"
    ]
  },
  {
    "objectID": "course/chapters/Plots/SimplePlots.html#basic-data-visualization-technique",
    "href": "course/chapters/Plots/SimplePlots.html#basic-data-visualization-technique",
    "title": "Basic Data Visualization Techniques",
    "section": "",
    "text": "1. Generate some x-y data points.\n\n\n2. Plot the data points.\n\n\n\n\n\n\n\n\n\nTo add more graphs to the same figure, use plt.plot() multiple times before plt.show(). If you want to create a new figure, use plt.figure() before plt.plot().\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Adjust the plot.\nThe plot() function takes the following arguments:\n\nx-axis data points\ny-axis data points\ncolor: hex, or color name (e.g., ‘red’, ‘blue’,‘black’), abbreviated (e.g., ‘r’, ‘b’,‘k’)\nlinestyle: ‘-’, ‘–’, ‘-.’, ‘:’ or “solid”, “dashed”, “dashdot”, “dotted”\nmarker: ‘o’, ‘x’, ‘+’, ’*‘, ’s’, ‘d’, ‘^’, ‘v’, ‘&gt;’, ‘&lt;’, ‘p’, ‘h’\nlinewidth - width of the line\nalpha - transparency of the line\nmarkerfacecolor - color of the marker face\nmarkersize - size of the marker\nlabel - label for the data points\n\nYou have to call plt.legend() to show the labels.\n\n\n\n\n\n\n\n\n\n\n\n4. Adust the figure\nThis plot figure can be adjusted by changing the figure size, title, labels, and so on.\n\nplt.xlabel(): Set the x-axis label of the current axis.\nplt.ylabel(): Set the y-axis label of the current axis.\nplt.title(): Set a title for the axes.\nplt.legend(): Place a legend on the axes.\nplt.grid(): Configure the grid lines.\nplt.xlim(): Get or set the x-limits of the current axes.\nplt.ylim(): Get or set the y-limits of the current axes.\nplt.xticks(): Get or set the current tick locations and labels of the x-axis.\nplt.yticks(): Get or set the current tick locations and labels of the y-axis.\nplt.figure(): Create a new figure.\nplt.show(): Display a figure.\n\n\n\n\n\n\n\n\n\n\n\n\nCreating multiple plots\nYou can create multiple plots in the same figure by using the subplot() function.\n\n\n\n\n\n\n\n\n\n\n\nText(0.5, 0.98, 'Right subfigure')\n\n\n\n\n\n\n\n\n\nLet`s try to create 4x3 subplots with specific adjustments.\n\nplt.subplots(4, 3): create 4x3 subplots\nfigsize=(3, 4): set the figure size to 3x4 inches\nsharex=True: all subplots share the x-axis\nsharey=True: all subplots share the y-axis\nconstraint_layout=True: automatically adjust the subplot parameters to give the specified padding around the subplots\ngridspec_kw={'hspace': 0.5, 'wspace': 0.5}: set the horizontal and vertical space between the subplots to 0.5 inches\n\nPlay with the code below to understand the different parameters:\n\nviewof figx = Inputs.range(\n  [1, 10], \n  {value: 5, step: 0.5, label: \"Figure x-size:\"}\n)\nviewof figy = Inputs.range(\n  [1, 10], \n  {value: 5, step: 0.5, label: \"Figure y-size:\"}\n)\nviewof constrained = Inputs.select([\"True\", \"False\"], {label: \"Select an option:\"})\nviewof dpi = Inputs.range(\n  [70, 100], \n  {value:75, step:5 , label: \"Dpi:\"}\n)\nviewof space_h = Inputs.range(\n  [0.1, 1], \n  {value: 0.5, step: 0.05, label: \"horizonatal space:\"}\n)\nviewof space_w = Inputs.range(\n  [0.1, 1], \n  {value: 0.5, step: 0.05, label: \"width space:\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#| edit: False\n#| runbutton: True\n#| input:\n#|   - figx\n#|   - figy\n#|   - constrained\n#|   - dpi\n#|   - space_w\n#|   - space_h\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nfig, ax = plt.subplots(3,2, figsize=(figx,figy), dpi=dpi, sharex=True, sharey=True, constrained_layout=constrained,gridspec_kw={'hspace': space_h, 'wspace': space_w})\n\nfor i in range(3):\n    for j in range(2):\n        ax[i,j].plot(x, y*(i+1)*(j+1))\n\nplt.show()",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic Data Visualization Techniques</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exercise1.html",
    "href": "course/exercises/exercises/exercise1.html",
    "title": "Exercise 1",
    "section": "",
    "text": "Exercise:",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise 1</span>"
    ]
  },
  {
    "objectID": "course/exercises/exercises/exercise1.html#temperature-in-synthesis-reactor-part-1",
    "href": "course/exercises/exercises/exercise1.html#temperature-in-synthesis-reactor-part-1",
    "title": "Exercise 1",
    "section": "Temperature in Synthesis Reactor Part 1",
    "text": "Temperature in Synthesis Reactor Part 1\nIn this exercise, we will repeat the first part of the course. We will:\n\nWe will learn how to read data from a file.\nWe will learn how to inspect the data.\nWe will learn how to make quickly some plots.\n\n\nData\nThe experiment:\n\nWe have multiple synthesis reactors in which we are running a series of experiments.\nEach reactor has a temperature sensor which monitors the temperature inside of it every minute for one month.\nOur task is to check if the temperatures in the reactors are stable.\nThe data for reactor 1 is stored in Temperatures_1.csv.\nThe data for reactor 2 is stored in Temperatures_2.csv.\nThe data for reactor 3 is stored in Temperatures_3.csv.\n\n\n\nData Path:\nhttps://raw.githubusercontent.com/stkroe/PythonForChemists/main/course/data/exercises/exercises1/\n\n\nTask\n\nLook at the plain data files. In which format are the data stored?\nRead the data from the files.\nInspect the data. Are there any missing values? Are the data in the correct format? Is the data measured really over one month?\nTransform the data into days.\nLook at the distribution of the data. Is there some striking pattern?\nMake a plot of the temperatures trend over time for reactor 1, 2 and 3.\n\n\n\nQuestions\nWhat do you think? Are the temperatures stable in the reactors? Which reactor has the most stable temperature?",
    "crumbs": [
      "Simple Plotting",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Exercise 1</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataManipulation.html",
    "href": "course/chapters/SEDA/SimpleDataManipulation.html",
    "title": "Simple Data Manipulation and Analysis",
    "section": "",
    "text": "Data Manipulation",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Data Manipulation and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataManipulation.html#numpy",
    "href": "course/chapters/SEDA/SimpleDataManipulation.html#numpy",
    "title": "Simple Data Manipulation and Analysis",
    "section": "Numpy",
    "text": "Numpy\n\nimport numpy as np\n\n\nArray manipulation\nOften data has to be manipulated before it can be analyzed.  Numpy has many methods for array manipulation. \nFor example,\n\nthe shape of an array can be changed,\nmultiple arrays can be concatenated,\nthe elements of an array can be sorted\nnon valid values NaN can be removed,\nlinear algebra operations can be performed,\n\netc.\n\nSort arrays:\n\nunsorted_arr = np.array([[3, 1, 5, 2, 4], [5, 2, 0, 8, 1], [3, 2, 9, 4 , 5]])\nprint(\"unsortd_arr: \\n\", unsorted_arr)\nsorted_arr = np.sort(unsorted_arr,axis = 0) # sorted along the column\nprint(\"sorted_arr along columns: \\n\" , sorted_arr)\nsorted_arr = np.sort(unsorted_arr,axis = 1) # sorted along the row\nprint(\"sorted_arr along rows: \\n\" , sorted_arr)\n\nunsortd_arr: \n [[3 1 5 2 4]\n [5 2 0 8 1]\n [3 2 9 4 5]]\nsorted_arr along columns: \n [[3 1 0 2 1]\n [3 2 5 4 4]\n [5 2 9 8 5]]\nsorted_arr along rows: \n [[1 2 3 4 5]\n [0 1 2 5 8]\n [2 3 4 5 9]]\n\n\n\n\nConcatenate arrays:\n\na = np.array([1, 2, 3]) \nb = np.array([4, 5, 6])\nc = np.concatenate((a, b))\nprint(c)\n\n[1 2 3 4 5 6]\n\n\n\n\nReshape arrays:\n\na = np.array([[1, 2],[3, 4],[5, 6]])\nb = np.array([[7, 8],[9, 10],[11, 12]])\nc = np.vstack((a, b)) # vertical stack\nd = np.hstack((a, b)) # horizontal stack\nprint(\"vertical stack: \\n\", c)\nprint(\"horizontal stack: \\n\", d)\nprint(\"flatten array: \\n\", c.flatten())\n\nvertical stack: \n [[ 1  2]\n [ 3  4]\n [ 5  6]\n [ 7  8]\n [ 9 10]\n [11 12]]\nhorizontal stack: \n [[ 1  2  7  8]\n [ 3  4  9 10]\n [ 5  6 11 12]]\nflatten array: \n [ 1  2  3  4  5  6  7  8  9 10 11 12]\n\n\n\n\nLinear algebra operations e.g.:\n\nnp.sum(a) returns element-wise sum of two arrays\nnp.add(a) returns sum of all elements an arrays\nnp.subtract(a,b) returns the difference of two arrays\nnp.multiply(a,b) returns the product of two arrays\nnp.divide(a,b) returns the division of two arrays\nnp.dot(a,b) returns the dot product of two arrays\nnp.cross(a,b) returns the cross product of two arrays\nnp.linalg.inv(a) returns the inverse of a matrix\nnp.linalg.det(a) returns the determinant of a matrix\nnp.linalg.eig(a) returns the eigenvalues and eigenvectors of a matrix\nnp.linalg.solve(a,b) returns the solution of a linear system of equations\nnp.mean(a) returns the mean of the elements of an array\nnp.average(a) returns the weighted average of the elements of an array\nnp.max(a) returns the maximum of the elements of an array\nnp.min(a) returns the minimum of the elements of an array\nnp.std(a) returns the standard deviation of the elements of an array\nnp.var(a) returns the variance of the elements of an array\nnp.covar(a) returns the covariance of the elements of an array\nnp.median(a) returns the median of the elements of an array\n`np.percentile(a,p) `` returns the p-th percentile of the elements of an array\nnp.histogram(a, [,bins,range,density,weights]) returns the histogram of the elements of an array\n\n\na = np.array([[1, 2],[3, 4],[5, 6]])\nb = np.array([[7, 8],[9, 10],[11, 12]])\nc = np.sum(a) # sum of all elements\nprint(\"Summ of all elements: \", c)\nc = np.add(a,b) # element wise addition\nprint(\"Element wise addition: \", c)\n\nSumm of all elements:  21\nElement wise addition:  [[ 8 10]\n [12 14]\n [16 18]]\n\n\n\nnp.argmin(a) returns the index of the minimum element of an array\nnp.argmax(a) returns the index of the maximum element of an array\nnp.where(a) returns the indices of the elements of an array that are non-zero\nnp.argwhere(a) returns the indices of the elements of an array that are non-zero\nnp.nonzero(a) returns the indices of the elements of an array that are non-zero\nnp.searchsorted(a,v) returns the index of the first element of an array  that is greater than or equal to a value\nnp.extract(condition,a) returns the elements of an array that satisfy a condition\n\n\narr = np.array([1, 2, 3, 4, 5, 4, 7, 8, 9])\nprint(arr)\nprint(\"Index of the maximum element: \", np.argmax(arr))\nprint(\"Index of the minimum element: \", np.argmin(arr))\nprint(\"Maximum element: \", np.max(arr))\nprint(\"Minimum element: \", np.min(arr))\nprint(\"Find the index of element \\\"4\\\": \", np.where(arr == 4))\nprint(\"Find the index of element \\\"&gt;4\\\": \", np.argwhere(arr &gt; 4),\n       \" and the elements are: \", arr[np.where(arr &gt; 4)])\n\n[1 2 3 4 5 4 7 8 9]\nIndex of the maximum element:  8\nIndex of the minimum element:  0\nMaximum element:  9\nMinimum element:  1\nFind the index of element \"4\":  (array([3, 5]),)\nFind the index of element \"&gt;4\":  [[4]\n [6]\n [7]\n [8]]  and the elements are:  [5 7 8 9]",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Data Manipulation and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataManipulation.html#pandas",
    "href": "course/chapters/SEDA/SimpleDataManipulation.html#pandas",
    "title": "Simple Data Manipulation and Analysis",
    "section": "Pandas",
    "text": "Pandas\n\nGet information about the data\n\nimport pandas as pd\ndata = pd.DataFrame({\n    \"Name\": [\"Water\", \"Sulfuric Acid\", \"Ethanol\", \"Acetone\", \"Ammonia\", \"Benzene\", \"Methanol\", \"Glycerol\"],\n    \"Formula\": [\"H2O\", \"H2SO4\", \"C2H5OH\", \"C3H6O\", \"NH3\", \"C6H6\", \"CH3OH\", \"C3H8O3\"],\n    \"Molecular Weight (g/mol)\": [18.015, 98.079, 46.07, 58.08, 17.03, 78.11, 32.04, 92.09],\n    \"Viscosity (mPa·s)\": [1.002, 24.0, 1.2, 0.32, 0.26, 0.65, 0.544, 1412],\n    \"pH (Acidity)\": [7, 1, 7.33, 7, 11.6, 7, 7.4, 5.5],\n    \"Chemical Type\": [\"Inorganic\", \"Acid\", \"Alcohol\", \"Ketone\", \"Base\", \"Aromatic Hydrocarbon\", \"Alcohol\", \"Polyol\"],\n    \"Concentration (M)\": [55.5, 18.0, 1.0, 0.8, 0.5, 0.1, 1.5, 1.2]})\n\n\nprint(\"head of data: \\n\",data.head()) # print the first 5 rows\nprint(\"\\n\")\nprint(\"Number of data set: \\n\", data.shape[0]) # number of data set\nprint(\"\\n\")\nprint(\"Column Name: \\n\", data[\"Name\"]) # print the column \"Name\"\nprint(\"\\n\")\nprint(\"2. Row: \\n\", data.iloc[1]) # print the second row\n\nhead of data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0          7.00     Inorganic               55.5  \n1          1.00          Acid               18.0  \n2          7.33       Alcohol                1.0  \n3          7.00        Ketone                0.8  \n4         11.60          Base                0.5  \n\n\nNumber of data set: \n 8\n\n\nColumn Name: \n 0            Water\n1    Sulfuric Acid\n2          Ethanol\n3          Acetone\n4          Ammonia\n5          Benzene\n6         Methanol\n7         Glycerol\nName: Name, dtype: object\n\n\n2. Row: \n Name                        Sulfuric Acid\nFormula                             H2SO4\nMolecular Weight (g/mol)           98.079\nViscosity (mPa·s)                    24.0\npH (Acidity)                          1.0\nChemical Type                        Acid\nConcentration (M)                    18.0\nName: 1, dtype: object\n\n\n\nSelect subsets of the data:\nOften you need only a specific part of your data with pandasit is quite easy to filter the data after specific properties.\n\n# filter the rows where concentration is greater than 5\nprint(\"Filter the rows where concentration is greater than 5: \\n\", data[data[\"Concentration (M)\"] &gt; 5]) \nprint(\"\\n\")\n# get the chemical type with a concentration higher than 5\nprint(\"Get the chemical type with a concentration higher than 5: \\n\", \n      data.loc[data[\"Concentration (M)\"]&gt;5,\"Chemical Type\"])\nprint(\"\\n\")\n\nFilter the rows where concentration is greater than 5: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n\n   pH (Acidity) Chemical Type  Concentration (M)  \n0           7.0     Inorganic               55.5  \n1           1.0          Acid               18.0  \n\n\nGet the chemical type with a concentration higher than 5: \n 0    Inorganic\n1         Acid\nName: Chemical Type, dtype: object\n\n\n\n\n\n\n\nManipulate data\nYou can\n\ncopy data,\ndrop columns,\ndrop rows,\nfill not valid values,\nreplace values,\nmerge data,\njoin data,\n\netc.\n\ndata2 = data.copy()\ndata2[\"Concentration 2 (M)\"] = data2[\"Concentration (M)\"] + 10 # increase the concentration by 10 M\nprint(\"data2: \\n\" , data2)\n\n\ndata3 = pd.concat([data, data2]) # concatenate the dataframes\nprint(\"concated data: \\n\", data3)\n\ndata2: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \n0          7.00             Inorganic               55.5                 65.5  \n1          1.00                  Acid               18.0                 28.0  \n2          7.33               Alcohol                1.0                 11.0  \n3          7.00                Ketone                0.8                 10.8  \n4         11.60                  Base                0.5                 10.5  \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1  \n6          7.40               Alcohol                1.5                 11.5  \n7          5.50                Polyol                1.2                 11.2  \nconcated data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \n0          7.00             Inorganic               55.5                  NaN  \n1          1.00                  Acid               18.0                  NaN  \n2          7.33               Alcohol                1.0                  NaN  \n3          7.00                Ketone                0.8                  NaN  \n4         11.60                  Base                0.5                  NaN  \n5          7.00  Aromatic Hydrocarbon                0.1                  NaN  \n6          7.40               Alcohol                1.5                  NaN  \n7          5.50                Polyol                1.2                  NaN  \n0          7.00             Inorganic               55.5                 65.5  \n1          1.00                  Acid               18.0                 28.0  \n2          7.33               Alcohol                1.0                 11.0  \n3          7.00                Ketone                0.8                 10.8  \n4         11.60                  Base                0.5                 10.5  \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1  \n6          7.40               Alcohol                1.5                 11.5  \n7          5.50                Polyol                1.2                 11.2  \n\n\n\n # create a Panda Series\nacidity = pd.Series([\"basic\", \"acid\", \"basic\", \"neutral\",\n                    \"acid\", \"acid\", \"basic\", \"basic\"], name=\"Aciditiy Type\")\nprint(\"acidity: \\n\",acidity)\n# concatenate the dataframes\ndata4 = pd.concat([data3, acidity], axis=1) \nprint(\"concated data: \\n\", data4)\n\nacidity: \n 0      basic\n1       acid\n2      basic\n3    neutral\n4       acid\n5       acid\n6      basic\n7      basic\nName: Aciditiy Type, dtype: object\nconcated data: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \\\n0          7.00             Inorganic               55.5                  NaN   \n1          1.00                  Acid               18.0                  NaN   \n2          7.33               Alcohol                1.0                  NaN   \n3          7.00                Ketone                0.8                  NaN   \n4         11.60                  Base                0.5                  NaN   \n5          7.00  Aromatic Hydrocarbon                0.1                  NaN   \n6          7.40               Alcohol                1.5                  NaN   \n7          5.50                Polyol                1.2                  NaN   \n0          7.00             Inorganic               55.5                 65.5   \n1          1.00                  Acid               18.0                 28.0   \n2          7.33               Alcohol                1.0                 11.0   \n3          7.00                Ketone                0.8                 10.8   \n4         11.60                  Base                0.5                 10.5   \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1   \n6          7.40               Alcohol                1.5                 11.5   \n7          5.50                Polyol                1.2                 11.2   \n\n  Aciditiy Type  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n\n\n\ndata5 = data4.drop_duplicates() # drop the duplicates\nprint(\"data5: \\n\", data5)\ndata6 = data5.dropna() # drop the NaNs\nprint(\"data6: \\n\", data6)\n\ndata5: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \\\n0          7.00             Inorganic               55.5                  NaN   \n1          1.00                  Acid               18.0                  NaN   \n2          7.33               Alcohol                1.0                  NaN   \n3          7.00                Ketone                0.8                  NaN   \n4         11.60                  Base                0.5                  NaN   \n5          7.00  Aromatic Hydrocarbon                0.1                  NaN   \n6          7.40               Alcohol                1.5                  NaN   \n7          5.50                Polyol                1.2                  NaN   \n0          7.00             Inorganic               55.5                 65.5   \n1          1.00                  Acid               18.0                 28.0   \n2          7.33               Alcohol                1.0                 11.0   \n3          7.00                Ketone                0.8                 10.8   \n4         11.60                  Base                0.5                 10.5   \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1   \n6          7.40               Alcohol                1.5                 11.5   \n7          5.50                Polyol                1.2                 11.2   \n\n  Aciditiy Type  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \ndata6: \n             Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n0          Water     H2O                    18.015              1.002   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n2        Ethanol  C2H5OH                    46.070              1.200   \n3        Acetone   C3H6O                    58.080              0.320   \n4        Ammonia     NH3                    17.030              0.260   \n5        Benzene    C6H6                    78.110              0.650   \n6       Methanol   CH3OH                    32.040              0.544   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  Concentration 2 (M)  \\\n0          7.00             Inorganic               55.5                 65.5   \n1          1.00                  Acid               18.0                 28.0   \n2          7.33               Alcohol                1.0                 11.0   \n3          7.00                Ketone                0.8                 10.8   \n4         11.60                  Base                0.5                 10.5   \n5          7.00  Aromatic Hydrocarbon                0.1                 10.1   \n6          7.40               Alcohol                1.5                 11.5   \n7          5.50                Polyol                1.2                 11.2   \n\n  Aciditiy Type  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n\n\n\n\nTransform Pandas DataFrame to Numpy Array and viceversa\n\ndf = data6\nprint(len(df))\n# convert the dataframe to a numpy array\narr = df.to_numpy() \nprint(\"arr: \\n\", arr)\n# convert the numpy array to a dataframe\ndf2 = pd.DataFrame(arr, columns=[\"Name\", \"Formula\", \"Molecular Weight (g/mol)\",\"Viscosity (mPa·s)\",\"pH (Acidity)\",\"Chemical Type\",\"Concentration (M)\",\"Concentration 2 (M)\", \"Aciditiy Type\"])\nprint(\"df2: \\n\", df2)\n\n8\narr: \n [['Water' 'H2O' 18.015 1.002 7.0 'Inorganic' 55.5 65.5 'basic']\n ['Sulfuric Acid' 'H2SO4' 98.079 24.0 1.0 'Acid' 18.0 28.0 'acid']\n ['Ethanol' 'C2H5OH' 46.07 1.2 7.33 'Alcohol' 1.0 11.0 'basic']\n ['Acetone' 'C3H6O' 58.08 0.32 7.0 'Ketone' 0.8 10.8 'neutral']\n ['Ammonia' 'NH3' 17.03 0.26 11.6 'Base' 0.5 10.5 'acid']\n ['Benzene' 'C6H6' 78.11 0.65 7.0 'Aromatic Hydrocarbon' 0.1 10.1 'acid']\n ['Methanol' 'CH3OH' 32.04 0.544 7.4 'Alcohol' 1.5 11.5 'basic']\n ['Glycerol' 'C3H8O3' 92.09 1412.0 5.5 'Polyol' 1.2 11.2 'basic']]\ndf2: \n             Name Formula Molecular Weight (g/mol) Viscosity (mPa·s)  \\\n0          Water     H2O                   18.015             1.002   \n1  Sulfuric Acid   H2SO4                   98.079              24.0   \n2        Ethanol  C2H5OH                    46.07               1.2   \n3        Acetone   C3H6O                    58.08              0.32   \n4        Ammonia     NH3                    17.03              0.26   \n5        Benzene    C6H6                    78.11              0.65   \n6       Methanol   CH3OH                    32.04             0.544   \n7       Glycerol  C3H8O3                    92.09            1412.0   \n\n  pH (Acidity)         Chemical Type Concentration (M) Concentration 2 (M)  \\\n0          7.0             Inorganic              55.5                65.5   \n1          1.0                  Acid              18.0                28.0   \n2         7.33               Alcohol               1.0                11.0   \n3          7.0                Ketone               0.8                10.8   \n4         11.6                  Base               0.5                10.5   \n5          7.0  Aromatic Hydrocarbon               0.1                10.1   \n6          7.4               Alcohol               1.5                11.5   \n7          5.5                Polyol               1.2                11.2   \n\n  Aciditiy Type  \n0         basic  \n1          acid  \n2         basic  \n3       neutral  \n4          acid  \n5          acid  \n6         basic  \n7         basic  \n\n\n\n\nSort data\n\nprint(\"sorted after concentration: \",data.sort_values(by=\"Concentration (M)\", ascending=True))\n\nsorted after concentration:              Name Formula  Molecular Weight (g/mol)  Viscosity (mPa·s)  \\\n5        Benzene    C6H6                    78.110              0.650   \n4        Ammonia     NH3                    17.030              0.260   \n3        Acetone   C3H6O                    58.080              0.320   \n2        Ethanol  C2H5OH                    46.070              1.200   \n7       Glycerol  C3H8O3                    92.090           1412.000   \n6       Methanol   CH3OH                    32.040              0.544   \n1  Sulfuric Acid   H2SO4                    98.079             24.000   \n0          Water     H2O                    18.015              1.002   \n\n   pH (Acidity)         Chemical Type  Concentration (M)  \n5          7.00  Aromatic Hydrocarbon                0.1  \n4         11.60                  Base                0.5  \n3          7.00                Ketone                0.8  \n2          7.33               Alcohol                1.0  \n7          5.50                Polyol                1.2  \n6          7.40               Alcohol                1.5  \n1          1.00                  Acid               18.0  \n0          7.00             Inorganic               55.5  \n\n\n\n\nStatistial Analysis\n\nprint(\"mean of concentration: \\n\", data[\"Concentration (M)\"].mean()) # mean of concentration\nprint(\"std of concentration: \\n\", data[\"Concentration (M)\"].std()) # standard deviation of concentration\nprint(\"median of concentration: \\n\", data[\"Concentration (M)\"].median()) # median of concentration\nprint(\"max of concentration: \\n\", data[\"Concentration (M)\"].max()) # max of concentration\nprint(\"min of concentration: \\n\", data[\"Concentration (M)\"].min()) # min of concentration\nprint(\"sum of concentration: \\n\", data[\"Concentration (M)\"].sum()) # sum of concentration\nprint(\"Statisical summary: \\n\", data.describe()) # statistical summary\n# mean of concentration group by type\nprint(\"Mean of concentration group by type: \\n\", data.groupby(\"Chemical Type\")[\"Concentration (M)\"].mean()) \n# count the chemicals with different concentration\nprint(\"Count the chemicals with the different concentrations \\n\", data[\"Concentration (M)\"].value_counts())\n\nmean of concentration: \n 9.825\nstd of concentration: \n 19.41131849499888\nmedian of concentration: \n 1.1\nmax of concentration: \n 55.5\nmin of concentration: \n 0.1\nsum of concentration: \n 78.6\nStatisical summary: \n        Molecular Weight (g/mol)  Viscosity (mPa·s)  pH (Acidity)  \\\ncount                  8.000000           8.000000      8.000000   \nmean                  54.939250         179.997000      6.728750   \nstd                   32.052446         497.871465      2.905421   \nmin                   17.030000           0.260000      1.000000   \n25%                   28.533750           0.488000      6.625000   \n50%                   52.075000           0.826000      7.000000   \n75%                   81.605000           6.900000      7.347500   \nmax                   98.079000        1412.000000     11.600000   \n\n       Concentration (M)  \ncount           8.000000  \nmean            9.825000  \nstd            19.411318  \nmin             0.100000  \n25%             0.725000  \n50%             1.100000  \n75%             5.625000  \nmax            55.500000  \nMean of concentration group by type: \n Chemical Type\nAcid                    18.00\nAlcohol                  1.25\nAromatic Hydrocarbon     0.10\nBase                     0.50\nInorganic               55.50\nKetone                   0.80\nPolyol                   1.20\nName: Concentration (M), dtype: float64\nCount the chemicals with the different concentrations \n Concentration (M)\n55.5    1\n18.0    1\n1.0     1\n0.8     1\n0.5     1\n0.1     1\n1.5     1\n1.2     1\nName: count, dtype: int64\n\n\n\n\nPivot Table\nPivot tables are used to summarize and aggregate data inside a dataframe.\n\ndata.pivot_table(index=\"Chemical Type\", columns=\"Concentration (M)\", values=\"Molecular Weight (g/mol)\", aggfunc=\"mean\") \n\n\n\n\n\n\n\nConcentration (M)\n0.1\n0.5\n0.8\n1.0\n1.2\n1.5\n18.0\n55.5\n\n\nChemical Type\n\n\n\n\n\n\n\n\n\n\n\n\nAcid\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n98.079\nNaN\n\n\nAlcohol\nNaN\nNaN\nNaN\n46.07\nNaN\n32.04\nNaN\nNaN\n\n\nAromatic Hydrocarbon\n78.11\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nBase\nNaN\n17.03\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nInorganic\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n18.015\n\n\nKetone\nNaN\nNaN\n58.08\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nPolyol\nNaN\nNaN\nNaN\nNaN\n92.09\nNaN\nNaN\nNaN",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Simple Data Manipulation and Analysis</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html",
    "href": "course/chapters/SEDA/SimpleDataModels.html",
    "title": "Simple Data Models",
    "section": "",
    "text": "Linear Regression\nOften we want to find a model which can explain the data. It is important to understand the data and the model to be able to interpret the results and make predictions.\nThe simplest model is the linear regression model. It assumes that the data has a linear relationship with the target variable. The model is defined as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon\n\\]\nwhere \\(y\\) is the target variable, \\(x_1, x_2, \\ldots, x_n\\) are the features, \\(\\beta_0, \\beta_1, \\ldots, \\beta_n\\) are the coefficients, and \\(\\epsilon\\) is the error term.\nIn python there exists serveral libraries which can be used to fit a linear regression model.",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#using-scipy",
    "href": "course/chapters/SEDA/SimpleDataModels.html#using-scipy",
    "title": "Simple Data Models",
    "section": "Using scipy",
    "text": "Using scipy\nA easy way is to use scipy library. The scipy library has a function called linregress which can be used to fit a linear regression model. The function returns the slope, intercept, r-value, p-value, and the standard error of the estimate. And with scipy version 1.15.2 also the intercept error.\n\nfrom scipy.stats import linregress\nimport numpy as np\n\nx = [1, 2, 3, 4, 5]\ny = x*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)\n\nresults = linregress(x, y)\n\nslope = results.slope\nintercept = results.intercept\nr_value = results.rvalue\np_value = results.pvalue\nstd_err = results.stderr\nintercept_err = results.intercept_stderr\nprint(\"slope: %f    intercept: %f\" % (slope, intercept))\nprint(\"R-squared: %f\" % r_value**2)\nprint(\"p-value: %f\" % p_value)\nprint(\"standard error: %f\" % std_err)\nprint(\"Intercept error: %f\" %intercept_err)\n# Two-sided inverse Students t-distribution\n# p - probability, df - degrees of freedom\nfrom scipy.stats import t\ntinv = lambda p, df: abs(t.ppf(p/2, df))\nprint(\"95% confidence interval: \" + str(intercept - tinv(0.05, len(x)-2)*intercept_err) + \" to \" + str(intercept + tinv(0.05, len(x)-2)*intercept_err))\n\nslope: 2.037344    intercept: -6.029941\nR-squared: 0.353465\np-value: 0.290343\nstandard error: 1.590838\nIntercept error: 5.276214\n95% confidence interval: -22.82120858295395 to 10.761327278553265\n\n\n\n\n\n\n\n\nImportant\n\n\n\nFor older version scipyonly returned 5 values with fields slope, intercept, rvalue, pvalue and stderr. For compatiblity reasons the return values are 5 elements tuple.\nslope, intercept, r, p, se = linregress(x, y)\nAnd if you want to get the intercept error you can use the following return value as a object:\nresults = linregress(x, y)\nprint(results.intercept_stderr)",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#using-scikit-learn",
    "href": "course/chapters/SEDA/SimpleDataModels.html#using-scikit-learn",
    "title": "Simple Data Models",
    "section": "Using scikit-learn",
    "text": "Using scikit-learn\nOne of the most popular libraries is scikit-learn. The following code shows how to fit a linear regression model using scikit-learn:",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#using-scikit-learn-1",
    "href": "course/chapters/SEDA/SimpleDataModels.html#using-scikit-learn-1",
    "title": "Simple Data Models",
    "section": "Using scikit-learn",
    "text": "Using scikit-learn\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data\nX = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\ny = X*np.random.normal(0, 1, 5)+np.random.normal(0, 1, 5)\n\n# Model fitting\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predictions\ny_pred = model.predict(X)\n\n# Plot\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, y_pred, color='red', label='Linear Fit')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#using-statsmodels",
    "href": "course/chapters/SEDA/SimpleDataModels.html#using-statsmodels",
    "title": "Simple Data Models",
    "section": "Using statsmodels",
    "text": "Using statsmodels\nimport statsmodels.api as sm\n\nX_with_const = sm.add_constant(X)  # Add intercept term\nmodel = sm.OLS(y, X_with_const).fit()\nprint(model.summary())",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#polynomial-regression",
    "href": "course/chapters/SEDA/SimpleDataModels.html#polynomial-regression",
    "title": "Simple Data Models",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\npoly_model = make_pipeline(PolynomialFeatures(3), LinearRegression())\npoly_model.fit(X, y)\ny_poly_pred = poly_model.predict(X)\n\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X, y_poly_pred, color='green', label='Polynomial Fit')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/SimpleDataModels.html#curve-fitting-with-scipy",
    "href": "course/chapters/SEDA/SimpleDataModels.html#curve-fitting-with-scipy",
    "title": "Simple Data Models",
    "section": "Curve Fitting with scipy",
    "text": "Curve Fitting with scipy\nfrom scipy.optimize import curve_fit\n\ndef nonlinear_func(x, a, b, c):\n    return a * np.sin(b * x) + c\n\nparams, _ = curve_fit(nonlinear_func, X.flatten(), y)\n\nX_range = np.linspace(1, 5, 100)\ny_fit = nonlinear_func(X_range, *params)\n\nplt.scatter(X, y, color='blue', label='Data')\nplt.plot(X_range, y_fit, color='purple', label='Non-Linear Fit')\nplt.legend()\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Simple Data Models</span>"
    ]
  },
  {
    "objectID": "course/chapters/SEDA/ModelTesting.html",
    "href": "course/chapters/SEDA/ModelTesting.html",
    "title": "Statistical Analysis & EDA",
    "section": "",
    "text": "1. Descriptive Statistics (``)\n\nOverview:\nDescriptive statistics summarize and analyze datasets by providing measures of central tendency, dispersion, and correlation. These statistics help in understanding the underlying patterns in chemical and materials science data.\n\nMeasures of Central Tendency: Mean, median, and mode provide insights into the average or most common values in the dataset.\nVariability: Variance, standard deviation, and interquartile range (IQR) measure how spread out the data is.\nCorrelation & Covariance: Determine relationships between two or more variables, useful for identifying dependencies in experimental data.\nSummary Statistics: Using Pandas, scientists can quickly summarize large datasets.\n\n\n\nCode Example:\nimport pandas as pd\nimport numpy as np\n\n# Sample dataset\ndata = pd.DataFrame({\n    'Sample': ['A', 'B', 'C', 'D', 'E'],\n    'Property': [10.2, 14.5, 13.3, 9.8, 12.7]\n})\n\n# Descriptive statistics\nprint(data.describe())\n\n\n\n\n2. Hypothesis Testing (``)\n\nOverview:\nHypothesis testing is a statistical method for making decisions or inferences about a population based on a sample. It is widely used in chemistry and materials science to validate experimental results and compare datasets.\n\nNull & Alternative Hypothesis: The null hypothesis (H₀) assumes no effect, while the alternative hypothesis (H₁) suggests a significant difference.\nt-tests: Used to compare the means of two groups (one-sample, independent, paired t-tests).\nANOVA (Analysis of Variance): Compares means of multiple groups to check for significant differences.\nChi-square Test: Used for categorical data to examine associations between different groups.\n\n\n\nCode Example:\nfrom scipy import stats\n\n# Example: Comparing two material compositions\ngroup1 = [10.2, 14.5, 13.3, 9.8, 12.7]\ngroup2 = [9.5, 13.1, 12.9, 8.7, 11.3]\n\n# Perform t-test\nt_stat, p_value = stats.ttest_ind(group1, group2)\nprint(f\"T-statistic: {t_stat}, P-value: {p_value}\")\n\n\n\n\n3. Principal Component Analysis (PCA) (``)\n\nOverview:\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets while retaining most of the variance. It is widely applied in spectroscopy, crystallography, and material property analysis.\n\nWhy Use PCA? Reduces noise and redundancy in large datasets.\nEigenvalues & Eigenvectors: Determine principal components that explain the most variance.\nInterpretation: PCA helps visualize multi-dimensional data in 2D or 3D spaces.\nApplications in Materials Science: PCA can extract significant features from spectral or compositional data, aiding in classification and clustering.\n\n\n\nCode Example:\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n# Sample dataset\nX = np.random.rand(10, 3)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plot PCA results\nplt.scatter(X_pca[:, 0], X_pca[:, 1])\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Analysis')\nplt.show()\n\n\n\n\n4. Clustering Techniques (``)\n\nOverview:\nClustering is an unsupervised learning technique used to group similar data points based on patterns. In chemistry and materials science, clustering helps in categorizing material properties, identifying experimental trends, and classifying samples.\n\nk-Means Clustering: Partitions data into k clusters based on similarity.\nHierarchical Clustering: Builds a tree of clusters using agglomerative or divisive methods.\nApplications: Clustering can classify unknown chemical samples, group materials with similar thermal or mechanical properties, and aid in phase identification in materials research.\n\n\n\nCode Example:\nfrom sklearn.cluster import KMeans\n\n# Example dataset\nX = np.random.rand(20, 2)\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nlabels = kmeans.labels_\n\n# Plot Clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.title('K-Means Clustering')\nplt.show()",
    "crumbs": [
      "Statistical and Exploratory Data Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Statistical Analysis & EDA</span>"
    ]
  },
  {
    "objectID": "informations/license.html",
    "href": "informations/license.html",
    "title": "License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Stefanie Kröll\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>License</span>"
    ]
  },
  {
    "objectID": "informations/privacy.html",
    "href": "informations/privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Hosting: GitHub Pages\nThis website does not collect, store or process any personally identifiable information and does not use cookies or other tracking technologies.\nThis website is hosted on GitHub Pages. GitHub says at",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Privacy Policy</span>"
    ]
  },
  {
    "objectID": "informations/privacy.html#hosting-github-pages",
    "href": "informations/privacy.html#hosting-github-pages",
    "title": "Privacy Policy",
    "section": "",
    "text": "Github-Docs &gt; GitHub Pages &gt; Get started &gt; About GitHub Pages (https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages):\n\n\n“When a GitHub Pages site is visited, the visitor’s IP address is logged and stored for security purposes, regardless of whether the visitor has signed into GitHub or not. For more information about GitHub’s security practices, see GitHub Privacy Statement”",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Privacy Policy</span>"
    ]
  },
  {
    "objectID": "informations/impressum.html",
    "href": "informations/impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "This Data Analysis and Visualization for Chemists and Material Scientists tutorial is created by:\nStefanie Kröll, MSc\nUniversity of Innsbruck\nInstitute of General, Inorganic and Theoretical Chemistry\nUniversity of Innsbruck\nCenter for Chemistry and Biomedicine\nInnrain 80-82\nA-6020 Innsbruck\nAUSTRIA, Europe\n\nas part of the lecture course “VU Data Science and Visualization Primer for Chemists and Material Scientists” at UIBK\n\nContact Information\nEmail: s.kroell@student.uibk.ac.at\n\n\nResponsible for Content: Stefanie Kröll\nCopyright Notice: © 2025 Stefanie Kröll. All rights reserved.\n\nPrivacy Policy\nExternal Link Policy and Disclaimers",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Impressum</span>"
    ]
  },
  {
    "objectID": "informations/disclaimer.html",
    "href": "informations/disclaimer.html",
    "title": "Disclaimers",
    "section": "",
    "text": "Disclaimer:\nAll information provided on this website is offered “as is” without any warranties, expressed or implied, including, but not limited to, warranties of merchantability, fitness for a particular purpose, or non-infringement. While efforts are made to ensure the accuracy of the information, the website owner makes no guarantee of its correctness or completeness. Use the information at your own discretion and risk.\n\n\n\n\n\n\nExternal Link Policy\n\n\n\nIt is imperative to note that the external links present on this website are intended to serve as a reference point, providing insights into software programs, packages, tools and tutorials that can be utilised. The primary function of these links is to facilitate further information on the subject matter. It is crucial to acknowledge that the website author is not held responsible for the content or information found on external websites. Furthermore, the website author does not exercise any control over the content of such external websites. It is, therefore, incumbent upon the user to exercise due diligence when engaging with external links. The user uses the links at his own risk and is subject to the terms of services of the respective providers.",
    "crumbs": [
      "Informations",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Disclaimers</span>"
    ]
  }
]